[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Notes for ECE 2714: Signals and Systems",
    "section": "",
    "text": "About the Notes\nThis is a set of supplementary notes and examples for ECE 2714 in the Bradley Department of Electrical and Computer Engineering at Virginia Tech.\nSee a mistake? file an issue. This helps improve the notes.",
    "crumbs": [
      "About the Notes"
    ]
  },
  {
    "objectID": "index.html#about-the-notes",
    "href": "index.html#about-the-notes",
    "title": "Supplementary Notes for ECE 2714: Signals and Systems",
    "section": "",
    "text": "License\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\nUpdate History\nThis book is continually updated as new content becomes available and errata corrected.\n\nAugust 2025: Conversion of Chapter 6 complete.\nJuly 2025: Conversion of Chapters 2-5 complete.\nJune 2025: Conversion of Chapter 1 complete.\nFeb 2025: Conversion from LaTeX pdf to accessible html started.",
    "crumbs": [
      "About the Notes"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "To the student:\nThis is a set of supplementary notes and examples for ECE 2714. It is not a replacement for the textbook, but can act as a reference and guide your reading. These notes are not comprehensive – often additional material and insights are covered during class.\nThis material is well covered in the official course text “Oppenheim, A. V., Willsky, A. S., and Nawab, S. H. Signals and Systems, Prentice Hall Pearson, 1996.” (abbreviated OW). This is an older, but very good book. However there are many, many texts that cover the same material. Engaged reading a textbook is one of the most important things you can do to learn this material. Again, these notes should not be considered a replacement for a textbook.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#to-the-instructor",
    "href": "preface.html#to-the-instructor",
    "title": "Preface",
    "section": "To the instructor:",
    "text": "To the instructor:\nThese notes are simply a way to provide some consistency in topic coverage and notation between and within semesters. Feel free to share these with your class but you are under no obligation to do so. There are many alternative ways to motivate and develop this material and you should use the way that you like best. This is just how I do it.\nEach chapter corresponds to a “Topic Learning Objective” and would typically be covered in one class meeting on a Tuesday-Thursday or Monday-Wednesday schedule. Note CT and DT topics are taught interleaved rather than in separate blocks. This gets the student used to going back and forth between the two signal and system types. We introduce time-domain topics first, followed by (real) frequency domain topics, using complex frequency domain for sinusoidal analysis only and as a bridge. Detailed analysis and application of Laplace and Z-transforms is left to ECE 3704.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements:",
    "text": "Acknowledgements:\nThe development of this course has been, and continues to be, a team effort. Dr. Mike Buehrer was instrumental in the initial design and roll-out of the course. Dr. Mary Lanzerotti has helped enormously with the course organization and academic integrity. All the instructors thus far: Drs. Buehrer, Safaai-Jazi, Lanzerotti, Kekatos, Poon, Xu, and Talty, have shaped the course in some fashion.\n\nC.L. Wyatt\nMay 7, 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Course Introduction",
    "section": "",
    "text": "1.1 Example Signals and Systems\nThe concepts and techniques in this course are probably the most useful in engineering. A signal is a function of one or more independent variables conveying information about a physical (or virtual) phenomena. A system may respond to signals to produce other signals, or produce signals directly.\nThis course is about the mathematical models and related techniques for the design and understanding of systems as signal transformations. We focus on a broadly useful class of systems, known as linear, time-invariant systems. You will learn about:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#example-signals-and-systems",
    "href": "01-intro.html#example-signals-and-systems",
    "title": "1  Course Introduction",
    "section": "",
    "text": "Example\n\n\nElectrical Circuits. This is a Sallen-Key filter, a second-order system commonly use to select frequencies from a signal:\n\n\n\nA circuit that implements a Sallen-Key filter.\n\n\nThere are two signals we can easily identify, the input signal as the voltage applied across \\(x(t)\\), and the output voltage measured across \\(y(t)\\). We build on your circuits course by viewing this circuit as an implementation of a more abstract linear system. We see how it can be viewed as a frequency selective filter. We will see how to answer questions such as: how do we choose the values of the resistors and capacitors to select the frequencies we are interested in? and how do we determine what those frequencies are?\n\n\n\n\nExample\n\n\nRobotic Joint. This is a Linear, Time-Invariant model of a DC motor, a mixture of electrical and mechanical components.\n\n\n\nA model of a DC motor.\n\n\nHow do we convert the motor into a servo for use in a robotic joint? What are its characteristics (e.g. how fast can it move)?\n\n\n\n\nExample\n\n\nAudio Processing. Suppose you record an interview for a podcast, but during an important part of the discussion, the HVAC turns on and there is an annoying noise in the background.\n\n\n\nA plot of a noisy signal in the time domain.\n\n\nHow could you remove the noise minimizing distortion to the rest of the audio?\n\n\n\n\nExample\n\n\nCommunications. Consider a wireless sensor, that needs to transmit to a base station, e.g. a wireless mic system.\n\n\n\nDiagram illustrating a wireless transmitter and reciever.\n\n\nHow should the signal be processed so it can be transmitted? How should the received signal be processed?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#types-of-problems",
    "href": "01-intro.html#types-of-problems",
    "title": "1  Course Introduction",
    "section": "1.2 Types of Problems",
    "text": "1.2 Types of Problems\nApplications of this material occur in all areas of science and engineering. When we have a measured output but are unsure what combination of inputs and system components could have produced it, we have a modeling problem.\n\n\n\nA Modeling Problem\n\n\nModels are the bedrock of the scientific method and are required to apply the concepts of this course to engineering problems.\nWhen we know the input and the system description and desire to know the output we have an analysis problem.\n\n\n\nAn Analysis Problem\n\n\nAnalysis problems are the kind you have encountered most often already. For example, given an electrical circuit and an applied voltage or current, what are the voltages and currents across and through the various components.\nWhen we know either the input and desired output and seek the system to perform this transformation,\n\n\n\nAn System Identification Problem\n\n\nor we know the system description and output and desire the input that would generate the output,\n\n\n\nAn Input Identification Problem\n\n\nwe have a design problem or identification problem.\nThis course focuses on modeling and analysis with applications to electrical circuits and devices for measurement and control of the physical world and is broadly applicable to all ECE majors. Some Examples:\n\nControls, Robotics, & Autonomy: LTI systems theory forms the basis of perception and control of machines.\nCommunications & Networking: LTI systems theory forms the basis of transmission and reception of signals, e.g. AM and FM radio.\nMachine Learning: LTI systems are often used to pre-process samples or to create basis functions to improve learning.\nEnergy & Power Electronic Systems: linear circuits are often modeled as LTI systems.\n\nSubsequent courses, e.g. ECE 3704, focus more on analysis and design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-objectives",
    "href": "01-intro.html#learning-objectives",
    "title": "1  Course Introduction",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\nThe learning objectives (LOs) for the course are:\n\nDescribe a given system using a block-level description and identify the input/output signals.\nMathematically model continuous and discrete linear, time-invariant systems using differential and difference equations respectively.\nAnalyze the use of filters and their interpretation in the time and frequency domains and implement standard filters in hardware and/or software.\nApply computations of the four fundamental Fourier transforms to the analysis and design of linear systems.\nCommunicate solutions to problems and document projects within the domain of signals and systems through formal written documents.\n\nThese are broken down further into the following topic learning objectives (TLOs). The TLOs generally map onto one class meeting but are used extensively in later TLOs.\nTLO 1: Course introduction (OW Forward and §1.0)\nTLO 2: Continuous-time (CT) signals (OW §1.1 through 1.4 and 2.5): A continuous-time (CT) signal is a function of one or more independent variables conveying information about a physical phenomena. This lecture gives an introduction to continuous-time signals as functions. You learn how to characterize such signals in a number of ways and are introduced to two very important signals: the unit impulse and the complex exponential.\nTLO 3: Discrete-time (DT) signals (OW §1.1 through 1.4)\nTLO 4: CT systems as linear constant coefficient differential equations (OW §2.4.1)\nTLO 5: DT systems as linear constant coefficient difference equations (OW §2.4.2)\nTLO 6: Linear time invariant CT systems (OW §1.5, 1.6, 2.3)\nTLO 7: Linear time invariant DT systems (OW §1.5, 1.6, 2.3)\nTLO 8: CT convolution (OW §2.2)\nTLO 9: DT convolution (OW §2.1)\nTLO 10: CT block diagrams (OW §1.5.2 and 2.4.3)\nTLO 11: DT block diagrams (OW §1.5.2 and 2.4.3)\nTLO 12: Eigenfunctions of CT systems (OW §3.2 and 3.8)\nTLO 13: Eigenfunctions of DT systems (OW §3.2 and 3.8)\nTLO 14: CT Fourier Series representation of signals (OW §3.3 through 3.5)\nTLO 15: DT Fourier Series representation of signals (OW §3.6 and 3.7)\nTLO 16: CT Fourier Transform (OW §4.0 through 4.7)\nTLO 17: DT Fourier Transform (OW §5.0 though 5.8)\nTLO 18: CT Frequency Response (OW §6.1, 6.2, 6.5)\nTLO 19: DT Frequency Response (OW §6.1, 6.2, 6.6)\nTLO 20: Frequency Selective Filters in CT (OW §3.9, 3.10, 6.3, 6.4)\nTLO 21: Frequency Selective Filters in DT (OW §3.11, 6.3, 6.4)\nTLO 22: The Discrete Fourier Transform\nTLO 23: Sampling (OW §7.1, 7.3, 7.4)\nTLO 24: Reconstruction (OW §7.2)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#graphical-outline",
    "href": "01-intro.html#graphical-outline",
    "title": "1  Course Introduction",
    "section": "1.4 Graphical Outline",
    "text": "1.4 Graphical Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html",
    "href": "02-ct-signals.html",
    "title": "2  Continuous-Time Signals",
    "section": "",
    "text": "2.1 Signals as Functions\nA continuous-time (CT) signal is a function of one or more independent variables conveying information about a physical phenomena. This lecture gives an introduction to continuous-time signals as functions. You learn how to characterize such signals in a number of ways and are introduced to two very important signals: the unit impulse and the complex exponential.\nIn order to reason about signals mathematically we need a representation or model. Signals are modeled as functions, mappings between sets \\[\nf: A \\rightarrow B\n\\] where \\(A\\) is a set called the domain and \\(B\\) is a set called the range.\nThe most basic classification of signals depends on the sets that makeup the domain and co-domain. We will be interested in two versions of the domain, the reals denoted \\(\\mathbb{R}\\) and the integers denoted \\(\\mathbb{Z}\\). We will be interested in two versions of the co-domain, the reals \\(\\mathbb{R}\\) and the set of complex numbers \\(\\mathbb{C}\\).\nSome other possibilities:\nThe co-domain can also be complex.\nSince the domains \\(\\mathbb{R}\\) and \\(\\mathbb{Z}\\) are usually interpreted as time, we will call these time-domain signals. In the time-domain, when the co-domain is \\(\\mathbb{R}\\) we call these real signals. All physical signals are real. However complex signals will become important when we discuss the frequency domain.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#signals-as-functions",
    "href": "02-ct-signals.html#signals-as-functions",
    "title": "2  Continuous-Time Signals",
    "section": "",
    "text": "Example\n\n\nAnalog Signal: If the function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), we call this an analog or real, continuous-time signal, e.g. a voltage at time \\(t \\in \\mathbb{R}\\), \\(v(t)\\). We will write these as \\(x(t)\\), \\(y(t)\\), etc. The units of \\(t\\) are seconds. Figure 2.1 shows some graphical representations, i.e. plots.\n\n\n\n\n\n\n\n\nFigure 2.1: Example plots of analog signals.\n\n\n\n\n\n\n\n\n\nExample\n\n\nReal, Discrete-time Signal: If the function \\(f: \\mathbb{Z} \\rightarrow \\mathbb{R}\\), we call this a real, discrete-time signal, e.g. the temperature every day at noon. We will write these as \\(x[n]\\), \\(y[n]\\), etc. Note \\(n\\) is dimensionless. Figure 2.2 shows some graphical representations.\n\n\n\n\n\n\n\n\nFigure 2.2: Example plots of real-valued, discrete-time signals.\n\n\n\n\n\n\n\n\n\n\\(f: \\mathbb{R} \\rightarrow \\mathbb{Z}\\), digital, continuous-time signals, e.g. the output of a general purpose pin on a microcontroller\n\\(f: \\mathbb{Z} \\rightarrow \\mathbb{Z}\\), digital, discrete-time signals, e.g. the signal on a computer bus\n\n\n\n\\(f: \\mathbb{R} \\rightarrow \\mathbb{C}\\), complex-valued, continuous-time signals, e.g. \\[\nx(t) = e^{j\\omega t} = \\cos(\\omega t) + j\\sin(\\omega t)\n\\]\n\\(f: \\mathbb{Z} \\rightarrow \\mathbb{C}\\), complex-valued, discrete-time signals, e.g. \\[\nx[n] = e^{j\\omega n} = \\cos(\\omega n) + j\\sin(\\omega n)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#primitive-models",
    "href": "02-ct-signals.html#primitive-models",
    "title": "2  Continuous-Time Signals",
    "section": "2.2 Primitive Models",
    "text": "2.2 Primitive Models\nWe mathematically model signals by combining elementary/primitive functions, for example:\n\npolynomials: \\(x(t) = t\\), \\(x(t) = t^2\\), etc.\ntransendental functions: \\(x(t) = e^t\\), \\(x(t) = \\sin(t)\\), \\(x(t) = \\cos(t)\\), etc.\npiecewise functions, e.g. \\[\n   x(t) = \\left\\{  \\begin{array}{cl}\n     f_1(t) & t &lt; 0\\\\\n     f_2(t) & t \\geq 0\\\\\n   \\end{array}\\right.\n\\]\n\n\n\nExample\n\n\nModeling a Switch: Consider a mathematical model of a switch, which moves positions at time \\(t = 0\\).\n\n\n\n\n\n\nFigure 2.3: Single pole, single throw switch connected to a unit DC source.\n\n\n\nWe use this model so much we give it it’s own name and symbol: Unit Step, \\(u(t)\\)\n\\[\nu(t) = \\left\\{  \\begin{array}{cl}\n        0 & t &lt; 0\\\\\n        1 & t \\geq 0\\\\\n      \\end{array}\\right.\n\\] so a mathematical model of the switch circuit above would be \\(x(t) = V u(t)\\).\nNote: some texts define the step function at \\(t=0\\) to be \\(1\\) or \\(\\frac{1}{2}\\). It is typically plotted like so:\n\n\n\n\n\n\nFigure 2.4: Plot of the unit step function. It turns on at the time origin and stays on forever.\n\n\n\n\n\n\n\nExample\n\n\nPure audio tone at “middle C”. A signal modeling the air pressure of a specific tone might be\n\\[\n  x(t) = \\sin\\left(2\\pi (261.6) t\\right)\n\\]\n\n\n\n\nExample\n\n\nChord. The chord “G”, an additive mixture of tones at G, B, and D and might be modeled as\n\\[\nx(t) = \\sin\\left(2\\pi (392) t\\right) + \\sin\\left(2\\pi (494) t\\right) + \\sin\\left(2\\pi (293) t\\right)\n\\]\nThis example shows we can use addition to build-up signals to approximate real signals of interest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#basic-transformations",
    "href": "02-ct-signals.html#basic-transformations",
    "title": "2  Continuous-Time Signals",
    "section": "2.3 Basic Transformations",
    "text": "2.3 Basic Transformations\nWe can also apply transformations to signals to increase their modeling flexibility.\n\nmagnitude scaling \\[\nx_2(t) = a x_1(t)\n\\] for \\(a \\in \\mathbb{R}\\).\nderivatives \\[\nx_2(t) = x_1^\\prime(t) = \\frac{d x_1}{dt}(t)\n\\]\nintegrals \\[\nx_2(t) = \\int\\limits_{-\\infty}^t x_1(\\tau) \\; d\\tau\n\\]\nsums \\[\ny(t) = \\sum\\limits_{i} x_i(t)\n\\] an important example we will see is the CT Fourier series.\n\nmultiplication (modulation) \\[\ny(t) = x_1(t) x_2(t)\n\\] For example amplitude modulation \\(y(t) = x(t)\\sin(\\omega_0 t)\\)\ntime shift \\[\nx_2(t) = x_1(t+\\tau)\n\\]\n\nif \\(\\tau &lt;0\\) it is called a delay\nif \\(\\tau &gt;0\\) it is called an advance\n\ntime scaling \\[\n  x_2(t) = x_1\\left(\\frac{t}{\\tau}\\right)\n  \\]\n\nif \\(\\tau &gt;1\\) increasing \\(\\tau\\) expands in time, slows down the signal\nif \\(0 &lt; \\tau &lt; 1\\) decreasing \\(\\tau\\) contracts in time, speeds up the signal\nif \\(-1 &lt; \\tau &lt;0\\) time reverses and increasing \\(\\tau\\) contracts in time, speeding up the signal\nif \\(\\tau &lt; -1\\) time reverses and decreasing \\(\\tau\\) expands in time, slows down the signal\n\nCommon uses are time reversal, \\(x_2(t) = x_1(-t)\\), and changing the frequency of of sinusoids.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#characterization-of-signals",
    "href": "02-ct-signals.html#characterization-of-signals",
    "title": "2  Continuous-Time Signals",
    "section": "2.4 Characterization of Signals",
    "text": "2.4 Characterization of Signals\nThere are a few basic ways of characterizing signals.\n\n\nDefinition\n\n\nCausal CT Signal. A CT signal is if \\(x(t) = 0\\) \\(\\forall t &lt; 0\\).\nAnti-Causal CT Signal. A CT signal is or acausal if \\(x(t) = 0\\) \\(\\forall t \\geq 0\\).\n\n\nA signal can be written as the sum of a causal and anti-causal signal.\n\n\nDefinition\n\n\nPeriodic Signals. A CT signal is if \\(x(t) = x(t + T)\\) \\(\\forall t\\) for a fixed parameter \\(T \\in \\mathbb{R}\\) called the .\n\n\nThe simplest periodic signals are those based on the sinusoidal functions.\n\n\nDefinition\n\n\nEven Signal. A CT signal is if \\(x(t) = x(-t)\\) \\(\\forall t\\).\nOdd Signal.  A CT signal is if \\(x(t) = -x(-t)\\) \\(\\forall t\\).\n\n\nAny CT signal can be written in terms of an even and odd component \\[\nx(t) = x_e(t) + x_o(t)\n\\] where \\[\n\\begin{array}{ll}\n  x_e(t) &= \\frac{1}{2}\\left\\{x(t) + x(-t)\\right\\} \\\\\n  & \\\\\n  x_o(t) &= \\frac{1}{2}\\left\\{x(t) - x(-t)\\right\\}\n\\end{array}\n\\]\n\n\nDefinition\n\n\nEnergy of a CT Signal. The energy of a CT signal \\(x(t)\\) is defined as a measure of the function \\[\n  E_x = \\lim_{T\\rightarrow\\infty} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n  \\]\n\n\n\n\nDefinition\n\n\nPower of a CT Signal. The power of a CT signal is the energy averaged over an interval as that interval tends to infinity. \\[\n  P_x = \\lim_{T\\rightarrow\\infty} \\frac{1}{2T} \\int_{-T}^T \\lvert x(t)\\rvert^2 dt \\; .\n  \\]\n\n\nSignals can be characterized based on their energy or power:\n\nSignals with finite, non-zero energy and zero power are called energy signals.\nSignals with finite, non-zero power (and by implication infinite energy) are called power signals.\n\nNote, these categories are non-exclusive, some signals are neither energy or power signals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#unit-impulse-function",
    "href": "02-ct-signals.html#unit-impulse-function",
    "title": "2  Continuous-Time Signals",
    "section": "2.5 Unit Impulse Function",
    "text": "2.5 Unit Impulse Function\nAn important CT signal is the unit impulse function, also called the “delta” \\(\\delta\\) function for the symbol traditionally used to define it. Applying this signal to a system models a “kick” to that system. For example, consider striking a tuning fork. The reason this signal is so important is that it will turn out that the response of the system to this input tells us all we need to know about a linear, time-invariant system!\n\n\nExample\n\n\nCT Impulse Function. The CT impulse function is not really a function at all, but a mathematical object called a “distribution”. Some equivalent definitions:\n\\[\n\\delta(t) = \\lim_{\\epsilon \\rightarrow 0}\\left\\{\n\\begin{array}{ll}\n  \\frac{1}{2\\epsilon} & |t| &lt; \\epsilon\\\\\n  0 & \\text{else}\n\\end{array}\n\\right.\n\\]\n\\[\n\\delta(t) = \\lim_{\\epsilon \\rightarrow 0} \\frac{1}{\\sqrt{2\\pi}\\epsilon} e^{-\\frac{t^2}{2\\epsilon^2}}\n\\] Note the area under each definition is always one.\n\n\nIn practice we can often use the following definition and some properties, without worrying about the distribution functions. \\[\n\\delta(t) = \\left\\{\n\\begin{array}{ll}\n  0 & t \\neq 0\\\\\n  \\infty & t = 0\n\\end{array}\n\\right.\n\\] which we draw as a vertical arrow in plots:\n\n\n\n\n\n\nFigure 2.5: Plot of the CT delta function.\n\n\n\nNote the height of the arrow is arbitrary. Often in the case of a non-unit impulse function the area is written in parenthesis near the arrow tip.\nThe following properties of the impulse function will be used often.\n\nThe area under the unit impulse is unity since by definition \\[\n\\int\\limits_{-\\infty}^{\\infty} \\delta(t) \\; dt = 1\n\\]\nSampling property: \\(x(t)\\delta(t-t_0) = x(t_0)\\delta(t-t_0)\\)\nSifting Property: \\[\n\\int\\limits_{a}^{b} x(t)\\delta(t-t_0) \\; dt = x(t_0)\n\\] for any \\(a &lt; t_0 &lt; b\\).\n\nWe previously defined the unit step function. The impulse can be defined in terms of the step: \\[\n\\delta(t) = \\frac{du}{dt}\n\\] and vice-versa \\[\nu(t) = \\int\\limits_{-\\infty}^{t} \\delta(\\tau) \\; d\\tau\n\\] using the notion of distributions, e.g.\n\\[\nu(t) = \\int\\limits_{-\\infty}^{t} \\delta(\\tau) \\; d\\tau = \\lim_{\\epsilon \\rightarrow 0} \\int\\limits_{-\\infty}^{t} \\frac{1}{\\sqrt{2\\pi}\\epsilon} e^{-\\frac{\\tau^2}{2\\epsilon^2}} \\; d\\tau = \\lim_{\\epsilon \\rightarrow 0} \\frac{1}{2}\\left(1+\\text{erf}\\left( \\frac{t}{\\sqrt{2}\\epsilon}\\right)\\right)\n\\]\nThe step and impulse function are related, but in many cases finding the response of a system to a step input is easier.\nWe can apply additional transformations to the impulse and step functions to get other useful signals, e.g.\n\nramp \\[\nr(t) = \\int\\limits_{-\\infty}^{t} u(\\tau) \\; d\\tau = tu(t)\n\\]\ncausal pulse of width \\(\\epsilon\\) \\[\np(t) = u(t) - u(t-\\epsilon)\n\\]\nnon-causal pulse of width \\(2\\epsilon\\) \\[\n    p(t) = u(t+\\epsilon) - u(t-\\epsilon)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#ct-complex-exponential",
    "href": "02-ct-signals.html#ct-complex-exponential",
    "title": "2  Continuous-Time Signals",
    "section": "2.6 CT Complex Exponential",
    "text": "2.6 CT Complex Exponential\nOne of the most important signals in systems theory is the complex exponential: \\[\nx(t) = C\\, e^{a t}\n\\] where the parameters \\(C, a \\in \\mathbb{C}\\) in general.\nWhen \\(C\\) and \\(a\\) are both real (\\(\\Im(C) = \\Im(a) = 0\\)), we have the familiar exponential. When \\(a &gt; 0\\) and \\(C &gt; 0\\), \\(x(t) = C e^{a t}\\) looks like:\n\n\n\n\n\n\nFigure 2.6: Plot of the expoential function with real, positive parameter.\n\n\n\nWhen \\(a &lt; 0\\) and \\(C &gt; 0\\), \\(x(t) = C e^{a t}\\) looks like:\n\n\n\n\n\n\nFigure 2.7: Plot of the expoential function with real, negative parameter.\n\n\n\nIf \\(C &lt; 0\\) the signals reflect about the time axis.\nTo get the pure sinusoidal case, let \\(C \\in \\mathbb{R}\\) and \\(a\\) be purely imaginary: \\(a = j\\omega_0\\): \\[\nx(t) = Ce^{j\\omega_0 t}\n\\] where \\(\\omega_0\\) is the frequency (in radians/sec). This is called the complex sinusoid.\nBy Euler’s identity: \\[\ne^{j\\omega_0 t} = \\cos(\\omega_0 t) + j\\sin(\\omega_0 t)\n\\] and \\[\n\\Re(x(t)) = \\cos(\\omega_0 t) = \\frac{1}{2}\\left( e^{j\\omega_0 t} + e^{-j\\omega_0 t} \\right)\n\\]\n\\[\n\\Im(x(t)) = \\sin(\\omega_0 t) = \\frac{1}{2j}\\left( e^{j\\omega_0 t} - e^{-j\\omega_0 t} \\right)\n\\] are both real sinusoids.\nNote that the sinusoids are periodic. Recall a signal \\(x(t)\\) is periodic with period \\(T\\) if \\[\nx(t) = x(t+T) \\; \\forall t\n\\] In the case of the complex sinusoid \\[\nCe^{j\\omega_0 t} = Ce^{j\\omega_0 (t+T)}= Ce^{j\\omega_0 t}\\underbrace{e^{j\\omega_0 T}}_{\\text{must be 1}}\n\\]\n\nif \\(\\omega_0 = 0\\) this is true for all \\(T\\)\nif \\(\\omega_0 \\neq 0\\), then to be periodic \\(\\omega_0 T = 2\\pi m\\) for \\(m = \\pm 1, \\pm 2, \\cdots\\). The smallest \\(T\\) for which this is true is the fundamental period \\(T_0\\) \\[\nT_0 = \\frac{2\\pi}{|\\omega_0|}\n\\] or equivalently \\(\\omega_0 = \\frac{2\\pi}{T_0}\\)\n\nSome useful properties of sinusoids:\n\nIf \\(x(t)\\) is periodic with period \\(T\\) and \\(g\\) is any function then \\(g(x(t))\\) is periodic with period \\(T\\).\nIf \\(x_1(t)\\) is periodic with period \\(T_1\\) and \\(x_2(t)\\) is periodic with period \\(T_2\\), and if there exists positive integers \\(a,b\\) such that \\[\naT_1 = b T_2 = P\n\\] then \\(x_1(t) + x_2(t)\\) and \\(x_1(t)x_2(t)\\) are periodic with period \\(P\\)\n\nThe last property implies that both \\(T_1\\) and \\(T_2\\) must both be rational in \\(\\pi\\) or neither should be. For example\n\n\\(x(t) = \\sin(2\\pi t) + \\cos(5\\pi t)\\) is periodic\n\\(x(t) = \\sin(2 t) + \\cos(5 t)\\) is periodic\n\\(x(t) = \\sin(2\\pi t) + \\cos(5 t)\\) is not periodic\n\nWhen the parameter \\(C\\) is complex we get a phase shift. Again let \\(a = j\\omega_0\\). When \\(C\\) is complex we can write it as \\(C = Ae^{j\\phi}\\) where \\(A = |C|\\) and \\(\\phi = \\angle C\\). Then\n\\[\nx(t) = Ae^{j\\phi} e^{j\\omega_0 t} = Ae^{j(\\omega_0 t+\\phi)}\n\\] and \\[\n\\Re(x(t)) = A\\cos(\\omega_0 t+\\phi)\n\\]\n\\[\n\\Im(x(t)) = A\\sin(\\omega_0 t+\\phi)\n\\]\nSince \\(\\sin\\) is a special case of \\(\\cos\\), i.e. \\(\\cos(\\theta) = \\sin(\\theta + \\frac{\\pi}{2})\\), the general real sinusoid is\n\\[\nA\\cos(\\omega_0 t + \\phi)\n\\]\n\n\\(A\\) is called the amplitude\n\\(\\omega_0\\) is again the frequency in radians/sec.\n\\(\\phi\\) is called the phase shift and is related to a time shift \\(T_s\\) by \\[\n\\phi = \\omega_0T_s\n\\]\n\nFor example the signal graphically represented as follows\n\n\n\n\n\n\n\n\nFigure 2.8: Example plot of sinusoidal signal.\n\n\n\n\n\nhas the functional representation\n\\[\nx(t) = 2\\cos\\left(\\frac{\\pi}{2} (t+\\tfrac{1}{2}) \\right) =  2\\cos\\left(\\frac{\\pi}{2} t +\\frac{\\pi}{4} \\right)\n\\]\n\n2.6.1 Energy of CT complex sinusoid\nRecall the energy of a CT signal \\(x(t)\\) is\n\\[\n  E_x = \\lim_{T\\rightarrow\\infty} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n\\] Substituting \\(x(t) = e^{j\\omega_0 t}\\) and letting \\(T = N T_0\\) \\[\n    E_x = \\lim_{N\\rightarrow\\infty} \\int\\limits_{-N T_0}^{N T_0} \\underbrace{\\lvert e^{j\\omega_0 t} \\rvert^2}_{\\text{always 1}} \\; dt = \\lim_{N\\rightarrow\\infty} 2NT_0 = \\infty\n\\]\n\n\n2.6.2 Power of CT complex sinusoid\nRecall the power of a CT signal \\(x(t)\\) is \\[\n  P_x = \\lim_{T\\rightarrow\\infty} \\frac{1}{2T} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n\\] Again, substituting \\(x(t) = e^{j\\omega_0 t}\\) and letting \\(T = N T_0\\) \\[\n  P_x = \\lim_{N\\rightarrow\\infty} \\frac{1}{2NT_0} \\int\\limits_{-N T_0}^{N T_0} \\underbrace{\\lvert e^{j\\omega_0 t} \\rvert^2}_{\\text{always 1}} \\; dt = \\lim_{N\\rightarrow\\infty} \\frac{1}{2NT_0} 2NT_0 = 1\n\\]\n\n\n2.6.3 Harmonics\nTwo CT complex sinusoids are harmonics of one another is both are periodic in \\(T_0\\). This occurs when\n\\[\n    x_k(t) = e^{jk\\omega_0 t} \\; \\text{for} \\; k = 0, \\pm 1, \\pm 2, \\cdots\n\\]\nThe term comes from music where the vibrations of a string instrument are modeled as a weighted combination of harmonic tones.\n\n\n2.6.4 Geometric interpretation of the Complex Exponential\nIn the general case we get a sinusoid signal modulated by an exponential. Let \\(C = Ae^{j\\phi}\\) and \\(a = r + j\\omega_0\\), then \\[\n  x(t) = C e^{a t} =  Ae^{j\\phi} e^{(r+j\\omega_0)t}\n\\] Expanding the terms and using Euler’s identity gives: \\[\nx(t) = \\underbrace{Ae^{rt}\\cos(\\omega_0 t+\\phi)}_{\\Re \\text{part}} + j \\underbrace{Ae^{rt}\\sin(\\omega_0 t+\\phi)}_{\\Im \\text{part}}\n\\] Each part is a real sinusoid whose amplitude is modulated by a real exponential.\nAn important visualization of the general case is to view the signal \\(x(t)\\) as a vector rotating counter-clockwise in the complex plane for positive \\(t\\).\n\n\n\n\n\n\nFigure 2.9: The CT complex sinusoid at a specific point in time.\n\n\n\nFor \\(r &lt; 0\\) the tip of the arrow traces out an inward spiral, whereas for \\(r &gt; 0\\) it traces out an outward spiral. For \\(r = 0\\) it traces out the unit circle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#example-problems",
    "href": "02-ct-signals.html#example-problems",
    "title": "2  Continuous-Time Signals",
    "section": "2.7 Example Problems",
    "text": "2.7 Example Problems\n\n2.7.1 \nConsider a signal described by the function \\[\n  x(t) = e^{-3t}\\sin(10\\pi t)u(t)\n   \\]\n\nDetermine the magnitude and phase of \\(x\\left( \\frac{1}{20}\\right)\\)\n\nSolution:\nSubstituting \\(t = \\frac{1}{20}\\) gives \\[\n  x\\left( \\frac{1}{20}\\right) = e^{-3\\frac{1}{20}}\\sin\\left(10\\pi \\frac{1}{20}\\right)u\\left( \\frac{1}{20}\\right) = e^{-\\frac{3}{20}} \\approx 0.86\n  \\] Since the signal is purely real and exponential is always positive, the magnitude is \\[\n  \\left|x\\left( \\frac{1}{20}\\right)\\right| = \\left| e^{-\\frac{3}{20}}\\right| =  e^{-\\frac{3}{20}}  \\approx 0.86\n  \\] and the phase is \\[\n  \\angle x\\left( \\frac{1}{20}\\right) = 0\n  \\]\n\nUsing Matlab, plot the signal \\(|x(t)|\\) between \\([-2, 2]\\). Give your code and embed the plot.\n\nSolution:\n% Solution to Example Problem 2.7.1b\n1t = -2:0.001:2;\n2x = exp(-3*t).*sin(10*pi*t).*heaviside(t);\n3hp = plot(t,abs(x));\ngrid on;\nxh = xlabel('t');\nyh = ylabel('x(t)');\nth = title('Plot for Example Problem 2.7.1b');\n\n% make the plot more readable\nset(gca, 'FontSize', 12, 'Box', 'off', 'LineWidth', 2);\nset(hp, 'linewidth', 2);\nset([xh, yh, th], 'FontSize', 12);\n\nset(gcf, 'PaperPositionMode', 'auto');\nprint -dpng example_2_7_1.png\n\n1\n\nCreate time slices from -2 seconds to 2 seconds in increments of 1 millisecond\n\n2\n\nCompute the signal value at each time slice\n\n3\n\nPlot the signal\n\n\n\n\n2.7.2 \nFind a solution to the differential equation \\[\n  \\frac{dy}{dt}(t) + 9y(t) = e^{-t}\n  \\] for \\(t \\geq 0\\), when \\(y(0) = 1\\).\nSolution: The homogeneous equation is \\[\n  \\frac{dy_h}{dt}(t) + 9y_h(t) = 0\n  \\] with initial condition \\(y_h(0) = 1\\). Its solution is of the form \\[\n  y_h(t) = C\\, e^{-9t}\n  \\] for constant \\(C\\). Using the initial condition \\[\n  y_h(0) = C\\, e^{-0} = C = 1\n  \\] gives \\[\n  y_h(t) = e^{-9t}\n  \\] The particular solution is of the form \\[\n  y_p(t) = C_1 e^{-t} + C_2 e^{-9t}\n  \\] Substitution and equating coefficients gives \\(C_1 = \\frac{1}{8}\\) and \\(C_2 = -\\frac{1}{8}\\). The total solution is the sum of the two solutions or \\[\n  y(t) = \\frac{1}{8} e^{-t} - \\frac{1}{8} e^{-9t} + e^{-9t} = \\frac{1}{8} e^{-t} + \\frac{7}{8} e^{-9t}\n  \\]\n\n\n2.7.3 \nFind a solution to the differential equation \\[\n  \\frac{dy}{dt}(t) + 9y(t) = e^{-t}\n  \\] for \\(t \\geq 0\\), when \\(y(0) = 1\\).\nSolution: The homogeneous equation is \\[\n  \\frac{dy_h}{dt}(t) + 9y_h(t) = 0\n  \\] with initial condition \\(y_h(0) = 1\\). Its solution is of the form \\[\n  y_h(t) = C\\, e^{-9t}\n  \\] for constant \\(C\\). Using the initial condition \\[\n  y_h(0) = C\\, e^{-0} = C = 1\n  \\] gives \\[\n  y_h(t) = e^{-9t}\n  \\] The particular solution is of the form \\[\n  y_p(t) = C_1 e^{-t} + C_2 e^{-9t}\n  \\] Substitution and equating coefficients gives \\(C_1 = \\frac{1}{8}\\) and \\(C_2 = -\\frac{1}{8}\\). The total solution is the sum of the two solutions or \\[\n  y(t) = \\frac{1}{8} e^{-t} - \\frac{1}{8} e^{-9t} + e^{-9t} = \\frac{1}{8} e^{-t} + \\frac{7}{8} e^{-9t}\n  \\]\n\n\n2.7.4 \nCompute the integral \\[\n    \\int\\limits_{-\\infty}^{\\infty} e^{-t^2} \\, \\delta(t-10)\\; dt\n    \\] where \\(\\delta(t)\\) is the delta function.\nSolution:\nUsing the sifting property of the delta function \\[\n  \\int\\limits_{a}^{b} f(t) \\, \\delta(t-t_0)\\; dt = f(t_0)\n  \\] for \\(a &lt; t_0 &lt; b\\), we get \\[\n  \\int\\limits_{-\\infty}^{\\infty} e^{-t^2} \\, \\delta(t-10)\\; dt = e^{-100} \\approx 0\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html",
    "href": "03-dt-signals.html",
    "title": "3  Discrete-Time Signals",
    "section": "",
    "text": "3.1 Primitive Models\nRecall from the previous chapter that a discrete-time (DT) signal is modeled as a function \\(f: \\mathbb{Z} \\rightarrow \\mathbb{C}\\). We will write these as \\(x[n]\\), \\(y[n]\\), etc. Note \\(n\\) is dimensionless. These are graphically plotted as stem or “lollipop” plots, as demonstrated in Chapter 2.\nSince the domain \\(\\mathbb{Z}\\) is usually interpreted as a time index, we will still call these time-domain signals. In the time-domain, when the co-domain is \\(\\mathbb{R}\\) we call these real DT signals. Unlike with CT signals there are no physical limitations requiring DT signals to be real, since in discrete hardware, a value at a given index can be a complex number, i.e. just a pair of numbers. However it is computationally advantageous to restrict ourselves to real arithmetic and such signals are often converted to or from CT signals, which do have to be real. For this reason, real DT signals dominate in models.\nAs with CT signals, we mathematically model DT signals by combining elementary/primitive functions, for example:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#primitive-models",
    "href": "03-dt-signals.html#primitive-models",
    "title": "3  Discrete-Time Signals",
    "section": "",
    "text": "polynomials: \\(x[n] = n\\), \\(x[n] = n^2\\), etc.\ntransendental functions: \\(x[n] = e^n\\), \\(x[n] = \\sin(n)\\), \\(x[n] = \\cos(n)\\), etc.\npiecewise functions, e.g. \\[\nx[n] = \\left\\{  \\begin{array}{cl}\nf_1[n] & n &lt; 0\\\\\nf_2[n] & n \\geq 0\\\\\n\\end{array}\\right.\n\\]\n\n\n\nDefinition\n\n\nThe DT counterpart of the CT step function is the DT Unit Step, \\(u[n]\\): \\[u[n] = \\left\\{  \\begin{array}{cl}\n    0 & n &lt; 0\\\\\n    1 & n \\geq 0\\\\\n  \\end{array}\\right.\\] Note, there are not continuity issues at \\(n=0\\) as DT functions have discrete domains.\n\n\n\n\nExample\n\n\nA sampled signal modeling the air pressure of a specific tone, sampled at 8kHz, might be \\[x[n] = \\sin\\left(2\\pi (261.6) \\tfrac{1}{8000} n\\right)\\] Such DT signals are commonly used in digital music generation, storage, and playback.\n\n\n\n\nExample\n\n\nSimilarly, the sampled chord \"G\", an additive mixture of tones at G, B, and D and might be modeled as \\[x[n] = \\sin\\left(2\\pi (392) \\tfrac{1}{8000} n\\right) + \\sin\\left(2\\pi (494) \\tfrac{1}{8000} n\\right) + \\sin\\left(2\\pi (293) \\tfrac{1}{8000} n\\right)\\] again sampled at 8kHz. This example shows we can use addition to build-up signals to approximate real signals of interest.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#basic-transformations",
    "href": "03-dt-signals.html#basic-transformations",
    "title": "3  Discrete-Time Signals",
    "section": "3.2 Basic Transformations",
    "text": "3.2 Basic Transformations\nSimilar to CT signals, we can also apply transformations to DT signals to increase their modeling flexibility.\n\nmagnitude scaling \\[x_2[n] = a x_1[n]\\] for \\(a \\in \\mathbb{R}\\).\ntime differences \\[x_2[n] = x_1[n] - x_1[n-1]\\]\nrunning sums \\[x_2[n] = \\sum\\limits_{m = -\\infty}^{n} x_1[m]\\]\nsums \\[y[n] = \\sum\\limits_{i} x_i[n]\\] an important example we will see is the DT Fourier series.\nmultiplication (modulation) \\[y[n] = x_1[n] x_2[n]\\]\ntime index shift \\[x_2[n] = x_1[n+m]\\]\n\nif \\(m &lt; 0\\) it is called a delay\nif \\(m &gt; 0\\) it is called an advance\n\ntime reversal \\[x_2[n] = x_1[-n]\\]\ndecimation \\[y[n] = x[m n]\\] for \\(m \\in \\mathbb{Z}^+\\).\n\ne.g. for \\(m=2\\) only keep every other sample\ne.g. for \\(m=3\\) only keep every third sample\netc.\n\ninterpolation \\[y[n] = \\left\\{  \\begin{array}{cl}\nx\\left[ \\frac{n}{m}\\right] & n = 0\\; , \\; \\pm m, , \\; \\pm 2m \\cdots\\\\\n0 & \\mbox{else}\n\\end{array}\\right.\\] When \\(m = 2\\) this inserts a zero sample between every sample of the signal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#characterization-of-signals",
    "href": "03-dt-signals.html#characterization-of-signals",
    "title": "3  Discrete-Time Signals",
    "section": "3.3 Characterization of Signals",
    "text": "3.3 Characterization of Signals\nThere are a few basic ways of characterizing DT signals.\n\n\nDefinition\n\n\nA DT signal is causal if \\(x[n] = 0\\) \\(\\forall n &lt; 0\\).\n\n\n\n\nDefinition\n\n\nA DT signal is anti-causal or acausal if \\(x[n] = 0\\) \\(\\forall n \\geq 0\\).\n\n\nA DT signal can be written as the sum of a causal and anti-causal signal.\nA DT signal is periodic if \\(x[n] = x[n + N] \\; \\forall n\\) for a fixed period \\(N \\in \\mathbb{Z}\\).\nA DT signal is even if \\(x[n] = x[-n] \\; \\forall n\\).\nA DT signal is odd if \\(x[n] = -x[-n] \\; \\forall n\\).\nAny DT signal can be written in terms of an even and odd component \\[x[n] = x_e[n] + x_o[n]\\] where \\[\\begin{array}{ll}\nx_e[n] &= \\frac{1}{2}\\left\\{x[n] + x[-n]\\right\\} \\\\\n& \\\\\nx_o[n] &= \\frac{1}{2}\\left\\{x[n] - x[-n]\\right\\}\n\\end{array}\\]\nAnalogous to CT signals, the energy of a DT signal is \\[E_x = \\lim_{N\\rightarrow\\infty} \\sum\\limits_{-N}^N \\lvert x[n]\\rvert^2 \\; .\\]\nAnd the power of a DT signal is the energy averaged over an interval as that interval tends to infinity.\n\\[P_x = \\lim_{N\\rightarrow\\infty} \\frac{1}{2N+1} \\sum\\limits_{-N}^N \\lvert x[n]\\rvert^2 \\; .\\]\nDT Signals with finite, non-zero energy and zero power are called energy signals. DT Signals with finite, non-zero power (and by implication infinite energy) are called power signals. These categories are non-exclusive, some signals are neither energy or power signals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#dt-unit-impulse-function",
    "href": "03-dt-signals.html#dt-unit-impulse-function",
    "title": "3  Discrete-Time Signals",
    "section": "3.4 DT Unit Impulse Function",
    "text": "3.4 DT Unit Impulse Function\nIn DT the unit impulse function, denoted \\(\\delta[n]\\) is defined as \\[\\delta[n] = \\left\\{\n\\begin{array}{ll}\n  1 & n = 0\\\\\n  0 & \\text{else}\n\\end{array}\n\\right.\\] Note this definition is straightforward compared to the CT impulse as there are no continuity issues and it is not defined in terms of a distribution. It is typically drawn as\n\n\n\n\n\n\n\n\nFigure 3.1: Plot of discrete-time delta function.\n\n\n\n\n\nSome useful properties of the DT impulse function are:\n\nEnergy is 1: \\(\\sum\\limits_{n=-\\infty}^{\\infty} \\delta[n] = 1\\)\nSampling: \\(x[n]\\delta[n-n_0] = x[n_0]\\delta[n-n_0]\\)\nSifting: \\(\\sum\\limits_{n=-\\infty}^{\\infty} x[n]\\delta[n-n_0] = x[n_0]\\)\n\nThe impulse can be defined in terms of the step: \\[\\delta[n] = u[n] - u[n-1]\\] and vice-versa \\[u[n] = \\sum\\limits_{m=-\\infty}^{n} \\delta[m]\\] or \\[u[n] = \\sum\\limits_{k=0}^{\\infty} \\delta[n-k]\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#dt-complex-exponential",
    "href": "03-dt-signals.html#dt-complex-exponential",
    "title": "3  Discrete-Time Signals",
    "section": "3.5 DT Complex Exponential",
    "text": "3.5 DT Complex Exponential\nThe DT Complex Exponential is defined in a similar fashion the the CT version, but with some important differences. The general DT complex exponential is given by the expression: \\[x[n] = Ce^{\\beta n}\\] where in general \\(C \\in \\mathbb{C}\\) and \\(\\beta \\in \\mathbb{C}\\). It is sometimes convenient (for reasons we will see later) to write this as \\[x[n] = C \\alpha^n\\] where \\(\\alpha = e^{j\\theta}\\) is a complex number \\(\\alpha = \\cos(\\theta) + j\\sin(\\theta)\\).\nWe now examine several special cases.\n\n3.5.1 DT Complex Exponential: real case\nLet \\(C\\) and \\(\\alpha\\) be real, then there are four intervals of interest:\n\n\\(\\alpha &gt; 1\\)\n\\(0 &lt; \\alpha &lt; 1\\)\n\\(-1 &lt; \\alpha &lt; 0\\)\n\\(\\alpha &lt; -1\\)\n\nEach of these are shown in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: DT Complex Exponential: real case, four intervals of interest.\n\n\n\n\n\n\n\n3.5.2 DT Complex Exponential: sinusoidal case\nLet \\(C = 1\\). When \\(\\beta\\) is purely imaginary, \\(\\beta = j\\omega_0\\) \\[x[n] = e^{j\\omega_0 n}\\]\nAs in CT, by Euler’s identity: \\[e^{j\\omega_0 n} = \\cos(\\omega_0 n) + j\\sin(\\omega_0 n)\\] and \\[\\Re(x[n]) = \\cos(\\omega_0 n) = \\frac{1}{2}\\left( e^{j\\omega_0 n} + e^{-j\\omega_0 n} \\right)\\] \\[\\Im(x[n]) = \\sin(\\omega_0 n) = \\frac{1}{2j}\\left( e^{j\\omega_0 n} - e^{-j\\omega_0 n} \\right)\\]\nThe energy and power are the same as for the CT complex sinusoid: \\(E_x = \\infty\\) and \\(P_x = 1\\).\n\n\n3.5.3 DT Complex Exponential: sinusoidal case with phase shift\nThe general DT sinusoid is\n\\[x[n] = A\\cos(\\omega_0 n + \\phi)\\]\n\n\\(A\\) is called the amplitude\n\\(\\phi\\) is called the phase shift\n\\(\\omega_0\\) is now in radians (assuming \\(n\\) is dimensionless)\n\n\n\n\n\n\n\n\n\n\nFor CT sinusoids as \\(\\omega_0\\) increases the signal oscillates faster and faster. However for DT sinusoids there is a \"fastest\" oscillation.\n\\[e^{j\\omega_0 n}\\rvert_{\\omega_0 = \\pi} = e^{j\\pi n} = (-1)^n\\]\n\n\n\n\n\n\n\n\n\n\n\n3.5.4 Properties of DT complex sinusoid\nIf we consider two frequencies: \\(\\omega_0\\) and \\(\\omega_0+2\\pi\\). In the first case: \\[x[n] = e^{j\\omega_0 n}\\] In the second case: \\[\\begin{array}{ll}\nx[n] &= e^{j(\\omega_0+2\\pi) n} \\\\\n&= \\underbrace{e^{j2\\pi n}}_{\\text{always 1}}\\; e^{j\\omega_0 n} \\\\\n&= e^{j\\omega_0 n}\n\\end{array}\\]\nThus the two are the same signal. This has important implications later in the course.\nAnother difference between CT and DT complex sinusoids is periodicity. Recall for a DT signal to be periodic with period \\(N\\) \\[x[n] = x[n+N] \\; \\forall n\\] Substituting the complex sinusoid \\[e^{j\\omega_0 n} = e^{j\\omega_0 (n+N)} = e^{j\\omega_0 n}e^{j\\omega_0 N}\\] requires \\(e^{j\\omega_0 N} = 1\\), which implies \\(\\omega_0 N\\) is a multiple of \\(2\\pi\\): \\[\\omega_0 N = 2\\pi m \\;\\;\\; m = \\pm 1, \\pm 2, \\cdots\\] or equivalently \\[\\frac{|\\omega_0|}{2\\pi} = \\frac{m}{N}\\] thus \\(\\omega_0\\) must be a rational multiple of \\(\\pi\\).\nTwo DT complex sinusoids are harmonics of one another is both are periodic in \\(N\\), i.e when\n\\[x_k(t) = e^{jk\\frac{2\\pi}{N} n} \\; \\text{for} \\; k = 0, \\pm 1, \\pm 2, \\cdots\\]\nThis implies there are only \\(N\\) distinct harmonics in DT.\n\n\n3.5.5 DT Complex Exponential: general case\nIn the general case we get a sinusoid signal modulated by an exponential. Let \\(C = Ae^{j\\phi}\\) and \\(\\beta = r + j\\omega_0\\), then \\[x[n] = C e^{\\beta n} =  Ae^{j\\phi} e^{(r+j\\omega_0)n}\\] Expanding the terms and using Euler’s identity gives:\n\\[x[n] = \\underbrace{Ae^{rn}\\cos(\\omega_0 n+\\phi)}_{\\Re \\text{part}} + j \\underbrace{Ae^{rn}\\sin(\\omega_0 n+\\phi)}_{\\Im \\text{part}}\\] Each part is a real sinusoid whose amplitude is modulated by a real exponential.\nThe visualization of the general case is to view the signal \\(x[n]\\) as a vector rotating through fixed angles in the complex plane.\n\n\n\n\n\n\nFigure 3.3: The DT complex sinusoid at a specific point in time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html",
    "href": "04-ct-lccde.html",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "",
    "text": "4.1 Solving Linear, Constant Coefficient Differential Equations\nRecall a system is a transformation of signals, turning the input signal into the output signal. While this might seem like a new concept to you, you already know something about them from your differential equations course, i.e. MATH 2214 and your circuits course.\nFor example, consider the following circuit:\nwhere the switch moves position at \\(t = 0\\). The governing equation for the circuit when \\(t &lt; 0\\) is \\[\\frac{dV_c}{dt}(t) + \\frac{1}{RC}V_c(t) = 0\\] a homogeneous differential equation of first-order. From a DC analysis, the initial condition on the capacitor voltage is \\(V_C(0^-) = 0\\), so there is no current flowing prior to \\(t = 0\\) and the solution is \\(V_C(t) = 0\\) for \\(t &lt; 0\\).\nAfter the switch is thrown, the governing equation for the circuit when \\(t \\geq 0\\) is \\[\\frac{dV_c}{dt}(t) + \\frac{1}{RC}V_c(t) = \\frac{1}{RC}\\] Since the voltage across the capacitor cannot change instantaneously \\(V_C(0^-) = V_C(0^+) = 0\\), giving the auxillary condition necessary to solve this equation, which has the form \\[V_C(t) = A + Be^{-\\frac{1}{RC}t}\\] Using the auxillary condition we find \\[V_C(0) = A + Be^{-\\frac{1}{RC}0} = A + B = 0 \\mbox{ which implies } B = -A\\] Subsitution back into the differential equation and equating the coefficients gives \\(A = 1\\). Thus the voltage for \\(t \\geq 0\\) is \\[V_C(t) = 1 - e^{-\\frac{1}{RC}t}\\]\nSuppose we consider the voltage after the switch as the input signal \\(x(t)\\) to the system composed of the series RC. As we have seen previously a mathematical model of the switch is the unit step \\(x(t) = u(t)\\). Suppose we consider the capacitor voltage at the output of the system, so that \\(y(t) = V_C(t)\\). Then we can consider the system to be represented by the linear, constant-coefficient differential equation \\[\\frac{dy}{dt}(t) + \\frac{1}{RC}y(t) = \\frac{1}{RC}x(t)\\] where \\(x(t) = u(t)\\) and the solution \\(y(t)\\) is the step response \\[y(t) = \\left(1 - e^{-\\frac{1}{RC}t}\\right)u(t)\\]\nAs we will see later this representation of systems is central to the course, so we take some time here to review the solution of such equations.\nA linear, constant coefficient (LCC) differential equation is of the form \\[a_0\\, y + a_1\\, \\frac{dy}{dt} + a_2\\, \\frac{d^2y}{dt^2} + \\cdots + a_N\\, \\frac{d^Ny}{dt^N}  = b_0\\, x + b_1\\, \\frac{dx}{dt} + b_2\\, \\frac{d^2x}{dt^2} + \\cdots + b_M\\, \\frac{d^Mx}{dt^M}\\] which can be written compactly as \\[\\sum\\limits_{k = 0}^{N} a_k\\, \\frac{d^ky}{dt^k} = \\sum\\limits_{k = 0}^{M} b_k\\, \\frac{d^kx}{dt^k}\\]\nIt is helpful to clean up this notation using the derivative operator \\(D^n = \\frac{d^n}{dt^n}\\). For example \\(D^2y = \\frac{d^2y}{dt^2}\\) and \\(D^0 y= y\\). To give for form as \\[\\sum\\limits_{k = 0}^{N} a_k\\, D^k y = \\sum\\limits_{k = 0}^{M} b_k\\, D^k x\\]\nWe can factor out the derivative operators \\[a_0y + a_1Dy + a_2D^2y + \\cdots + a_ND^Ny  = b_0\\, x + b_1\\, Dx + b_2\\, D^2x + \\cdots + b_M\\, D^M x\\] \\[\\underbrace{\\left(a_0 + a_1D + a_2D^2 + \\cdots + a_ND^N\\right)}_{\\text{Polynomial in } D, Q(D)} y = \\underbrace{\\left(b_0 + b_1 D + b_2 D^2 + \\cdots + b_M D^M\\right)}_{\\text{Polynomial in } D, P(D)} x\\] to give:\n\\[Q(D)y = P(D)x\\] You learned how to solve these in differential equations (Math 2214) as \\[y(t) = y_\\text{h}(t) + y_\\text{p}(t)\\]\nThe term \\(y_\\text{h}(t)\\) is the solution of the homogeneous equation \\[Q(D)y = 0\\] Given the \\(N-1\\) auxillary conditions \\(y(t_0) = y_0\\), \\(Dy(t_0) = y_1\\), \\(D^2y(t_0) = y_2\\), up to \\(D^{N-1}y(t_0) = y_{N-1}\\).\nThe term \\(y_\\text{p}(t)\\) is the solution of the particular equation \\[Q(D)y = P(D)x\\] for a given \\(x(t)\\).\nRather than recapitulate the solution to \\(y_\\text{h}(t)\\) and \\(y_\\text{p}(t)\\) in the general case we focus on the homogeneous solution \\(y_\\text{h}(t)\\) only. The reason is that we will use the homogeneous solution to find the impulse response below and take a different approach to solving the general case for an arbitrary input using the impulse response and convolution (next week).\nTo solve the homogenous system:\nStep 1: Find the characteristic equation by replacing the derivative operators by powers of an arbitrary complex variable \\(s\\). \\[Q(D) = a_0 + a_1D + a_2D^2 + \\cdots + a_ND^N\\] becomes \\[Q(s) = a_0 + a_1s + a_2s^2 + \\cdots + a_Ns^N\\] a polynomial in \\(s\\) with \\(N\\) roots \\(s_i\\) for \\(i = 1, 2, \\cdots, N\\) such that \\[(s - s_1)(s-s_2)\\cdots(s-s_N) = 0\\]\nStep 2: Select the form of the solution, a sum of terms corresponding to the roots of the characteristic equation.\nStep 3: Solve for the unknown constants in the solution using the auxillary conditions.\nWe now examine two common special cases, when \\(N=1\\) (first-order) and when \\(N=2\\) (second-order).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html#solving-linear-constant-coefficient-differential-equations",
    "href": "04-ct-lccde.html#solving-linear-constant-coefficient-differential-equations",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "",
    "text": "For a real root \\(s_1\\in \\mathbb{R}\\) the term is of the form \\[C_1 e^{s_1 t}.\\]\nFor a pair of complex roots (they will always be in pairs) \\(s_{1,2} = a \\pm jb\\) the term is of the form \\[C_1 e^{s_1 t} + C_2 e^{s_2 t} = e^{a t}\\left(C_3\\cos(bt) + C_4\\sin(bt)\\right) = C_5 e^{a t}\\cos(bt + C_6).\\]\nFor a repeated root \\(s_1\\), repeated \\(r\\) times, the term is of the form \\[e^{s_1 t} (C_0 + C_1 t + \\cdots + C_{r-1} t^{r-1}).\\]\n\n\n\n\n4.1.1 First-Order Homogeneous LCCDE\nConsider the first order homogeneous differential equation \\[\\frac{dy}{dt}(t) + ay(t) = 0 \\mbox{ for } a \\in \\mathbb{R}\\] The characteristic equation is given by \\[s + a = 0\\] which has a single root \\(s_1 = -a\\). The solution is of the form \\[y(t) = Ce^{s_1 t} = Ce^{-a t}\\] where the constant \\(C\\) is found using the auxillary condition \\(y(t_0) = y_0\\).\n\n\nExample\n\n\nConsider the homogeneous equation \\[\\frac{dy}{dt}(t) + 3y(t) = 0 \\mbox{ where } y(0) = 10\\] The solution is \\[y(t) = Ce^{-3 t}\\] To find \\(C\\) we use the auxillary condition \\[y(0) = Ce^{-3 \\cdot 0} = C = 10\\] and the final solution is \\[y(t) = 10e^{-3 t}\\]\n\n\n\n\n4.1.2 Second-Order Homogeneous LCCDE\nConsider the second-order homogeneous differential equation \\[\\frac{d^2y}{dt^2}(t) + a\\frac{dy}{dt}(t) + by(t) = 0 \\mbox{ for } a,b \\in \\mathbb{R}\\] The characteristic equation is given by \\[s^2 + as + b = 0\\]\nLet’s look at several examples to illustrate the functional forms.\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 7\\frac{dy}{dt}(t) + 10y(t) = 0\\] The characteristic equation is given by \\[s^2 + 7s + 10 = 0\\] which has roots \\(s_1 = -2\\) and \\(s_2 = -5\\). Thus the form of the solution is \\[y(t) = C_1e^{-2t} + C_2e^{-5t}\\]\n\n\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 2\\frac{dy}{dt}(t) + 5y(t) = 0\\] The characteristic equation is given by \\[s^2 + 2s + 5 = 0\\] which has complex roots \\(s_1 = -1+j2\\) and \\(s_1 = -1-j2\\). Thus the form of the solution is \\[y(t) = e^{-t}\\left(C_1\\cos(2t) + C_2\\sin(2t)\\right)\\]\n\n\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 2\\frac{dy}{dt}(t) + y(t) = 0\\] The characteristic equation is given by \\[s^2 + 2s + 1 = 0\\] which has a root \\(s_1 = -1\\) repeated \\(r=2\\) times. Thus the form of the solution is \\[y(t) = e^{-t}\\left(C_1 + C_2t\\right)\\]\n\n\nIn each of the above cases the constants, \\(C_1\\) and \\(C_2\\), are found using the auxillary conditions \\(y(t_0)\\) and \\(y\\prime(t_0)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html#finding-the-impulse-response-of-a-system-described-by-a-lccde",
    "href": "04-ct-lccde.html#finding-the-impulse-response-of-a-system-described-by-a-lccde",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "4.2 Finding the impulse response of a system described by a LCCDE",
    "text": "4.2 Finding the impulse response of a system described by a LCCDE\nAs we will see next week an important response of a system is the one that corresponds to an impulse input, i.e. the impulse response \\(y(t) = h(t)\\) when \\(x(t) = \\delta(t)\\). Thus we focus here on a recipe for solving LCCDEs for this special case when \\(M \\leq N\\). We will skip the derivation of why this works.\nOur goal is to find the solution to \\(Q(D)y = P(D)x\\) when \\(x(t)=\\delta(t)\\).\nStep 1: Rearrange the LCCDE so that \\(a_N = 1\\), i.e. divide through by \\(a_N\\) to put it into a standard form.\nStep 2: Let \\(y_h(t)\\) be the homogeneous solution to \\(Q(D)y_h = 0\\) for auxillary conditions \\[D^{N-1}y_h(0^+) = 1 \\; , \\; D^{N-2}y_h(0^+) = 0 \\; , \\; \\text{etc.} \\; y_h(0^+) = 0\\]\nStep 3: Assume a form for \\(h(t)\\) given by: \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0 \\text{ unless } N=M} + \\underbrace{\\left[ P(D)y_h\\right]}_{\\text{apply } P(D) \\text{ to } y_n(t)}u(t)\\]\nRecall from above the homogeneous solution depends on the roots of the characteristic equation \\(Q(D) = 0\\).\n\nroots are either real, or\nroots occur in complex conjugate pairs, or\nrepeated roots.\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[2\\frac{dy}{dt}(t) + 2y(t) = 2x(t)\\] In the standard for the LCCDE is \\[\\frac{dy}{dt}(t) + y(t) = x(t)\\] The characteristic equation is given by \\[s + 1 = 0\\] which has a single root \\(s_1 = -1\\). The solution is of the form \\[y_h(t) = Ce^{-t}\\] with the special auxillary condition \\(y(0) = 1\\), so that \\[y_h(t) = e^{-t}\\] Since \\(P(D) = 1\\) and \\(N = 1 \\neq M = 0\\) the impulse response is \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0} + \\left[ \\underbrace{P(D)}_{1}y_h(t)\\right]u(t) = e^{-t}u(t)\\]\n\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[\\frac{dy}{dt}(t) + y(t) = \\frac{dx}{dt}(t) + x(t)\\] It is already in the standard form. The homogeneous solution is the same as in Example 1, \\[y_h(t) = e^{-t}\\] however now \\(M = N = 1\\) with \\(b_1 = 1\\) and \\(P(D) = D+1\\). Thus, the impulse response is \\[h(t) = \\underbrace{b_N}_{=1}\\delta(t) + \\left[ \\underbrace{P(D)}_{D+1}y_h(t)\\right]u(t) = \\delta(t) + \\left\\{[D+1]e^{-t}\\right\\}u(t) = \\delta(t) + [- e^{-t} + e^{-t}]u(t) = \\delta(t)\\]\n\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[\\frac{d^2y}{dt^2}(t) + 7\\frac{dy}{dt}y(t) + 10y(t) = x(t)\\] It is already in the standard form. The characteristic equation is given by \\[s^2 + 7s + 10 = 0\\] which has roots \\(s_1 = -2\\) and \\(s_2 = -5\\). Thus the form of the solution is \\[y_h(t) = C_1e^{-2t} + C_2e^{-5t}\\] The special auxillary conditions are \\(y_h(0) = 0\\) and \\(y^\\prime_h(0) = 1\\). Using these conditions \\[y_h(0) = C_1e^{-2t} + C_2e^{-5t} |_{t = 0} = C_1 + C_2 = 0\\] \\[y^\\prime_h(0) = -2C_1e^{-2t} - 5C_2e^{-5t} |_{t = 0} = -2C_1 -5C_2 = 1\\] Solving for the constants gives \\(C_1 = \\frac{1}{3}\\) and \\(C_2 = -\\frac{1}{3}\\). Since \\(P(D) = 1\\) and \\(N = 2 \\neq M = 0\\) the impulse response is \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0} + \\left[ \\underbrace{P(D)}_{1}y_h(t)\\right]u(t) = \\frac{1}{3} e^{-2t}u(t) - \\frac{1}{3} e^{-5t}u(t)\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html",
    "href": "05-dt-lccde.html",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "",
    "text": "5.1 Definition of linear constant coefficient difference equation\nA difference equation is a relation among combinations of two DT functions and shifted versions of them. Similar to differential equations where the solution is a CT function, the solution to a difference equation is a DT function. For example: \\[y[n+1] + \\frac{1}{2}y[n] = x[n]\\] is a first order, linear, constant-coefficient difference equation. Given \\(x[n]\\) the solution is a function \\(y[n]\\). We can view this as a representation of a DT system, where \\(x[n]\\) is the input signal and \\(y[n]\\) is the output.\nThere is a parallel theory to differential equations for solving difference equations. However in this lecture we will focus specifically on the iterative solution of linear, constant-coefficient difference equations and the case when the input is a delta function, as this is all we need for this course.\nA linear, constant-coefficient, difference equation (LCCDE) comes in one of two forms.\nThe order of the system is given by \\(N\\). The delay and advance forms are equivalent because the equation holds for any \\(n\\), and we can move back and forth between them as needed by a constant index-shift.\nIt will be convenient to define the operator \\(E^m\\) as shifting a DT function by positive \\(m\\), i.e. \\(E^m x[n] = x[n+m]\\), and the operator \\(D^m\\) as shifting a DT function by negative \\(m\\), i.e. \\(D^m x[n] = x[n-m]\\). These are called the advance and delay operators respectively. Then, the advance form of the difference equation using this operator notation is \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\] \\[a_0 E^Ny + a_1E^{N-1}y + \\cdots a_N y = b_0 E^{N}x + \\cdots b_M E^{N-M}x\\] Factoring out the advance operators gives \\[\\underbrace{\\left(a_0E^N + a_1E^{N-1} + \\cdots a_N\\right)}_{Q(E)} y = \\underbrace{\\left(b_0 E^{N} + \\cdots b_M E^{N-M}\\right)}_{P(E)} x\\] or \\[Q(E)y[n] = P(E)x[n]\\]\nSimilarly, the delay form of the difference equation using this operator notation is \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] \\[a_0y[n] + a_1 Dy + \\cdots a_N D^N y = b_0 x + \\cdots b_MD^M x\\] Note: The DT delay operator \\(D\\) is similar, but not identical to the derivative operator \\(D\\) in CT.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#definition-of-linear-constant-coefficient-difference-equation",
    "href": "05-dt-lccde.html#definition-of-linear-constant-coefficient-difference-equation",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "",
    "text": "Delay form. \\[\\sum\\limits_{k = 0}^N a_k y[n-k] = \\sum\\limits_{k = 0}^M b_k x[n-k]\\] or \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\]\nAdvance form. Let \\(n\\rightarrow n+N\\), then the delay form becomes \\[\\sum\\limits_{k = 0}^N a_k y[n+N-k] = \\sum\\limits_{k = 0}^M b_k x[n+N-k]\\] or \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\]\n\n\n\n\nExample\n\n\nThe delay form is \\[a_0y[n] + a_1 y[n-1] + a_2 y[n-2] = b_0 x[n] + b_1 x[n-1]\\] Replacing \\(n \\rightarrow n+2\\), the advance form is \\[a_0 y[n+2] + a_1 y[n+1] + a_2 y[n] = b_0 x[n+2] + b_1 x[n+1]\\]\n\n\n\n\n\n\nExample\n\n\nConsider the difference equation \\[3y[n+1] + 4y[n] + 5y[n-1] = 2x[n+1]\\] The advance form would be: \\[3y[n+2] + 4y[n+1] + 5y[n] = 2x[n+2]\\] or using the advance operator \\[\\left(3E^2 + 4E + 5\\right)y = 2E^2x\\] with \\(Q(E) = 3E^2 + 4E + 5\\) and \\(P(E) = 2E^2\\).\nThe delay form would be: \\[3y[n] + 4y[n-1] + 5y[n-2] = 2x[n]\\] or using the delay operator \\[\\left(5D^2 + 4D + 3\\right)y = 2x\\] with \\(Q(D) = 5D^2 + 4D + 3\\) and \\(P(D) = 2\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#iterative-solution-of-lccdes",
    "href": "05-dt-lccde.html#iterative-solution-of-lccdes",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.2 Iterative solution of LCCDEs",
    "text": "5.2 Iterative solution of LCCDEs\nDifference equations are different (pun!) from differential equations in that they can be solved by manually running the equation forward using previous values of the output and current and previous values of the input, given some initial conditions. This is called an iterative solution for this reason.\nTo perform an iterative solution we need the difference equation in delay form \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] We then solve for the current output \\(y[n]\\) \\[y[n] =  - \\left(\\frac{a_1}{a_0}y[n-1] + \\cdots \\frac{a_N}{a_0} y[n-N]\\right) + \\frac{b_0}{a_0} x[n] + \\cdots \\frac{b_M}{a_0}x[n-M]\\]\nNow lets examine what this expression says in words. To compute the current output \\(y[n]\\) we need the value of the previous \\(N-1\\) outputs, the value of the current input \\(x[n]\\) and \\(M-1\\) previous inputs (and the coefficients). Then we can compute the next output \\(y[n+1]\\) by adding the previous computation result for \\(y[n]\\) to our list of things to remember, and forgetting one previous value of \\(y\\). This can continue as long as we like.\n\n\nExample\n\n\nConsider the first-order difference equation \\[y[n+1] + y[n] = x[n+1]\\] where \\(y[-1] = 1\\) and \\(x[n] = u[n]\\). We first convert this to delay form \\[y[n] = -y[n-1] + x[n]\\; .\\] Then we can compute \\(y[0]\\) as \\[y[0] = -y[-1] + x[0] = -1 + 1 = 0\\] and continuing\n\\[\\begin{align*}\n  y[1] &= -y[0] + x[1] = 0 + 1 = 1\\\\\n  y[2] &= -y[1] + x[2] = -1 + 1 = 0\\\\\n  y[3] &= -y[2] + x[3] = 0 + 1 = 1\\\\\n  \\mbox{etc.} &\n\\end{align*}\\]\nWe can see that this will continue to give the alternating sequence \\(1,0,1,0,1,\\cdots\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#solution-of-the-homogeneous-lccde",
    "href": "05-dt-lccde.html#solution-of-the-homogeneous-lccde",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.3 Solution of the homogeneous LCCDE",
    "text": "5.3 Solution of the homogeneous LCCDE\nNote the iterative solution does not give us (directly) and analytical expression for the output at arbitrary \\(n\\). We have to start at the initial conditions and compute our way up to \\(n\\). We now consider an analytical solution when the input is zero, the solution to the homogeneous difference equation \\[Q(E)\\, y = a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = 0 \\; .\\] given \\(N\\) sequential auxiliary conditions on \\(y\\).\nSimilar to differential equations, the homogeneous solution depends on the roots of the characteristic equation \\(Q(E)=0\\) whose roots are either real or occur in complex conjugate pairs. Let \\(\\lambda_i\\) be the \\(i\\)-th root of \\(Q(E) = 0\\), then the solution is of the form \\[y[n] = \\sum\\limits_{i=1}^N C_i \\lambda_i^{n}\\] where the parameters \\(C_i\\) are determined from the auxiliary conditions.\nFor a real system (when the coefficients of the difference equation are real) and when the roots are complex \\(\\lambda_{1,2} = |\\lambda|e^{\\pm j\\beta}\\), it is cleaner to assume a form for those terms as \\[y[n] = C |\\lambda|^n\\cos(\\beta n + \\theta)\\] for constants \\(C\\) and \\(\\theta\\).\n\n\nExample\n\n\nFind the solution to the first-order homogeneous LCCDE \\[y[n+1] + \\frac{1}{2}y[n] = 0 \\mbox{ with } y[0] = 5 \\; .\\] Note \\(Q(E) = E + \\frac{1}{2}\\) has a single root \\(\\lambda_1 = -\\frac{1}{2}\\). Thus the solution is of the form \\[y[n] = C\\left( -\\frac{1}{2}\\right)^n\\] where the parameter \\(C\\) is found using \\[y[0] = C = 5\\] to give the final solution \\[y[n] = 5\\left( -\\frac{1}{2}\\right)^n\\]\n\n\n\n\nExample\n\n\nFind the solution to the second-order homogeneous LCCDE \\[y[n+2] + y[n+1] + \\frac{1}{2}y[n] = 0 \\mbox{ with } y[0] = 1 \\mbox{ and } y[1] = 0\\; .\\] Note \\(Q(E) = E^2 + E + \\frac{1}{2}\\) has a pair of complex roots \\(\\lambda_{1,2} = -\\frac{1}{2} \\pm j\\frac{1}{2}\\). Thus the solution is of the form \\[y[n] = C \\left|\\frac{1}{\\sqrt{2}}\\right|^n\\cos\\left(\\frac{3\\pi}{4} n + \\theta\\right)\\] where the parameters are found using \\[y[0] = C\\cos\\left(\\theta\\right) = 1\\] \\[y[1] = C\\frac{1}{\\sqrt{2}}\\cos\\left(\\frac{3\\pi}{4} + \\theta\\right) = 0\\] This is true when \\[C = \\sqrt{2} \\mbox{ and } \\theta = -\\frac{\\pi}{4} + 2\\pi m\\] for any \\(m\\in \\mathbb{Z}\\) since \\(\\cos\\) is periodic in \\(2\\pi\\). A final solution is then \\[y[n] = \\sqrt{2} \\left|\\frac{1}{\\sqrt{2}}\\right|^n\\cos\\left(\\frac{3\\pi}{4} n - \\frac{\\pi}{4}\\right)\\]\n\n\nSee the appendix for a general technique to solve for these constants.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#impulse-response-from-lccde",
    "href": "05-dt-lccde.html#impulse-response-from-lccde",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.4 Impulse response from LCCDE",
    "text": "5.4 Impulse response from LCCDE\nToday our goal is to find the solution to \\(Q(E)y=P(E)x\\) when \\(x[n] = \\delta[n]\\) assuming \\(y[n] = 0\\) for \\(n &lt; 0\\), giving the impulse response \\(y[n] = h[n]\\). We skip the derivation here and just give a procedure.\nStep 1: Let \\(y_h\\) be the homogeneous solution to \\(Q(E)y_h=0\\) for \\(n &gt; N\\).\nStep 2: Assume a form for \\(h[n]\\) given by \\[h[n] = \\frac{b_N}{a_N}\\delta[n] + y_h[n]u[n]\\]\nStep 3: Using the iterative procedure above find the \\(N\\) auxiliary conditions we need by,\n\nfirst, rewrite the equation in delay form and solve for \\(y[n]\\),\nthen let \\(x[n] = \\delta[n]\\) and manually compute \\(h[0]\\) assuming \\(h[n] = 0\\) for \\(n &lt; 0\\),\nrepeating the previous step for \\(h[1]\\), continuing up to \\(h[N-1]\\).\n\nStep 4: Using the auxillary conditions in step 3, solve for the constants in the solution \\(h[n]\\) from step 2.\n\n\nExample\n\n\nFind the impulse response of the system given by \\[y[n+2] -\\frac{1}{4}y[n+1] -\\frac{1}{8}y[n]= 2x[n+1]\\]\nFor step 1 we solve the equation \\[y_h[n+2] -\\frac{1}{4}y_h[n+1] -\\frac{1}{8}y_h[n] = 0\\] which is of the form \\[y_h[n] = C_1 \\left( -\\frac{1}{4}\\right)^n + C_2 \\left( \\frac{1}{2}\\right)^n\\] since the roots of \\(Q(E) = E^2 - \\frac{1}{4}E - \\frac{1}{8}\\) are \\(-\\frac{1}{4}\\) and \\(\\frac{1}{2}\\).\nFor step 3, we find the auxiliary conditions needed to find \\(C_1\\) and \\(C_2\\) by rewriting the original equation in delay form and solving for \\(y[0]\\) and \\(y[1]\\) when \\(x[n] = \\delta[n]\\). \\[y[n] = \\frac{1}{4}y[n-1] + \\frac{1}{8}y[n-2] + 2x[n-1]\\] Let \\(x[n] = \\delta[n]\\) and manually compute \\(y[0]\\) assuming \\(y[n] = 0\\) for \\(n &lt; 0\\) \\[y[0] = \\frac{1}{4}\\underbrace{y[0-1]}_{0} + \\frac{1}{8}\\underbrace{y[0-2]}_{0} + 2\\underbrace{\\delta[0-1]}_{0} = 0\\] Repeat for \\(y[1]\\) \\[y[1] = \\frac{1}{4}\\underbrace{y[1-1]}_{0} + \\frac{1}{8}\\underbrace{y[1-2]}_{0} + 2\\underbrace{\\delta[1-1]}_{1} = 2\\] Now we find the constants using step 4 \\[h[0] = C_1  + C_2  = 0\\] \\[h[1] = C_1 \\left( -\\frac{1}{4}\\right) + C_2 \\left( \\frac{1}{2}\\right) = 2\\] which gives \\(C_1 = -\\frac{8}{3}\\) and \\(C_2 = \\frac{8}{3}\\). Thus the final impulse response is \\[h[n] = \\frac{b_N}{a_N}\\delta[n] + y_h[n]u[n] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^nu[n] + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^n u[n]\\] since \\(b_N = 0\\).\n\n\nNote we can confirm our closed-form result in the previous example, for a few values of \\(n\\), by iteratively solving the difference equation \\[h[0] = \\frac{1}{4}\\underbrace{h[0-1]}_{0} + \\frac{1}{8}\\underbrace{h[0-2]}_{0} + 2\\underbrace{\\delta[0-1]}_{0} = 0\\] \\[h[1] = \\frac{1}{4}\\underbrace{h[1-1]}_{0} + \\frac{1}{8}\\underbrace{h[1-2]}_{0} + 2\\underbrace{\\delta[1-1]}_{1} = 2\\] \\[h[2] = \\frac{1}{4}\\underbrace{h[2-1]}_{2} + \\frac{1}{8}\\underbrace{h[2-2]}_{0} + 2\\underbrace{\\delta[2-1]}_{0} = \\frac{1}{2}\\] \\[h[3] = \\frac{1}{4}\\underbrace{h[3-1]}_{\\frac{1}{2}} + \\frac{1}{8}\\underbrace{h[3-2]}_{2} + 2\\underbrace{\\delta[2-1]}_{0} = \\frac{3}{8}\\] and comparing to our closed-form solution a the same values of \\(n\\) \\[h[0] = -\\frac{8}{3} + \\frac{8}{3} = 0\\] \\[h[1] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right) + \\frac{8}{3}\\left( \\frac{1}{2}\\right) = 2\\] \\[h[2] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^2 + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^2 = \\frac{1}{2}\\] \\[h[3] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^3 + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^3 = \\frac{3}{8}\\]\n\n\nExample\n\n\nFind the impulse response of the system given by \\[y[n+1] - \\frac{1}{2}y[n] = x[n+1] + x[n]\\]\nIn step 1 we note the solution to \\(Q(E)y[n] = 0\\) is of the form \\[y_h[n] = C\\left( \\frac{1}{2}\\right)^n\\] From step 2 we note \\(b_N = 1\\) and \\(a_N = -\\frac{1}{2}\\), so that \\[h[n] = -2\\delta[n]  +  C\\left( \\frac{1}{2}\\right)^n\\, u[n]\\] In step 3 we manually find \\(h[0]\\)\n\\[\\begin{align*}\n    y[n] &= \\frac{1}{2}y[n-1] + x[n] + x[n-1]\\\\\n    h[n] &= \\frac{1}{2}y[n-1] + \\delta[n] + \\delta[n-1]\\\\\n    h[0] &= 0 + 1 + 0 = 1  \n\\end{align*}\\]\nAnd in step 4 we solve for \\(C\\) \\[h[0] = -2  +  C = 1 \\mbox{ implies } C = 3\\] to give \\[h[n] = -2\\delta[n]  +  3\\left( \\frac{1}{2}\\right)^n\\, u[n]\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html",
    "href": "06-ct-lti.html",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "",
    "text": "6.1 System types\nToday’s topic is our introduction to CT systems and the important case of CT Linear, Time-Invariant Systems.\nA system is an interconncted set of components or sub-systems. Mathematically a system is a transformation, \\(T\\), between one or more signals, a rule that maps functions to functions.\nWe will focus on single input - single output, CT and DT systems.\nAs a shorthand notation for the graphical description above we can use \\(x \\mapsto y\\). A system maps a function \\(x\\) to a function \\(y\\):\nWhen a system has no input, the system is autonomous. An autonomous system just produces output: \\(\\mapsto y\\).\nWe can think of an autonomous system as a function generator, producing signals for use, or as modeling a measurement process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#system-types",
    "href": "06-ct-lti.html#system-types",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "",
    "text": "single input - single output (SISO) system.\n\n\n\nSISO Block Diagram\n\n\nsingle input - multiple output (SIMO) system\n\n\n\nSIMO Block Diagram\n\n\ngeneral case, multiple input - multiple output (MIMO)\n\n\n\nMIMO Block Diagram\n\n\n\n\n\nIf both input and output are CT signals, it is a CT system.\n\n\n\nGeneric Block Diagram of CT System\n\n\nIf both input and output are DT signals, it is a DT system.\n\n\n\nGeneric Block Diagram of a DT System\n\n\nIf input and output are not both CT or DT signals, it is a hybrid CT-DT system.\n\n\n\nGeneric Block Diagram of a Hybrid DT/CT System\n\n\n\n\n\nGeneric Block Diagram of a Hybrid CT/DT System\n\n\n\n\n\nCT system \\[x(t) \\mapsto y(t)\\]\nDT system \\[x[n] \\mapsto y[n]\\]\nHybrid CT-DT system \\[x[n] \\mapsto y(t)\\]\nor\n\\[x(t) \\mapsto y[n]\\]\n\n\n\n\n\nGeneric Block Diagram of an Autonomous System",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#ct-system-representations",
    "href": "06-ct-lti.html#ct-system-representations",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.2 CT system representations",
    "text": "6.2 CT system representations\nWe can mathematically represent, or model, systems multiple ways.\n\npurely mathematically - in time domain we will use\n\nfor CT systems: linear, constant coefficient differential equations. e.g. \\[y^{\\prime\\prime} + ay^\\prime + by = x\\]\nfor DT systems: linear, constant coefficient difference equation, e.g. \\[y[n] = a y[n-1] + b y[n-2] + x[n]\\]\n\nor\n\nfor CT systems: CT impulse response\nfor DT systems: DT impulse response\n\npurely mathematically - in frequency domain we will use\n\nfrequency response\ntransfer function (complex frequency, covered in ECE 3704)\n\ngraphically, using a mixture of math and block diagrams\n\nMathematical models:\n\nprovide abstraction, removing (often) irrelevant detail.\ncan be more or less detailed, an internal v.s. external (block box) description\nare not unique with respect to instantiation (implementation)\nare limited to the regime they were designed for\n\n\n\nExample\n\n\nConsider the RC circuit. It is a single input - single output system. We will be able to represent it mathematically or graphically and internally or externally.\n\nExternal - Graphical\n\n\n\nExternal Model\n\n\nExternal - Symbolic\n\\(y(t) = h(t)*x(t)\\)\nInternal - Graphical\n\n\n\nInternal Model\n\n\nInternal - Symbolic\n\\(y^\\prime + \\frac{1}{RC} y = \\frac{1}{RC} x(t)\\)\n\n\n\nNote: internal models usually have several paramters (the resistor and capacitor values in the example above), while the external model does not. Thus another term for external model is a lumped parameter model.\nIt does not matter what the underlying system implementation is. For example, consider a mechanical system, described by a second-order ODE:\n\n\n\nMechanical Diagram\n\n\n\n\n\n\\(y\\) = position\n\\(M\\) = mass\n\n\n\\(y^\\prime\\) = velocity\n\\(K\\) = spring constant\n\n\n\\(y^{\\prime\\prime}\\) = acceleration\n\\(B\\) = coefficient of friction\n\n\n\n\\[y^{\\prime\\prime} + \\frac{B}{M} y^\\prime + \\frac{K}{M}y = \\frac{1}{M}f(t)\\]\nCompare this to the parallel RLC circuit, described by the second-order ODE:\n\n\n\nCircuit Diagram\n\n\n\n\n\n\\(y\\) = voltage\n\\(R\\) = resistance\n\n\n\\(Cy^\\prime\\) = capacitor current\n\\(L\\) = inductance\n\n\n\n\\(C\\) = capacitance\n\n\n\n\\[y^{\\prime\\prime} + \\frac{1}{RC} y^\\prime + \\frac{1}{LC}y = \\frac{1}{LC}f(t)\\]\nComparing these systems, if \\(R = \\frac{1}{B}\\), \\(L = \\frac{1}{K}\\), and \\(C = M\\), they are mathematically identical.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#system-properties-and-classification",
    "href": "06-ct-lti.html#system-properties-and-classification",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.3 System properties and classification",
    "text": "6.3 System properties and classification\nChoosing the right kind of system model is important. Here are some important properties that allow us to broadly classify systems.\n\nMemory\nInvertability\nCausality\nStability\nTime-invariance\nLinearity\n\nLet’s define each it turn.\n\n6.3.1 Memory\nThe output of a system with memory depends on previous or future inputs and is said to be dynamic. Otherwise the system is memoryless or instantaneous, and the output \\(y(t)\\) at time \\(t\\) depends only on \\(x(t)\\). For example in CT: \\[y(t) = 2x(t)\\] is a memoryless system, while \\[y(t) = \\int\\limits_{-\\infty}^{t} x(\\tau) \\; dt\\] has memory.\n\n\n6.3.2 Invertability\nA system is invertable if there exists a system that when placed in series with the original recovers the input. \\[x(t) \\mapsto{T} y(t) \\mapsto{T^{-1}} x(t)\\] where \\(T^{-1}\\) is the inverse system of \\(T\\). For example, consider a system \\[x(t) \\mapsto y(t) = \\int\\limits_{-\\infty}^t x(\\tau) \\; d\\tau\\] and a system \\[y(t) \\mapsto z(t) = \\frac{dy}{dt}\\] The combination in series \\(x(t) \\mapsto y(t) \\mapsto z(t) = x(t)\\), i.e. the derivative undoes the integral.\n\n\n6.3.3 Causality\nA CT system is causal if the output at time \\(t\\) depends on the input for time values at or before \\(t\\): \\[y(t) \\;\\text{depends on}\\; x(\\tau) \\;\\text{for} \\; \\tau \\leq t\\] All physical CT systems are causal, even if all continuous systems are not (e.g. continuous 2D images \\(f(u,v)\\), have no \"before\" and \"after\").\nFor example, consider a CT system whose impulse response is \\(h(t) = e^{-t^2}\\). This implies the system produces output before (i.e. for \\(t &lt; 0\\)) the impulse is applied at \\(t=0\\), somehow anticipating the arrival of the impulse. Barring time-travel, this is physically impossible.\n\n\n6.3.4 Stability\nA CT system is (BIBO) stable if applying a bounded-input (BI) \\[\\left|x(t)\\right| &lt; \\infty \\; \\forall \\; t\\] results in a bounded-output (BO) \\(x(t) \\mapsto y(t)\\) and \\[\\left|y(t)\\right| &lt; \\infty \\; \\forall \\; t\\] Note, bounded in practice is limited by the physical situation, e.g. positive and negative rails in a physical circuit.\nFor example, a CT system described by the LCCDE \\[\\frac{dy}{dt}(t) - 2y(t) = x(t)\\] is unstable because the solution \\(y(t)\\) will have one term of the form \\(Ce^{2t}\\), for most non-zero inputs \\(x(t)\\) or any non-zero initial condition, that grows unbounded as time increases.\n\n\n6.3.5 Time-invariance\nA CT system is time-invariant if, given \\[x(t) \\mapsto y(t)\\] then a time-shift of the input leads to the same time-shift in the output \\[x(t-\\tau) \\mapsto y(t-\\tau)\\]\nAn important counterexample is a CT system described by a LCCDE, e.g. \\[\\frac{dy}{dt}(t) + y(t) = x(t)\\] but non-zero auxillary conditions at some \\(t_0\\), \\(y(t_0) = y_0 \\neq 0\\). Such systems will have a term in its solution that depends on \\(y_0\\). However if I time shift the input, the term that depends on \\(y_0\\) does not shift (since it is anchored to \\(t_0\\)) and the total output does not shift identically with the input. Thus the system cannot be time-invariant.\n\n\n6.3.6 Linearity\nA CT system is linear if the output due to a sum of scaled individual inputs is the same as the scaled sum of the individual outputs with respect to those inputs. In other words given \\[x_1(t) \\mapsto y_1(t) \\;\\text{and}\\; x_2(t) \\mapsto y_2(t)\\] then \\[a x_1(t) + b x_2(t) \\mapsto a y_1(t) + b y_2(t)\\] for constants \\(a\\) and \\(b\\). Note this property extends to sums of arbitrary signals, e.g. if \\[x_i(t) \\mapsto y_i(t) \\; \\forall\\; i \\in [1 \\cdots N]\\] then given \\(N\\) constants \\(a_i\\), if the system is linear \\[\\sum\\limits_{i = 1}^N a_i x_i(t) \\mapsto \\sum\\limits_{i = 1}^N a_i y_i(t)\\] This is a very important property, called superposition, and it simplifies the analysis of systems greatly.\nSimilar to time-invariance an important non-linear system is that is described by a LCCDE with non-zero auxillary conditions at some \\(t_0\\), \\(y(t_0) = y_0\\). Again such systems will have a term in it’s solution that depends on \\(y_0\\). Given two inputs, each individual response will have that term in it, so thier sum has double that term. However the response due to the sum of the inputs would again only have one and the sum of the responses would not be the same as the response of the sum. Such a system cannot be linear.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#stable-lti-systems",
    "href": "06-ct-lti.html#stable-lti-systems",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.4 Stable LTI Systems",
    "text": "6.4 Stable LTI Systems\nThe remainder of this course is about stable, linear, time-invariant (LTI) systems. As we have seen in CT such systems can be described by a LCCDE with zero auxillary (initial) conditions (the system is at rest).\nWe have seen previously how to find the impulse response, \\(h(t)\\), of such systems. We now note some relationships between the impulse response and the system properties described above.\n\nIf a system is memoryless then \\(h(t) = C \\delta(t)\\) for some constant \\(C\\).\nIf a system is causal then \\(h(t) = 0\\) for \\(t &lt; 0\\).\nIf a system is BIBO stable then \\[\\int\\limits_{-\\infty}^{\\infty} |h(t)| \\; dt &lt; \\infty\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html",
    "href": "07-dt-lti.html",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "",
    "text": "7.1 DT system representations\nToday’s topic is our introduction to systems and the important case of DT Linear, Time-Invariant Systems.\nWe can mathematically represent, or model, DT systems multiple ways.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#dt-system-representations",
    "href": "07-dt-lti.html#dt-system-representations",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "",
    "text": "purely mathematically - in time domain we will use\n\nlinear, constant coefficient difference equations, e.g. \\[y[n] = a y[n-1] + b y[n-2] + x[n]\\]\nDT impulse response \\(h[n]\\)\n\npurely mathematically - in frequency domain we will use\n\nfrequency response\ntransfer function (complex frequency, covered in ECE 3704)\n\ngraphically, using a mixture of math and block diagrams",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#system-properties-and-classification",
    "href": "07-dt-lti.html#system-properties-and-classification",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.2 System properties and classification",
    "text": "7.2 System properties and classification\nChoosing the right kind of system model is important. Here are some important properties that allow us to broadly classify systems.\n\nMemory\nInvertability\nCausality\nStability\nTime-invariance\nLinearity\n\nLet’s define each it turn.\n\n7.2.1 Memory\nThe output of a DT system with memory depends on previous or future inputs and is said to be dynamic. Otherwise the system is memoryless or instantaneous, and the output \\(y[n]\\) at index \\(n\\) depends only on \\(x[n]\\). For example: \\[y[n] = 2x[n]\\] is a memoryless system, while \\[y[n+1] + y[n] = x[n]\\] has memory. To see this, write the difference equation in recursive form \\[y[n] = -y[n-1] + x[n-1]\\] and we see explicitly the current output \\(y[n]\\) depends on past values of output and input.\n\n\n7.2.2 Invertability\nA system is invertible if there exists a system that when placed in series with the original recovers the input. \\[x[n] \\mapsto{T} y[n] \\mapsto{T^{-1}} x[n]\\] where \\(T^{-1}\\) is the inverse system of \\(T\\). For example, consider a system \\[x[n] \\mapsto y[n] = \\sum\\limits_{m=-\\infty}^{n} x[m]\\] and a system \\[y[n] \\mapsto z[n] = y[n] - y[n-1]\\] The combination in series \\(x[n] \\mapsto y[n] \\mapsto z[n] = x[n]\\), since \\[z[n] = y[n] - y[n-1] = \\sum\\limits_{m=-\\infty}^{n} x[m] - \\sum\\limits_{m=-\\infty}^{n-1} x[m] = x[n]\\] i.e. the difference undoes the accumulation.\n\n\n7.2.3 Causality\nA DT system is causal if the output at index \\(n\\) depends on the input for index values at or before \\(n\\): \\[y[n] \\;\\text{depends on}\\; x[m] \\;\\text{for} \\; m \\leq n\\] While all physical CT systems are causal, practical DT systems may not be since we can use memory to \"shift time\". For CT systems we cannot store the infinite number of values between two time points \\(t_1\\) and \\(t_2\\), but we can store the \\(n_2-n_1\\) values of a DT system between between two indices \\(n_1\\) and \\(n_2\\) (assuming infinite precision).\n\n\nExample\n\n\nConsider a DT system whose difference equation is \\[y[n] = -x[n-1] + 2x[n] - x[n+1]\\] We see the current output \\(y[n]\\) depends on a \"future\" value of the input \\(x[n+1]\\). Thus the system is not causal. In practice we can shift the difference equation to \\[y[n-1] = -x[n-2] + 2x[n-1] - x[n]\\] and then delay the output by one sample to get \\(y[n]\\).\n\n\n\n\nExample\n\n\nConsider a DT system whose difference equation is \\[y[n] = -y[n-1] + 2x[n]\\] We see the current output \\(y[n]\\) depends on a \"past\" value of the output \\(y[n-1]\\) and the current input \\(x[n]\\). Thus the system is causal. In practice we can immediately compute \\(y[n]\\) with no delay.\n\n\n\n\n7.2.4 Stability\nA DT system is (BIBO) stable if applying a bounded-input (BI) \\[\\left|x[n]\\right| &lt; \\infty \\; \\forall \\; n\\] results in a bounded-output (BO) \\(x[n] \\mapsto y[n]\\) and \\[\\left|y[n]\\right| &lt; \\infty \\; \\forall \\; n\\] Note, bounded in practice is limited by the physical situation, e.g. the number of bits used to store values.\nFor example, a DT system described by the LCCDE \\[y[n+1] - 2 y[n] = x[n+1]\\] is unstable because the solution \\(y[n]\\) will have one term of the form \\(\\left( 2\\right)^n\\), for most non-zero inputs \\(x[n]\\) or any non-zero initial condition, that grows unbounded as \\(n\\) increases.\n\n\n7.2.5 Time-invariance\nA DT system is time(index)-invariant if, given \\[x[n] \\mapsto y[n]\\] then an index-shift of the input leads to the same index-shift in the output \\[x[n-m] \\mapsto y[n-m]\\]\nAn important example is a DT system described by a LCCDE, e.g. \\[y[n+1] - \\frac{1}{2} y[n] = x[n+1]\\] or in recursive form \\[y[n] = \\frac{1}{2} y[n-1] + x[n]\\]\nIf we index shift the input \\(x[n - m]\\) we replace \\(n\\) by \\(n-m\\) and the difference equation becomes \\[y[n-m+1] - \\frac{1}{2} y[n-m] = x[n-m+1]\\] which has the same solution shifted by \\(m\\) \\[y[n-m] = \\frac{1}{2} y[n-m -1] + x[n-m]\\]\nIf a coefficient depends on \\(n\\) however, e.g \\[y[n+1] - \\frac{n}{2} y[n] = x[n+1]\\] so that it is no longer LCC then the solution depends on \\(m\\) and the system is no longer time-invariant.\n\n\n7.2.6 Linearity\nA DT system is linear if the output due to a sum of scaled individual inputs is the same as the scaled sum of the individual outputs with respect to those inputs. In other words given \\[x_1[n] \\mapsto y_1[n] \\;\\text{and}\\; x_2[n] \\mapsto y_2[n]\\] then \\[a x_1[n] + b x_2[n] \\mapsto a y_1[n] + b y_2[n]\\] for constants \\(a\\) and \\(b\\). Note this property extends to sums of arbitrary signals, e.g. if \\[x_i[n] \\mapsto y_i[n] \\; \\forall\\; i \\in [1 \\cdots N]\\] then given \\(N\\) constants \\(a_i\\), if the system is linear \\[\\sum\\limits_{i = 1}^N a_i x_i[n] \\mapsto \\sum\\limits_{i = 1}^N a_i y_i[n]\\] This is a very important property, called superposition, and it simplifies the analysis of systems greatly.\nAn important non-linear system is that is described by a LCCDE with non-zero auxiliary conditions at some \\(n_0\\), \\(y[n_0] = y_0\\). As in CT, such systems will have a term in it’s solution that depends on \\(y_0\\). Given two inputs, each individual response will have that term in it, so their sum has double that term. However the response due to the sum of the inputs would again only have one and the sum of the responses would not be the same as the response of the sum. Such a system cannot be linear. Thus the system must be \"at rest\" before applying the input in order to be a linear system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#stable-lti-systems",
    "href": "07-dt-lti.html#stable-lti-systems",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.3 Stable LTI Systems",
    "text": "7.3 Stable LTI Systems\nThe remainder of this course is about stable, linear, time-invariant (LTI) systems. As we have seen in DT such systems can be described by a LCCDE with zero auxiliary (initial) conditions (the system is at rest).\nWe have seen previously how to find the impulse response, \\(h[n]\\), of such systems. We now note some relationships between the impulse response and the system properties described above.\n\nIf a system is memoryless then \\(h[n] = C \\delta[n]\\) for some constant \\(C\\).\nIf a system is causal then \\(h[n] = 0\\) for \\(n &lt; 0\\).\nIf a system is BIBO stable then \\[\\sum\\limits_{-\\infty}^{\\infty} |h[n]| &lt; \\infty\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#difference-equations",
    "href": "07-dt-lti.html#difference-equations",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.4 Difference Equations",
    "text": "7.4 Difference Equations\n\nA difference equation is a relation among combinations of two DT functions and shifted versions of them.\nSimilar to differential equations where the solution is a CT function, the solution to a difference equation is a DT function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#definition-of-lccde",
    "href": "07-dt-lti.html#definition-of-lccde",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.5 Definition of LCCDE",
    "text": "7.5 Definition of LCCDE\nA linear, constant-coefficient, difference equation (LCCDE) comes in one of two forms.\n\nDelay form. \\[\\sum\\limits_{k = 0}^N a_k y[n-k] = \\sum\\limits_{k = 0}^M b_k x[n-k]\\] or \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\]\nAdvance form. Let \\(n\\rightarrow n+N\\), then the delay form becomes \\[\\sum\\limits_{k = 0}^N a_k y[n+N-k] = \\sum\\limits_{k = 0}^M b_k x[n+N-k]\\] or \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\]\nThe order of the system is given by \\(N\\).\nThe delay and advance forms are equivalent because the equation holds for any \\(n\\), and we can move back and forth between them as needed by a constant index-shift.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#delay-and-advance-operators",
    "href": "07-dt-lti.html#delay-and-advance-operators",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.6 Delay and Advance Operators",
    "text": "7.6 Delay and Advance Operators\n\nThe advance operator \\(E^m\\) shifts a DT function by positive \\(m\\), i.e. \\(E^m x[n] = x[n+m]\\)\nThe delat operator \\(D^m\\) shifts a DT function by negative \\(m\\), i.e. \\(D^m x[n] = x[n-m]\\).\nThe advance form of the difference equation using this operator notation is \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\] \\[a_0 E^Ny + a_1E^{N-1}y + \\cdots a_N y = b_0 E^{N}x + \\cdots b_M E^{N-M}x\\] Factoring out the advance operators gives \\[\\underbrace{\\left(a_0E^N + a_1E^{N-1} + \\cdots a_N\\right)}_{Q(E)} y = \\underbrace{\\left(b_0 E^{N} + \\cdots b_M E^{N-M}\\right)}_{P(E)} x\\] or \\[Q(E)y[n] = P(E)x[n]\\]\nThe delay form of the difference equation using this operator notation is \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] \\[a_0y[n] + a_1 Dy + \\cdots a_N D^N y = b_0 x + \\cdots b_MD^M x\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#iterative-solution-of-lccdes",
    "href": "07-dt-lti.html#iterative-solution-of-lccdes",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.7 Iterative solution of LCCDEs",
    "text": "7.7 Iterative solution of LCCDEs\n\nDifference equations can be solved by manually running the equation forward using previous values of the output and current and previous values of the input, given some initial conditions.\nThis is called an iterative solution for this reason.\nTo perform an iterative solution we need the difference equation in delay form \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] along with the previous output values and the previous and current input values.\nWe then solve for the current output \\(y[n]\\) \\[y[n] =  - \\left(\\frac{a_1}{a_0}y[n-1] + \\cdots \\frac{a_N}{a_0} y[n-N]\\right) + \\frac{b_0}{a_0} x[n] + \\cdots \\frac{b_M}{a_0}x[n-M]\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html",
    "href": "08-ct-conv.html",
    "title": "8  CT Convolution",
    "section": "",
    "text": "8.1 Review CT LTI systems and superposition property\nRecall the superposition property of LTI systems. If a CT system is LTI then the superposition property holds. Given a system where \\[x_i(t) \\mapsto y_i(t) \\; \\forall\\; i\\] then \\[\\sum\\limits_{i} a_i x_i(t) \\mapsto \\sum\\limits_{i} a_i y_i(t)\\]\nSuperposition enables a powerful problem reduction strategy. The overall idea for is that if:\nThis will be a recurring pattern in this course. In this lecture, the simple signals are weighted, time shifts of one signal, the delta function, \\(\\delta(t)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#review-ct-lti-systems-and-superposition-property",
    "href": "08-ct-conv.html#review-ct-lti-systems-and-superposition-property",
    "title": "8  CT Convolution",
    "section": "",
    "text": "we can write an arbitrary signal as a sum of simple signals, and\nwe can determine the response to the simple signals, then\nwe can easily express the output due to the input using superposition",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#convolution-integral",
    "href": "08-ct-conv.html#convolution-integral",
    "title": "8  CT Convolution",
    "section": "8.2 Convolution Integral",
    "text": "8.2 Convolution Integral\nTo derive this we start with the sifting property of the CT impulse function (from chapter 2) \\[\\int\\limits_{a}^{b} x(t)\\delta(t-t_0) \\; dt = x(t_0)\\] for any \\(a &lt; t_0 &lt; b\\). A slight change of variables (\\(t_0 \\rightarrow \\tau\\)) and limits (\\(a \\rightarrow -\\infty\\) and \\(b \\rightarrow \\infty\\)) gives: \\[x(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)\\delta(t-\\tau) \\; d\\tau\\] showing that we can write any CT signal as an infinite sum (integral) of weighted and time-shifted impluse functions.\nLet \\(h(t)\\) be the CT impulse response, the output due to the input \\(\\delta(t)\\), i.e. \\(\\delta(t) \\mapsto h(t)\\). Then if the system is time-invariant: \\(\\delta(t-\\tau) \\mapsto h(t-\\tau)\\) and by superposition if the input is writen as \\[x(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)\\delta(t-\\tau) \\; d\\tau\\] then the output is given by \\[y(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau) \\; d\\tau = x(t) * h(t)\\] This is called the convolution integral .\nIt is worth pausing here to see the signifigance. For a LTI CT system, if I know its impulse response \\(h(t)\\), I can find the response due to any input using convolution. For this reason the impulse response is another way to represent an LTI system.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#graphical-view-of-the-convolution-integral.",
    "href": "08-ct-conv.html#graphical-view-of-the-convolution-integral.",
    "title": "8  CT Convolution",
    "section": "8.3 Graphical View of the Convolution Integral.",
    "text": "8.3 Graphical View of the Convolution Integral.\nLet us break the convolution expression down into pieces. In its general form the convolution of two signals \\(x_1(t)\\) and \\(x_2(t)\\) is \\[x_1(t) * x_2(t) = \\int\\limits_{-\\infty}^{\\infty} x_1(\\tau)x_2(t-\\tau) \\; d\\tau\\]\nSuppose \\(x_1(t)\\) and \\(x_2(t)\\) are signals that look like\n\n\n\nThe two signals being convolved.\n\n\nThen \\(x_1(\\tau)\\) and \\(x_2(-\\tau)\\) look like\n\n\n\nThe second signal reflected.\n\n\nThe signal \\(x_2(t-\\tau)\\) is \\(x_2(-\\tau)\\) shifted by \\(t\\) (since \\(x_2(-\\tau+t)= x_2(t-\\tau)\\)) and then looks like\n\n\n\nThe second reflected signal, shifted.\n\n\nThen the integrand of convolution is the product \\(x_1(\\tau)x_2(t-\\tau)\\) whose plot depends of the value of \\(t\\). Some examples, where the individual signals are dashed and their product is in bold:\n\n\n\nThe product of the two signals under the convolution integrand.\n\n\nThen convolution is the total integral of the product (bold curves above) for that value of \\(t\\). For the example above we see the integral will be zero for \\(t\\) less than \\(t_0\\) since the two signals do not overlap and their product is zero. For \\(t_0 &lt; t &lt; t_1\\) the signals overap and the product is non-zero, and the effective bounds of integration are \\([t_0,t]\\). For \\(t &gt; t_1\\) the signals again overap and the product is non-zero, but the effective bounds of integration are \\([t_0,t_1]\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#examples-of-ct-convolution",
    "href": "08-ct-conv.html#examples-of-ct-convolution",
    "title": "8  CT Convolution",
    "section": "8.4 Examples of CT Convolution",
    "text": "8.4 Examples of CT Convolution\n\n\nExample\n\n\nConsider the convolution of two unit step functions. \\[u(t) * u(t) = \\int\\limits_{-\\infty}^{\\infty} u(\\tau)u(t-\\tau) \\; d\\tau\\] The product \\(u(\\tau) u(t-\\tau)\\) is non-zero only when \\(t\\geq 0\\) as illustrated here\n\n\n\nThe product of the two step signals under the convolution integrand.\n\n\nThe convolution integral is then the shaded area \\[u(t) * u(t) = \\left\\{ \\begin{array}{lc}\n  0 & t&lt; 0\\\\\n  \\int\\limits_{0}^{t} d\\tau = t  & t \\geq 0\\\\\n\\end{array}\\right.\\] Combining this back into a single expression gives: \\[u(t) * u(t) = tu(t)\\] Thus the convolution of two step signals is a ramp signal.\n\n\n\n\nExample\n\n\nLet \\(x_1(t) = u(t)\\) and \\(x_2(t) = e^{-at}u(t)\\) for constant \\(a\\in\\mathbb{C}\\), then \\[u(t) * e^{-at}u(t) = \\int\\limits_{-\\infty}^{\\infty} u(\\tau)e^{-a(t-\\tau)}u(t-\\tau) \\; d\\tau\\] Similar to the previous example, the product \\(u(\\tau) e^{-a(t-\\tau)} u(t-\\tau)\\) is non-zero only when \\(t\\geq 0\\)\n\n\n\nThe product of the two step signals under the convolution integrand.\n\n\nThe convolution integral is then the shaded area \\[u(t) * e^{-at}u(t) = \\left\\{ \\begin{array}{lc}\n  0 & t&lt; 0\\\\\n  \\int\\limits_{0}^{t} e^{-a(t-\\tau)} d\\tau = \\frac{1-e^{-at}}{a}  & t \\geq 0\\\\\n\\end{array}\\right.\\] Combining this back into a single expression gives: \\[u(t) * e^{-at}u(t) = \\frac{1-e^{-at}}{a}u(t)\\]\n\n\n\n\nExample\n\n\nLet \\(x_1(t) = \\delta(t)\\) and \\(x_2(t)\\) be an arbitrary signal. Then \\[\\delta(t) * x_2(t) = \\int\\limits_{-\\infty}^{\\infty} \\delta(\\tau)x_2(t-\\tau) \\; d\\tau\\] By the sifting property of the delta function this evaluates to \\[\\delta(t) * x_2(t) = x_2(t)\\] or in other words convolution with a delta function just results in the signal it was convolved with. That is it acts like the identity function, with respect to convolution.\n\n\nThe appendix lists several CT convolution results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#properties-of-ct-convolution",
    "href": "08-ct-conv.html#properties-of-ct-convolution",
    "title": "8  CT Convolution",
    "section": "8.5 Properties of CT Convolution",
    "text": "8.5 Properties of CT Convolution\nThere are several useful properties of convolution. We do not prove these here, but it is not terribly difficult to do so. Given signals \\(x_1(t)\\), \\(x_2(t)\\), and \\(x_3(t)\\):\n\nCommunative Property\n\nThe ordering of the signals does not matter. \\[x_1(t) * x_2(t) = x_2(t) * x_1(t)\\]\n\nDistributive Property\n\nConvolution is distributed over addition. \\[x_1(t) * \\left[x_2(t) + x_3(t)\\right] = \\left[x_1(t) * x_2(t) \\right] + \\left[x_1(t) * x_3(t) \\right]\\]\n\nAssociative Property\n\nThe order of convolution does not matter. \\[x_1(t) * \\left[x_2(t) * x_3(t)\\right] = \\left[x_1(t) * x_2(t) \\right] * x_3(t)\\]\n\nTime Shift\n\nGiven \\(x_3(t) = x_1(t) * x_2(t)\\) then for time shifts \\(\\tau_1, \\tau_2 \\in \\mathbb{R}\\) \\[x_1(t-\\tau_1) * x_2(t-\\tau_2) = x_3(t-\\tau_1 - \\tau_2)\\]\n\nMultiplicative Scaling\n\nGiven \\(x_3(t) = x_1(t) * x_2(t)\\) then for constants \\(a,b \\in \\mathbb{C}\\) \\[\\left[a\\, x_1(t)\\right] * \\left[b\\, x_2(t)\\right] = a\\, b\\, x_3(t)\\]\n\n\nThese properties can be used in combination with a table like that above to compute the convolution of a wide variety of signals without evaluating the integrals.\n\n\nExample\n\n\nHere is a simple example. Let \\(x_1(t) = e^tu(t)\\) and \\(x_2(t) = 2\\delta(t) + 5e^{-3t}u(t)\\). \\[x_1(t) * x_2(t) =  e^tu(t) * \\left[2\\delta(t) + 5e^{-3t}u(t)\\right]\\] Using the distributive property \\[x_1(t) * x_2(t) =  2\\left[\\delta(t) * e^tu(t)\\right]  + 5\\left[e^tu(t) * e^{-3t}u(t)\\right]\\] Using previously derived results involving the delta function and the table row 3 \\[x_1(t) * x_2(t) = 2 e^t\\, u(t) + 5\\left[ \\frac{e^t-e^{-3t}}{4}\\right]u(t)\\] Doing some simplification gives the result \\[x_1(t) * x_2(t) = \\left[ \\frac{13}{4}e^t-\\frac{5}{4}e^{-3t}\\right]u(t)\\]\n\n\n\n\nExample\n\n\nHere is a more complicated example. Let \\(x_1(t) = 2e^{-5t}u(t-1)\\) and \\(x_2(t) = \\left(1-e^{-t}\\right)u(t)\\). \\[x_1(t) * x_2(t) = \\left[2e^{-5t}u(t-1)\\right] * \\left[\\left(1-e^{-t}\\right)u(t)\\right]\\] We first rewrite \\(e^{-5t}u(t-1)=e^{-5}e^{-5(t-1)}u(t-1) = e^{-5}e^{-5t}u(t)\\Big|_{t=t-1}\\) so that we can remove the time shift \\[x_1(t) * x_2(t) = 2e^{-5}\\left[e^{-5t}u(t)\\right] * \\left[\\left(1-e^{-t}\\right)u(t)\\right]\\Big|_{t=t-1}\\] We now apply the distributive property \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\left(e^{-5t}u(t) * u(t)\\right) - \\left(e^{-5t}u(t)* e^{-t}u(t)\\right)\\right]\\Big|_{t=t-1}\\] Using the table rows 1 and 3 we get \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5}\\left(1-e^{-5t}\\right)u(t) + \\frac{1}{4}\\left(e^{-5t} - e^{-t}\\right)u(t)\\right]\\Big|_{t=t-1}\\] Combining terms we simplify to \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5} - \\frac{1}{4}e^{-t} + \\frac{1}{20}e^{-5t} \\right]u(t)\\Big|_{t=t-1}\\] Replacing the time shift gives the final result \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5} - \\frac{1}{4}e^{-(t-1)} + \\frac{1}{20}e^{-5(t-1)} \\right]u(t-1)\\] which can be cleaned up a bit more by distributing the leading term \\[x_1(t) * x_2(t) =\\left[\\frac{2}{5}e^{-5} -\\frac{1}{2}e^{-(t+4)} +\\frac{1}{10}e^{-5t}\\right]u(t-1)\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html",
    "href": "09-dt-conv.html",
    "title": "9  DT Convolution",
    "section": "",
    "text": "9.1 Review DT LTI systems and superposition property\nRecall the superposition property of LTI systems. If a DT system is LTI then the superposition property holds. Given a system where \\[x_i[n] \\mapsto y_i[n] \\; \\forall\\; i\\] then \\[\\sum\\limits_{i} a_i x_i[n] \\mapsto \\sum\\limits_{i} a_i y_i[n]\\]\nAs in CT we can use superposition to enable a problem reduction strategy in DT systems, where we write the input as a weighted sum of simple signals. In this lecture, the simple signals are weighted, time shifts of one signal, the DT delta function, \\(\\delta[n]\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html#convolution-sum",
    "href": "09-dt-conv.html#convolution-sum",
    "title": "9  DT Convolution",
    "section": "9.2 Convolution Sum",
    "text": "9.2 Convolution Sum\nTo derive this we start with the sifting property of the DT impulse function (from lecture 3) \\[\\sum\\limits_{a}^{b} x[n]\\delta[n-n_0] = x[n_0]\\] for any \\(a &lt; n_0 &lt; b\\). A slight change of variables (\\(n_0 \\rightarrow m\\)) and limits (\\(a \\rightarrow -\\infty\\) and \\(b \\rightarrow \\infty\\)) gives: \\[x[n] = \\sum\\limits_{m = -\\infty}^{\\infty} x[m]\\delta[n-m]\\] showing that we can write any DT signal as an infinite sum of weighted and time-shifted impluse functions.\nLet \\(h[n]\\) be the DT impulse response, the output due to the input \\(\\delta[n]\\), i.e. \\(\\delta[n] \\mapsto h[n]\\). Then if the system is time-invariant: \\(\\delta[n-m] \\mapsto h[n-m]\\) and by superposition, if the input is writen as \\[x[n] = \\sum\\limits_{m = -\\infty}^{\\infty} x[m]\\delta[n-m]\\] then the output is given by \\[y[n] = \\sum\\limits_{m = -\\infty}^{\\infty} x[m]h[n-m] = x[n] * h[n]\\] This is called the convolution sum .\nThe significance is similar to that in CT convolution. For a LTI DT system, if I know its impulse response \\(h[n]\\), I can find the response due to any input using convolution. For this reason the impulse response is another way to represent an LTI system.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html#graphical-view-of-the-convolution-sum.",
    "href": "09-dt-conv.html#graphical-view-of-the-convolution-sum.",
    "title": "9  DT Convolution",
    "section": "9.3 Graphical View of the Convolution Sum.",
    "text": "9.3 Graphical View of the Convolution Sum.\nAs in CT, let us break the convolution expression down into pieces. In its general form the convolution of two signals \\(x_1[n]\\) and \\(x_2[n]\\) is \\[x_1[n] * x_2[n] = \\sum\\limits_{m = -\\infty}^{\\infty} x_1[m]x_2[n-m]\\]\nSuppose \\(x_1[n]\\) and \\(x_2[n]\\) are signals that look like\n\n\n\nThe two signals being convolved.\n\n\nThen \\(x_1[m]\\) and \\(x_2[-m]\\) look like\n\n\n\nThe second signal reflected.\n\n\nThe signal \\(x_2[n-m]\\) is \\(x_2[-m]\\) shifted by \\(n\\) (since \\(x_2[-m+n]= x_s[n-m]\\)) and looks like\n\n\n\nThe second signal reflected and shifted.\n\n\nThen the terms of the convolution sum is the product \\(x_1[m]x_2[n-m]\\) whose plot depends of the value of \\(n\\). Some examples, where the individual signals are in grey and their product is in bold:\n\n\n\nThe product of the two signals under the convolution sum.\n\n\nThen convolution is the total sum of the product (bold plots above) for that value of \\(n\\). For the example above we see the sum will be zero for \\(n\\) less than \\(n_0\\) since the two signals do not overlap and their product is zero. For \\(n_0 \\leq n \\leq n_1\\) the signals overap and the product is non-zero, and the effective bounds of summation are \\([n_0,n]\\). For \\(n &gt; n_1\\) the signals again overap and the product is non-zero, but the effective bounds of summation are \\([n_0,n_1]\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html#dt-convolution-of-finite-length-signals",
    "href": "09-dt-conv.html#dt-convolution-of-finite-length-signals",
    "title": "9  DT Convolution",
    "section": "9.4 DT Convolution of Finite-Length Signals",
    "text": "9.4 DT Convolution of Finite-Length Signals\nFor finite-length signals, DT convolution gives us an algorithm to determine their convolution. Suppose the signal \\(x_1\\) is non-zero only over the interval \\([N_1,M_1]\\), and the signal \\(x_2\\) is non-zero only over the interval \\([N_2,M_2]\\). The length of the signals are \\(L_1 = M_1-N_1+1\\) and \\(L_2 = M_2-N_2+1\\) respectively. The non-zero terms of the convolution sum (when the signals overlap) is then the range \\([N_1+N_2,M_1+M_2]\\) and the sum can be truncated as:\n\\[x_1[n] * x_2[n] = \\sum\\limits_{m = N_1+N_2}^{M_1+M_2} x_1[m]x_2[n-m]\\]\nIt is common to shift both signals so that they both start at index \\(0\\) (in order to be represented as arrays in a zero-based index programming language like C or C++), zero-padding them both to have length \\(L=L_1+L_2-1\\) (zero-pad means to just add zero values to the end of the sequence). Then the convolution becomes \\[y = x_1 * x_2 = \\sum\\limits_{m = 0}^{L-1} x_1[m]x_2[n-m]\\] where the indexing of \\(x_2\\) is modulo the signal length, i.e. \\(x_2[(n-m) \\mbox{ mod } L]\\). The resulting signal after convolution, \\(y\\), is also of length \\(L\\), and can then be shifted back to start at \\(N_1+N_2\\).\n\n\nExample\n\n\nThe following C++ code computes the convolution of the DT signals \\(\\{1,-1,1\\}\\) and \\(\\{1,1,1,1\\}\\).\n  const unsigned int L = 6;\n  double x1[L] = {1., -1., 1., 0, 0, 0};\n  double x2[L] = {1., 1., 1., 1., 0, 0};\n  double y[L];\n\n  for(int n = 0; n &lt; L; n++){\n    double sum = 0.;\n    for(int m = 0; m &lt; L; m++){\n      int idx = (L+n-m) % L;\n      sum += x1[m]*x2[idx];\n    }\n    y[n] = sum;\n  }\nNote that \\(L_1 = 3\\), \\(L_2 = 4\\), so that \\(L=6\\).\n\n\nAn interesting aside, convolution of finite length signals is equivalent to multiplication of two polynomials, where the signal values are the coefficients.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html#examples-of-dt-convolution",
    "href": "09-dt-conv.html#examples-of-dt-convolution",
    "title": "9  DT Convolution",
    "section": "9.5 Examples of DT Convolution",
    "text": "9.5 Examples of DT Convolution\n\n\nExample\n\n\nConsider the convolution of two unit step functions: \\[u[n] * u[n] = \\sum\\limits_{m = -\\infty}^{\\infty} u[m]u[n-m]\\] Note for \\(n &lt; 0\\) the product of the signals \\(u[m]\\) and \\(u[n-m]\\) is zero as shown in the following figure\n\n\n\nThe product of the signals under the convolution sum when \\(n &lt; 0\\).\n\n\nso that the resulting sum is zero for any \\(n &lt; 0\\). For \\(n \\geq 0\\) the signals \\(u[m]\\) and \\(u[n-m]\\) overlap from \\(0\\) to \\(n\\) as shown below\n\n\n\nThe product of the signals under the convolution sum when \\(n \\geq 0\\).\n\n\nand the convolution sum is \\[\\sum\\limits_{m = 0}^{n} 1 = (n+1)\\] so that \\[u[n] * u[n] = \\left\\{ \\begin{array}{lc}\n    0 & n &lt; 0\\\\\n    n+1 & n \\geq 0\n  \\end{array}\n  \\right.\\] Putting the piecewise result into a single expression gives \\[u[n] * u[n] = (n+1)u[n]\\]\n\n\n\n\nExample\n\n\nConsider the convolution of a unit step and the function \\(\\gamma^n\\,u[n]\\) for some constant \\(\\gamma \\neq 1\\): \\[\\gamma^n\\, u[n] * u[n] = \\sum\\limits_{m = -\\infty}^{\\infty} \\gamma^{m}u[m]u[n-m]\\] Since both signals are multiplied by a step, the product of \\(\\gamma^{m}u[m]u[n-m]\\) is non-zero only for \\(0 \\leq m \\leq n\\) (for the same reason as in the previous example). Thus for \\(n \\geq 0\\) the convolution sum is: \\[\\sum\\limits_{m = 0}^{n} \\gamma^{m} = \\frac{\\gamma^{n+1}-1}{\\gamma-1} = \\frac{1-\\gamma^{n+1}}{1-\\gamma}\\] Putting the two piecewise results together gives \\[\\gamma^n\\, u[n] * u[n] = \\frac{1-\\gamma^{n+1}}{1-\\gamma}\\,u[n]\\]\n\n\n\n\nExample\n\n\nConsider the convolution of an arbitrary signal \\(x[n]\\) with the impulse function \\[x[n] * \\delta[n] = \\sum\\limits_{m = -\\infty}^{\\infty} x[m]\\delta[n-m]\\] By the sifting property we get \\[\\sum\\limits_{m = -\\infty}^{\\infty} x[m]\\delta[n-m] = x[n]\\] Thus the convolution with the impulse gives back the same signal (the \\(\\delta\\) is the identity system).\n\n\nTable [table:dtconv] lists several DT convolution results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "09-dt-conv.html#properties-of-dt-convolution",
    "href": "09-dt-conv.html#properties-of-dt-convolution",
    "title": "9  DT Convolution",
    "section": "9.6 Properties of DT Convolution",
    "text": "9.6 Properties of DT Convolution\nThere are several useful properties of convolution. We do not prove these here, but it is not terribly difficult to do so. Given signals \\(x_1[n]\\), \\(x_2[n]\\), and \\(x_3[n]\\):\n\nCommutative Property\n\nThe ordering of the signals does not matter. \\[x_1[n] * x_2[n] = x_2[n] * x_1[n]\\]\n\nDistributive Propery\n\nConvolution is distributed over addition. \\[x_1[n] * \\left(x_2[n] + x_3[n]\\right) = \\left(x_1[n] * x_2[n] \\right) + \\left(x_1[n] * x_3[n] \\right)\\]\n\nAssociative Property\n\nThe order of convolution does not matter. \\[x_1[n] * \\left(x_2[n] * x_3[n]\\right) = \\left(x_1[n] * x_2[n] \\right) * x_3[n]\\]\n\nIndex Shift\n\nGiven \\(x_3[n] = x_1[n] * x_2[n]\\) then for index shifts \\(m_1, m_2 \\in \\mathbb{R}\\) \\[x_1[n-m_1] * x_2[n-m_2] = x_3[n-m_1 - m_2]\\]\n\nMultiplicative Scaling\n\nGiven \\(x_3[n] = x_1[n] * x_2[n]\\) then for constants \\(a,b \\in \\mathbb{C}\\) \\[\\left(a\\, x_1[n]\\right) * \\left(b\\, x_2[n]\\right) = a\\, b\\, x_3[n]\\]\n\n\nThese properties can be used in combination with a table like that above to compute the convolution of a wide variety of signals without evaluating the summations.\n\n\nExample\n\n\nConsider the convolution of the causal DT pulse of length \\(N\\), \\(x_1[n] = u[n] - u[n-N]\\), and the signal \\(x_2[n] = \\left( \\frac{1}{2}\\right)^nu[n]\\).\n$$\n\\[\\begin{aligned}\n    x_1[n] * x_2[n] &= \\left( u[n] - u[n-N]\\right) * \\left( \\left( \\frac{1}{2}\\right)^nu[n] \\right)\\\\\n    &= \\left( u[n] \\right) * \\left( \\left( \\frac{1}{2}\\right)^nu[n] \\right) - \\left( u[n-N]\\right) * \\left( \\left( \\frac{1}{2}\\right)^nu[n] \\right) \\mbox{ using distributive property}\\\\\n    &= \\frac{1-\\left(\\frac{1}{2}\\right)^{n+1}}{1-\\left(\\frac{1}{2}\\right)}u[n] - \\frac{1-\\left(\\frac{1}{2}\\right)^{n+1}}{1-\\left(\\frac{1}{2}\\right)}u[n] \\Big|_{n\\rightarrow n-N} \\mbox{ from Table row 2 and index shift property}\\\\\n    &= \\frac{1-\\left(\\frac{1}{2}\\right)^{n+1}}{\\left(\\frac{1}{2}\\right)}u[n] - \\frac{1-\\left(\\frac{1}{2}\\right)^{n-N+1}}{\\left(\\frac{1}{2}\\right)}u[n-N]\\\\\n    &= \\frac{1-\\left(\\frac{1}{2}\\right)^{n+1}}{\\left(\\frac{1}{2}\\right)}u[n] - \\frac{1-\\left(\\frac{1}{2}\\right)^{-N}\\left(\\frac{1}{2}\\right)^{n+1}}{\\left(\\frac{1}{2}\\right)}u[n-N]\\\\\n    &= 2\\left(1-\\left(\\frac{1}{2}\\right)^{n+1}\\right)u[n] - 2\\left(1-\\left(\\frac{1}{2}\\right)^{-N}\\left(\\frac{1}{2}\\right)^{n+1} \\right)u[n-N]\\\\\n    &= \\left(2-\\left(\\frac{1}{2}\\right)^{n}\\right)u[n] - \\left(2-\\left(\\frac{1}{2}\\right)^{-N}\\left(\\frac{1}{2}\\right)^{n} \\right)u[n-N]\n  \n\\end{aligned}\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DT Convolution</span>"
    ]
  },
  {
    "objectID": "10-ct-block.html",
    "href": "10-ct-block.html",
    "title": "10  CT Block Diagrams",
    "section": "",
    "text": "10.1 The Four Basic Motifs\nUnderstanding complex systems, with many interconnections, is aided by graphical representations, generally called block diagrams [^1]. They are a hybrid graphical-analytical approach.\nThere are just four basic motifs needed to build any block diagram. Let \\(\\mathcal{S}_i\\) denote a (sub) system. Then the four motifs are:\nNote the feedback is negative (the minus sign on the feedback summation input). These can be use in various combinations, as we shall see shortly.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>CT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "10-ct-block.html#the-four-basic-motifs",
    "href": "10-ct-block.html#the-four-basic-motifs",
    "title": "10  CT Block Diagrams",
    "section": "",
    "text": "A single block.\n\n\n\n\nA block represeting a system/subsystem.\n\n\n\nA series connection of two blocks\n\n\n\n\nA block represeting a series connection of subsystems.\n\n\n\nA parallel connection of two blocks\n\n\n\n\nA block represeting a parallel connection of subsystems.\n\n\n\nA feedback connection\n\n\n\n\nA block represeting a feedback connection of subsystems.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>CT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "10-ct-block.html#connections-to-convolution",
    "href": "10-ct-block.html#connections-to-convolution",
    "title": "10  CT Block Diagrams",
    "section": "10.2 Connections to Convolution",
    "text": "10.2 Connections to Convolution\nEach subsystem, \\(\\mathcal{S}_i\\), can be represented by a basic time-domain operation (e.g. derivatives, integrals, addition, and scaling) or more generally by its impulse response \\(h_i(t)\\).  For example a block representing an system acting as integrator is typically drawn as\n\n\n\nCAPTION\n\n\nThis is equivalent to an impulse response \\(h(t) = u(t)\\) so that it might also be drawn as\n\n\n\nCAPTION\n\n\nWe can use the concept of convolution to connect block diagrams to the properties of convolution\n\nA single block is equivalent to convolution with the impulse response for that subsystem\n\n\n\n\nCAPTION\n\n\nUsing the associative property, a series connection of two blocks becomes\nwhich can be reduced to a single convolution \\(y(t) = h_3(t)*x(t)\\) where \\(h_3(t) = h_1(t)*h_2(t)\\).\nUsing the distributive property, a parallel connection of two blocks becomes\n\n\n\nCAPTION\n\n\nwhich is equivalent to a single convolution \\(y(t) = h_3(t)*x(t)\\) where \\(h_3(t) = h_1(t) + h_2(t)\\).\nIn the feedback connection let \\(w(t)\\) be the output of the summation\n\n\n\nCAPTION\n\n\nThen \\(y(t) = h_1(t)*w(t)\\) and \\(w(t) = x(t) - h_2(t)*y(t)\\). Substituting the later into the former gives \\(y(t) = h_1*(x-h_2(t)*y(t))\\). Using the distributive property we get \\(y(t) = h_1(t)*x(t) - h_1(t)*h_2(t)*y(t)\\). Isolating the input on the right-hand side and using \\(y(t) = \\delta(t)*y(t)\\) we get \\[y(t) + h_1(t)*h_2(t)*y(t) = \\left[\\delta(t) + h_1(t)*h_2(t)\\right]*y(t) = h_1(t)*x(t)\\] We can solve this for \\(y(t)\\) using the concept of inverse systems. Let \\(h_3(t)* \\left[\\delta(t) + h_1(t)*h_2(t)\\right]= \\delta(t)\\), i.e. \\(h_3\\) is the inverse system of \\(\\delta(t) + h_1(t)*h_2(t)\\). Then \\[y(t) = h_3(t)*h_1(t)*x(t)\\]\n\nRecall, when the system is instantaneous (memoryless) the impulse response is \\(a\\delta(t)\\) for some constant \\(a\\). This is the same as scaling the signal by \\(a\\). We typically drop the block in such cases and draw the input-output operation as\n\n\n\nCAPTION\n\n\nThese properties allow us to perform transformations, either breaking up a system into subsystems, or reducing a system to a single block.\n\nConsider a second-order system system with impulse response \\[h(t) = \\left(e^{-3t} - e^{-t}\\right)\\, u(t)\\] We can express this as a block diagram consisting of two parallel blocks\n\n\n\nCAPTION\n\n\n\n\nConsider a system with block diagram\n\n\n\nCAPTION\n\n\nWe can determine the overall impulse response of this system using the distributive and associative properties \\[\\begin{aligned}\n  h(t) &= \\left[ h_1(t) + h_2(t)\\right]*h_3(t)\\\\\n  &= h_1(t)*h_3(t) + h_2(t)*h_3(t)\\\\\n  &= \\left[ e^{-2t}u(t)\\right]*\\left[ e^{-6t}u(t)\\right] + \\left[-e^{-4t}u(t) \\right]*\\left[ e^{-6t}u(t)\\right]\n\\end{aligned}\\] Using the convolution table from Lecture 8 we get the overall impulse response \\[h(t) = \\frac{e^{-2 t}-e^{-6 t}}{4}u(t) - \\frac{e^{-4 t}-e^{-6 t}}{2}u(t) = \\frac{1}{4}e^{-2t}u(t) -\\frac{1}{2}e^{-4t}u(t) + \\frac{1}{4}e^{-6t}u(t)\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>CT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "10-ct-block.html#connections-to-lccde",
    "href": "10-ct-block.html#connections-to-lccde",
    "title": "10  CT Block Diagrams",
    "section": "10.3 Connections to LCCDE",
    "text": "10.3 Connections to LCCDE\nThe other system representation we have seen are linear, constant-coefficient differential equations. These can be expressed as combinations of derivative and/or integration blocks.\n\nFirst-Order System\nTo illustrate this consider the first-order LCCDE \\[\\frac{dy}{dt}(t) + ay(t) = x(t)\\] We can solve this for \\(y(t)\\) \\[y(t) = -\\frac{1}{a} \\frac{dy}{dt}(t) + \\frac{1}{a}x(t)\\] and can express this as a feedback motif\n\n\n\nCAPTION\n\n\nAlternatively we could integrate the differential equation \\[\\begin{aligned}\n  \\frac{dy}{dt}(t) + ay(t) &= x(t)\\\\\n  \\int\\limits_{-\\infty}^t \\frac{dy}{dt}(\\tau)\\; d\\tau + a\\int\\limits_{-\\infty}^t y(\\tau)\\; d\\tau &= \\int\\limits_{-\\infty}^t x(\\tau)\\; d\\tau\\\\\n  y(\\tau) \\Big|_{-\\infty}^t  + a\\int\\limits_{-\\infty}^t y(\\tau)\\; d\\tau &= \\int\\limits_{-\\infty}^t x(\\tau)\\; d\\tau\\\\\n\\end{aligned}\\] Under the assumption \\(y(-\\infty) = 0\\) we can solve this for \\(y(t)\\) to get \\[y(t) = -a\\int\\limits_{-\\infty}^t y(\\tau)\\; d\\tau + \\int\\limits_{-\\infty}^t x(\\tau)\\; d\\tau\\] which can be expressed as the block diagram\n\n\n\nCAPTION\n\n\nWe can simplify this block diagram, by noting \\[\\begin{aligned}\n  y(t) &= -a\\int\\limits_{-\\infty}^t y(\\tau)\\; d\\tau + \\int\\limits_{-\\infty}^t\n  x(\\tau)\\; d\\tau\\\\\n  &= \\int\\limits_{-\\infty}^t \\left(-a y(\\tau) +  x(\\tau)\\right)\\; d\\tau\\\\\n\\end{aligned}\\] which requires only a single integrator\n\n\n\nCAPTION\n\n\nThe choice of using derivative or integrator blocks is not arbitrary in practice. Derivatives are sensitive to noise at high frequencies (for reasons we will see later in the semester) and so integrators perform much better when implemented in hardware.\n\n\nSecond-Order System\nNow consider the second-order system \\[\\frac{d^2y}{dt^2}(t) + a\\frac{dy}{dt}(t)  + by(t)= x(t)\\] Using a similar process to the first-order system, we can express this as (dropping the limits of integration for clarity): \\[y(t) = -a \\int y(\\tau)\\; d\\tau + \\int\\int \\left( -by(\\tau) + x(\\tau) \\right) \\; d\\tau^2\\] which has the block diagram\n\n\n\nCAPTION\n\n\nThis is equivalent to two systems in series\n\n\n\nCAPTION\n\n\nRecall that, from the commutative property of convolution, the order of systems in series can be swapped\n\n\n\n\nCAPTION\n\n\nWe then note that the signal \\(z\\) and the output of the integrator blocks are the same in both systems so that they can be combined into a single block diagram as follows, reducing the number of integrators by two\n\n\n\nCAPTION",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>CT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "10-ct-block.html#implementing-a-system-in-hardware",
    "href": "10-ct-block.html#implementing-a-system-in-hardware",
    "title": "10  CT Block Diagrams",
    "section": "10.4 Implementing a System in Hardware",
    "text": "10.4 Implementing a System in Hardware\nOne of the most powerful uses of block diagrams is the implementation of a CT system in hardware. As we shall see later in the semester, designing CT systems for a particular purpose leads to a mathematical description that is equivalent to either an impulse response, or a LCCDE. We have seen how these can be represented as block diagrams. Once we have reduced a system to blocks consisting of simple operations, we can then convert the block diagram to a circuit.\n\n\n\n\n\n\nBlock | Typical Circuit | :================================:+:================================:+  |  |",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>CT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "11-dt-block.html",
    "href": "11-dt-block.html",
    "title": "11  DT Block Diagrams",
    "section": "",
    "text": "11.1 The Four Basic Motifs\nBlock diagrams of DT systems are similar to CT systems.\nThe four motifs are:\nNote the feedback is negative (the minus sign on the feedback summation input). As in CT, these can be used in various combinations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "11-dt-block.html#the-four-basic-motifs",
    "href": "11-dt-block.html#the-four-basic-motifs",
    "title": "11  DT Block Diagrams",
    "section": "",
    "text": "A single block.\n\n\n\n\nA block represeting a system/subsystem.\n\n\n\nA series connection of two blocks\n\n\n\n\nA block represeting a series connection of subsystems.\n\n\n\nA parallel connection of two blocks\n\n\n\n\nA block represeting a parallel connection of subsystems.\n\n\n\nA feedback connection\n\n\n\n\nA block represeting a feedback connection of subsystems.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "11-dt-block.html#connections-to-convolution",
    "href": "11-dt-block.html#connections-to-convolution",
    "title": "11  DT Block Diagrams",
    "section": "11.2 Connections to Convolution",
    "text": "11.2 Connections to Convolution\nEach subsystem, \\(\\mathcal{S}_i\\), can be represented by a basic discrete time-domain operation (e.g. differences, running sums, addition, and scaling) or more generally by its impulse response \\(h_i[n]\\).\nFor example a block representing an system acting as a delay of one sample is typically drawn as\n\n\n\nA block represeting a delay.\n\n\nThis is equivalent to an impulse response \\(h[n] = \\delta[n-1]\\) so that it might also be drawn as\n\n\n\nA block represeting a delay as an impulse response.\n\n\nSimilarly, a block representing an system acting as an advance of one sample is typically drawn as\n\n\n\nA block represeting an advance.\n\n\nThis is equivalent to an impulse response \\(h[n] = \\delta[n+1]\\) so that it might also be drawn as\n\n\n\nA block represeting an advance as an impulse response.\n\n\nWe can use the concept of convolution to connect block diagrams to the properties of convolution\n\nA single block is equivalent to convolution with the impulse response for that subsystem\n\n\n\nA block represeted by an arbitrary impulse response.\n\n\nUsing the associative property, a series connection of two blocks becomes\n\n\n\nA series connection of blocks and the overall impulse response.\n\n\nwhich can be reduced to a single convolution \\(y[n] = h_3[n]*x[n]\\) where \\(h_3[n] = h_1[n]*h_2[n]\\).\nUsing the distributive property, a parallel connection of two blocks becomes\n\n\n\nA parallel connection of blocks and the overall impulse response.\n\n\nwhich is equivalent to a single convolution \\(y[n] = h_3[n]*x[n]\\) where \\(h_3[n] = h_1[n] + h_2[n]\\).\nIn the feedback connection let \\(w[n]\\) be the output of the summation\n\n\n\nA feedback connection of blocks and the overall impulse response.\n\n\nThen \\(y[n] = h_1[n]*w[n]\\) and \\(w[n] = x[n] - h_2[n]*y[n]\\). Substituting the later into the former gives \\(y[n] = h_1*(x-h_2[n]*y[n])\\). Using the distributive property we get \\(y[n] = h_1[n]*x[n] - h_1[n]*h_2[n]*y[n]\\). Isolating the input on the right-hand side and using \\(y[n] = \\delta[n]*y[n]\\) we get \\[y[n] + h_1[n]*h_2[n]*y[n] = \\left(\\delta[n] + h_1[n]*h_2[n]\\right)*y[n] = h_1[n]*x[n]\\] We can solve this for \\(y[n]\\) using the concept of inverse systems. Let \\(h_3[n]* \\left(\\delta[n] + h_1[n]*h_2[n]\\right)= \\delta[n]\\), i.e. \\(h_3\\) is the inverse system of \\(\\delta[n] + h_1[n]*h_2[n]\\). Then \\[y[n] = h_3[n]*h_1[n]*x[n]\\]\n\nRecall, when the system is instantaneous (memoryless) the impulse response is \\(a\\delta[n]\\) for some constant \\(a\\). This is the same as scaling the signal by \\(a\\). We typically drop the block in such cases and draw the input-output operation as\n\n\n\nA multiplier/amplifier connection between blocks and the overall impulse response.\n\n\nThese properties allow us to perform transformations, either breaking up a system into subsystems, or reducing a system to a single block.\n\n\nExample\n\n\nConsider a second-order system system with impulse response \\[h[n] = \\left(\\frac{1}{2}\\right)^n\\, u[n] + \\left(\\frac{3}{4}\\right)^n\\, u[n]\\] We can express this as a block diagram consisting of two parallel blocks\n\n\n\nBlock diagram corersponding to the example, using a parallel realization.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "11-dt-block.html#connections-to-lccde",
    "href": "11-dt-block.html#connections-to-lccde",
    "title": "11  DT Block Diagrams",
    "section": "11.3 Connections to LCCDE",
    "text": "11.3 Connections to LCCDE\nThe other DT system representation we have seen are linear, constant-coefficient difference equations. These can be expressed as combinations of advance or delay blocks. This is straightforward compared to the CT system case.\n\n11.3.1 First-Order System\nTo illustrate this consider the first-order LCCDE \\[y[n+1] + ay[n]= x[n+1]\\] We can solve this for \\(y[n]\\) \\[y[n] = -\\frac{1}{a}y[n+1]  + \\frac{1}{a}x[n+1]\\] and can express this as a feedback motif using the advance operator \\(E\\)\n\n\n\nBlock diagram corresponding to a first-order LCCDE in advance form.\n\n\nAlternatively we could rewrite the difference equation in recursive delay form \\[y[n] = -ay[n-1] +x[n]\\] which can be expressed as a block diagram using the delay operator, \\(D\\)\n\n\n\nBlock diagram corresponding to a first-order LCCDE in delay form.\n\n\nThe choice of using advance or delay blocks results in a non-causal or causal (respectively) system. Thus, delay blocks are required for real-time DT system implementations.\n\n\n11.3.2 Second-Order System\nNow consider the second-order system \\[y[n+2] + ay[n+1] + by[n] = x[n+2]\\] Again, writing in recursive delay form \\[y[n] = -ay[n-1] - by[n-2] + x[n]\\] we obtain the block diagram\n\n\n\nBlock diagram corresponding to a second-order LCCDE in delay form.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "11-dt-block.html#implementing-a-dt-system",
    "href": "11-dt-block.html#implementing-a-dt-system",
    "title": "11  DT Block Diagrams",
    "section": "11.4 Implementing a DT System",
    "text": "11.4 Implementing a DT System\nAs in the CT case, one of the most powerful uses of block diagrams is the implementation of a DT system in hardware. As we shall see later in the semester, designing a DT system for a particular purpose leads to a mathematical description that is equivalent to either an impulse response or a LCCDE. We have seen how these can be represented as block diagrams. Once we have reduced a system to blocks consisting of simple operations, we can then convert the block diagram to a digital circuit, implement using a digital signal processor, or write an equivalent program to run on an embedded or general purpose computer.\n\n\n\n\n\n\n\nBlock\nTypical Digital Circuit (MCU/CPU)\n\n\n\n\n\nMultiplier (ALU or FPU)\n\n\n\nAdder (ALU or FPU)\n\n\n\nShift Register (Memory Location)\n\n\n\n\n\nExample\n\n\nThe following C++ code implements the second order system given by\n\n\n\nExample block diagram corresponding to a second-order system.\n\n\nusing floating point calculations. It assumes the current input is obtained via the function read, and the output written using the function write. The delayed values of the output are stored in the array buffer and are initialized to zero (\"at rest\" prior to application of the input).\ndouble buffer[2] = {0.0,0.0};\nwhile(true){\n   double x = read();\n   double y = -0.5*buffer[1] - buffer[0]/9.0 + x;\n   write(y);\n   buffer[0] = buffer[1];\n   buffer[1] = y;\n}\nNote in real applications it is common to replace the floating point calculations with fixed-width (scaled integer) ones.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DT Block Diagrams</span>"
    ]
  },
  {
    "objectID": "12-ct-tf.html",
    "href": "12-ct-tf.html",
    "title": "12  Eigenfunctions of CT Systems",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Eigenfunctions of CT Systems</span>"
    ]
  },
  {
    "objectID": "13-dt-tf.html",
    "href": "13-dt-tf.html",
    "title": "13  Eigenfunctions of DT Systems",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigenfunctions of DT Systems</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html",
    "href": "14-ctfs.html",
    "title": "14  CT Fourier Series",
    "section": "",
    "text": "14.1 Synthesis and Analysis Equation\nRecall the complex exponential \\(e^{st}\\) for \\(s\\in\\mathbb{C}\\) is the Eigenfunction of CT LTI systems. If we can decompose an input into a (possibly infinite) sum of such signals, we can easily determine the output using the superposition principle. In this section we consider the decomposition when the input is periodic, called the CT Fourier Series (CTFS).\nRecall a signal \\(x(t)\\) is periodic, with fundamental frequency \\(\\omega_0 = \\frac{2\\pi}{T_0}\\) rad/sec or \\(f_0 = \\frac{1}{T_0}\\) Hertz, if \\(x(t) = x(t+kT_0)\\) for integer multiple \\(k\\) and fundamental period \\(T_0\\in \\mathbb{R}\\). As we shall see, in this case the complex exponent of the Eigenfunction becomes \\(s_k = jk\\omega_0\\), and the decomposition is a countably infinite sum. This gives the input-output relationship for a stable LTI system as \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t} \\; \\longrightarrow\\; y(t) = \\sum\\limits_{k = -\\infty}^{\\infty} H(j k\\omega_0)\\, a_k \\, e^{j k\\omega_0 t}\\] where \\(H(j k\\omega_0)\\) are the Eigenvalues or frequency response. We now turn to determining under what circumstances the decomposition exists and how to find the coefficients \\(a_k\\).\nSuppose we can approximate (we will revisit shortly when this approximation is exact) the periodic function \\(x(t)\\) by the sum \\[\\boxed{x(t) \\approx \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}\\;.}\\] This is called the synthesis equation of the CT Fourier series.\nAssuming equivalence, let us multiply both sides by the function \\(e^{-jn\\omega_0 t}\\), \\[x(t)e^{-jn\\omega_0 t} = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t}\\] and integrate over one period \\[\\int\\limits_{0}^{T_0} x(t)e^{-jn\\omega_0 t} \\; dt = \\int\\limits_{0}^{T_0} \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t} \\; dt\\] Exchanging the order of integration and summation in the right-hand expression gives \\[\\int\\limits_{0}^{T_0} x(t)e^{-jn\\omega_0 t} \\; dt = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\left[ \\int\\limits_{0}^{T_0} \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t} \\; dt\\right]\\] The bracketed term can be rewritten as \\[\\int\\limits_{0}^{T_0} \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t} \\; dt = \\int\\limits_{0}^{T_0} \\, e^{j (k-n)\\omega_0 t} \\; dt = \\int\\limits_{0}^{T_0} \\cos((k-n)\\omega_0 t) \\; dt + j \\int\\limits_{0}^{T_0} \\sin((k-n)\\omega_0 t) \\; dt\\] We now note that for \\(k\\neq n\\) the integrals of the real and imaginary parts are zero \\[\\int\\limits_{0}^{T_0} \\cos((k-n)\\omega_0 t) \\; dt = \\frac{1}{(k-n)\\omega_0}\\sin((k-n)\\omega_0 t) \\Big|_{0}^{T_0} = \\frac{1}{(k-n)\\omega_0}\\sin((k-n)2\\pi) - \\frac{1}{(k-n)\\omega_0}\\sin(0) = 0\\] \\[\\int\\limits_{0}^{T_0} \\sin((k-n)\\omega_0 t) \\; dt = -\\frac{1}{(k-n)\\omega_0}\\cos((k-n)\\omega_0 t) \\Big|_{0}^{T_0} = -\\frac{1}{(k-n)\\omega_0}\\cos((k-n)2\\pi) + \\frac{1}{(k-n)\\omega_0}\\cos(0) = 0\\] When \\(k=n\\) \\[\\int\\limits_{0}^{T_0} e^{j (k-n)\\omega_0 t} \\; dt = \\int\\limits_{0}^{T_0} \\; dt = T_0\\] Thus the bracketed term above is \\[\\int\\limits_{0}^{T_0} \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t} \\; dt = T_0\\, \\delta[k-n]\\] and the right-hand side is \\[\\sum\\limits_{k = -\\infty}^{\\infty} a_k \\left[ \\int\\limits_{0}^{T_0} \\, e^{j k\\omega_0 t}e^{-jn\\omega_0 t} \\; dt\\right] = \\sum\\limits_{k = -\\infty}^{\\infty} a_k  T_0\\, \\delta[k-n] = T_0 \\,a_n\\] Thus we obtain the analysis equation of the CT Fourier series: \\[\\boxed{a_n = \\frac{1}{T_0} \\int\\limits_{0}^{T_0} x(t)e^{-jn\\omega_0 t} \\; dt}\\] where the integration can be over any interval of length \\(T_0\\) that is equal to the period, and the symbol for the subscript (integer \\(n\\)) is arbitrary. The CT Fourier Series coefficients are also called the spectrum of the signal. In general the \\(a_k\\) are complex. The function of \\(k\\), \\(|a_k|\\) is called the amplitude spectrum. The function of \\(k\\), \\(\\angle a_k\\) is called the phase spectrum. When plotting the coefficients it is common to plot the amplitude and phase spectrum together.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html#synthesis-and-analysis-equation",
    "href": "14-ctfs.html#synthesis-and-analysis-equation",
    "title": "14  CT Fourier Series",
    "section": "",
    "text": "14.1.1 Example of the Fourier series.\n\n\nConsider the signal \\[x_p(t) = \\left\\{ \\begin{array}{lc}\n    t^2 & -1 &lt; t &lt; 1\\\\\n    0 & \\mbox{else}\n  \\end{array}\n\\right.\\] periodically extended with period \\(T_0 = 2\\) \\[x(t) = \\sum\\limits_{i = -\\infty}^{\\infty} x_p(t - 2i)\\] as shown below:\n\n\n\nPlot of the example signal.\n\n\nTo find the Fourier Series approximation of \\(x(t)\\), \\[x(t) \\approx \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}\\; ,\\] we need to find the coefficients \\[a_k = \\frac{1}{T_0} \\int\\limits_{0}^{T_0} x(t)e^{-jk\\omega_0 t} \\; dt\\] Since the integration can be over any period, we can use the limits \\([-1,1]\\) and note that \\(T_0 = 2\\) so that \\(\\omega_0 = \\pi\\), giving the sequence of expressions \\[\\begin{aligned}\n  a_k &= \\frac{1}{2} \\int\\limits_{-1}^{1} t^2\\,e^{-jk\\pi t} \\; dt\\\\\n  &= \\frac{1}{2} \\left[ \\int\\limits_{-1}^{1} t^2\\,\\cos(-k\\pi t) \\; dt + j \\int\\limits_{-1}^{1} t^2\\,\\sin(-k\\pi t) \\; dt \\right]\\\\\n  &= \\frac{1}{2} \\left[ \\int\\limits_{-1}^{1} t^2\\,\\cos(k\\pi t) \\; dt + j \\int\\limits_{-1}^{1} - t^2\\,\\underbrace{\\sin(k\\pi t)}_{\\text{always = 0}} \\; dt \\right]\\\\\n  &= \\frac{1}{2} \\int\\limits_{-1}^{1} t^2\\,\\cos(k\\pi t) \\; dt\\; \\mbox{ using an integration table }\\\\\n  &= \\frac{1}{2} \\frac{4k\\pi\\overbrace{\\cos(k\\pi)}^{(-1)^k} + 2(k^2\\pi^2-2)\\overbrace{\\sin(k\\pi)}^{\\text{always = 0}}}{k^3\\pi^3}\\\\\na_k &= \\frac{2}{k^2\\pi^2}\\left(-1\\right)^k\n\\end{aligned}\\] This result is undefined for when \\(k=0\\). In that case note the original integral is \\[a_0 = \\frac{1}{2} \\int\\limits_{-1}^{1} t^2 \\; dt = \\frac{1}{6}t^3 \\Big|_{-1}^{1} = \\frac{1}{3}\\] Thus the final approximation is \\[x(t) \\approx \\sum\\limits_{k = -\\infty}^{\\infty} \\underbrace{\\frac{2}{k^2\\pi^2}\\left(-1\\right)^k}_{a_k} \\, e^{j k\\pi t} \\;.\\] We can plot the spectrum of this signal (using for example Matlab)\nk = -10:10;\na = 2./(pi^2*k.^2);\na(11) = 1/3;\n\nsubplot(2,1,1);\nstem(k, abs(a));\nxlabel('k');\nylabel('|a(k)|');\ntitle('Amplitude Spectrum');\n\nsubplot(2,1,2);\nstem(k, angle(a));\nxlabel('k');\nylabel('Angle a(k)');\ntitle('Phase Spectrum');\nGiving the amplitude and phase spectrum plot\n\n\n\n\n\n\n\n\nFigure 14.1: Amplitude and phase spectrum plot for the example. For a detailed description, refer to Appendix B.1.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html#variations-on-the-synthesis-and-analysis-equations",
    "href": "14-ctfs.html#variations-on-the-synthesis-and-analysis-equations",
    "title": "14  CT Fourier Series",
    "section": "14.2 Variations on the Synthesis and Analysis Equations",
    "text": "14.2 Variations on the Synthesis and Analysis Equations\nThere are two commonly used, equivalent, expressions for computing the CTFS coefficients. They can be derived using Euler’s formula and related trig identities.\n\nExponential Form. This is the form derived above \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}\\] where \\[a_k = \\frac{1}{T_0} \\int\\limits_{T_0} x(t)e^{-jk\\omega_0 t} \\; dt\\]\nTrig Form \\[x(t) = b_0 + \\sum\\limits_{k = 1}^{\\infty} b_k \\,\\cos(k\\omega_0 t) + c_k\\,\\sin(k\\omega_0 t)\\] where \\[b_0 = \\frac{1}{T_0} \\int\\limits_{T_0} x(t) \\; dt\\] is the average value of the signal, and \\[b_k = \\frac{2}{T_0} \\int\\limits_{T_0} x(t)\\cos(k\\omega_0 t) \\; dt\\] \\[c_k = \\frac{2}{T_0} \\int\\limits_{T_0} x(t)\\sin(k\\omega_0 t) \\; dt\\]\nCompact Trig Form \\[x(t) = d_0 + \\sum\\limits_{k = 1}^{\\infty} d_k \\,\\cos(k\\omega_0 t + \\theta_k)\\] where \\[d_0 = \\frac{1}{T_0} \\int\\limits_{T_0} x(t) \\; dt\\] is the average value of the signal, and \\[d_k = \\sqrt{b_k^2 + c_k^2}\\] \\[\\theta_k = \\arctan\\left( \\frac{-c_k}{b_k} \\right)\\]\n\nNote that \\(2a_k = b_k -j c_k\\) for \\(k \\geq 1\\) and \\(a_0=b_0\\).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html#convergence-of-the-ct-fourier-series",
    "href": "14-ctfs.html#convergence-of-the-ct-fourier-series",
    "title": "14  CT Fourier Series",
    "section": "14.3 Convergence of the CT Fourier Series",
    "text": "14.3 Convergence of the CT Fourier Series\nAs mentioned above the Fourier Series is strictly speaking an approximation \\[x(t) \\approx \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t} \\mbox{ where } a_k = \\frac{1}{T_0} \\int\\limits_{T_0} x(t)e^{-jk\\omega_0 t} \\; dt\\] to determine when this approximation is an equivalence (and in what sense) we need to establish the existence and convergence of the integral and summation respectively.\nThe coefficients \\(a_k\\) will exist when the integral converges, or equivalently when \\[\\int\\limits_{T_0} \\left|x(t)\\right| \\; dt &lt; \\infty\\] i.e. the signal is absolutely integrable over any period.\nTo determine when the summation converges, first consider the truncated CT Fourier Series \\[x_N(t) \\approx \\sum\\limits_{k = -N}^{N} a_k \\, e^{j k\\omega_0 t}\\] where the infinite sum has been truncated to the finite range \\([-N,N]\\). Define the error between the original signal \\(x(t)\\) and the truncated approximation \\(x_N(t)\\) at each time point as \\[E(N,t) = x(t) - x_N(t)\\] There are two relevant notions of convergence. If \\[\\lim_{N\\rightarrow \\infty} \\int\\limits_{T_0} \\left| E(N,t] \\right|\\;dt = 0\\] we say the CT Fourier Series converges exactly to the signal. If \\[\\lim_{N\\rightarrow \\infty} \\int\\limits_{T_0} \\left| E(N,t) \\right|^2\\;dt = 0\\] we say the CT Fourier Series converges in the mean-square sense to the signal.\nMore formally the CTFS exists if the Dirichlet Conditions hold for the signal:\n\nThe signal has a finite number of discontinuities per period.\nThe signal has a finite number of maxima and minima per period.\nThe signal is bounded, i.e. \\[\\int_{T_0} |x(t)| \\;dt &lt; \\infty\\]\n\nThese conditions rule out pathological functions. For most practical signals of interest, the conditions hold.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html#examples-of-the-fourier-series",
    "href": "14-ctfs.html#examples-of-the-fourier-series",
    "title": "14  CT Fourier Series",
    "section": "14.4 Examples of the Fourier Series",
    "text": "14.4 Examples of the Fourier Series\n\n\n14.4.1 Fourier series of the impulse train.\n\n\nConsider the impulse train signal defined as \\[x(t) = \\sum\\limits_{m = -\\infty}^{\\infty} \\delta(t-mT_0)\\] which we be important later when we discuss sampling CT signals. Do the Dirichlet conditions hold? Yes. It has one discontinuity, one maximum, and one minimum per period. It is also bounded since \\[\\int_{T_0} |\\delta(t)| \\;dt = 1 \\mbox{ by definition.}\\] The spectrum for the impulse train is given by \\[a_k = \\frac{1}{T_0} \\int\\limits_{T_0} x(t)e^{-jk\\omega_0 t} \\; dt =  \\frac{1}{T_0} \\int\\limits_{-\\frac{T_0}{2}}^{\\frac{T_0}{2}} \\delta(t)e^{-jk\\omega_0 t} \\; dt = \\frac{1}{T_0}\\]\n\n\n\n\n14.4.2 Fourier series of a sinusoid.\n\n\nConsider the signal \\(x(t) = \\cos(\\omega t)\\). We can write this as the sum of two complex exponentials using Euler’s formula \\[x(t) = \\frac{1}{2}e^{j\\omega t} + \\frac{1}{2}e^{-j\\omega t}\\] Comparing this to the synthesis equation \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t} = \\cdots + a_{-2} \\, e^{j (-2)\\omega_0 t} + a_{-1} \\, e^{j (-1)\\omega_0 t} + a_0 + a_{1} \\, e^{j (1)\\omega_0 t} + a_{2} \\, e^{j (2)\\omega_0 t} + \\cdots\\] we note that if \\(\\omega_0 = \\omega\\) and \\[a_k = \\left\\{ \\begin{array}{lc}\n    \\tfrac{1}{2} & k = -1\\\\[0.5em]\n    \\tfrac{1}{2} & k = 1\\\\[0.5em]\n    0 & \\text{else}\n  \\end{array}\n  \\right.\\] then the two expressions are identical and the CT Fourier Series is an exact representation.\n\n\n\n\n14.4.3 Fourier series of a square wave.\n\n\nConsider the square wave signal of amplitude \\(A &gt; 0\\) \\[x(t) = \\sum\\limits_{m= -\\infty}^{\\infty} \\left\\{ \\begin{array}{lc}\n    -A & \\tfrac{T_0}{2} &lt; t-mT_0 &lt; 0\\\\[0.5em]\n    A & 0 &lt; t - mT_0 &lt; \\tfrac{T_0}{2}\n  \\end{array}\n  \\right.\\] shown below\n\n\n\n\n\n\nFigure 14.2: Plot of a square wave signal. For a detailed description, refer to the Appendix.\n\n\n\nThe coefficients are given by\n\\[\\begin{aligned}\n    a_k &= \\frac{1}{T_0} \\int\\limits_{T_0} x(t)e^{-jk\\omega_0 t} \\; dt\\\\\n    &= \\frac{1}{T_0} \\left[ \\int\\limits_{0}^{\\frac{T_0}{2}} Ae^{-jk\\omega_0 t} \\; dt + \\int\\limits_{\\frac{T_0}{2}}^{T_0} -Ae^{-jk\\omega_0 t} \\; dt\\right]\\\\\n    &= \\frac{1}{T_0} \\left[ \\frac{A}{-jk\\omega_0}e^{-jk\\omega_0 t} \\Big|_{0}^{\\frac{T_0}{2}} + \\frac{-A}{-jk\\omega_0}e^{-jk\\omega_0 t} \\Big|_{\\frac{T_0}{2}}^{T_0}\\right]\\\\\n    &= \\frac{1}{T_0} \\frac{A}{jk\\omega_0} \\left[ -\\left(e^{-jk\\omega_0 \\frac{T_0}{2}} - e^{0}\\right) + \\left(e^{-jk\\omega_0 T_0} - e^{-jk\\omega_0 \\frac{T_0}{2}}\\right)\\right]\n  \n\\end{aligned}\\]\nNote that \\(\\omega_0\\frac{T_0}{2} = \\frac{2\\pi}{T_0}\\frac{T_0}{2}= \\pi\\) and \\(\\omega_0 T_0 = \\frac{2\\pi}{T_0}T_0 = 2\\pi\\) . Thus\n\\[\\begin{aligned}\n    a_k &= \\frac{1}{T_0} \\frac{A}{jk\\frac{2\\pi}{T_0}} \\left[ -\\left(e^{-jk\\pi} - e^{0}\\right) + \\left(e^{-jk2\\pi} - e^{-jk\\pi}\\right)\\right]\\\\\n    &= \\frac{A}{jk\\pi}\\left( 1-e^{-jk\\pi}\\right) \\\\\n    &= \\left\\{ \\begin{array}{lc}\n      0 & k \\mbox{ even}\\\\\n      \\frac{2A}{jk\\pi} & k \\mbox{ odd}\n    \\end{array}\n\\right.  \n\\end{aligned}\\]\nThe amplitude spectrum is given by \\[|a_k| = \\left\\{ \\begin{array}{lc}\n    0 & k \\mbox{ even}\\\\\n    \\left|\\frac{2A}{k\\pi}\\right| & k \\mbox{ odd}\\end{array}\\right.\\]\nThe phase spectrum is given by \\[\\angle a_k = \\left\\{ \\begin{array}{lc}\n    \\pi & k &lt; 0 \\mbox{ and even}\\\\\n    -\\pi & k &gt; 0 \\mbox{ and even}\\\\\n    \\frac{\\pi}{2} & k &lt; 0 \\mbox{ and odd}\\\\\n    -\\frac{\\pi}{2} & k &gt; 0 \\mbox{ and odd}\\end{array}\\right.\\] This is plotted below for \\(A = 1\\).\nTODO\nWe can plot the truncated approximation for increasing number of terms N, the squared error, and the total error.\nTODO\nNote as \\(N\\) increases the approximation gets closer to the square wave, except at the discontinuities. This is called Gibbs Ringing. As \\(N \\rightarrow \\infty\\) the mean-square error goes to zero, so the CTFS approximation to the square wave converges in the mean-square sense.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "14-ctfs.html#properties-of-the-ct-fourier-series",
    "href": "14-ctfs.html#properties-of-the-ct-fourier-series",
    "title": "14  CT Fourier Series",
    "section": "14.5 Properties of the CT Fourier Series",
    "text": "14.5 Properties of the CT Fourier Series\nLet \\(a_k\\) and \\(b_k\\) be the CTFS coefficients for the periodic signals \\(x(t)\\) and \\(y(t)\\) respectively.\n\nLinearity. The coefficients of the signal \\[z(t) = Ax(t) + By(t) \\mbox{ for constants } A,B\\] are \\(Aa_k + Bb_k\\)\nTime Shifting. The coefficients of \\[z(t) = x(t-t_0) \\mbox{ are } e^{-jk\\omega_0 t_0}a_k\\] that is it adds a phase shift.\nTime reversal. The coefficients of \\[z(t) = x(-t) \\mbox{ are } a_{-k}\\] that is the sequence reverses.\nTime Scaling. Let \\(T_0\\) and \\(\\omega_0\\) be the fundamental period and frequency of a periodic \\(x(t)\\). The signal \\[z(t) = x(\\alpha t) \\mbox{ for } \\alpha &gt; 0\\] is periodic with period \\(\\frac{T_0}{\\alpha}\\) and fundamental frequency \\(\\alpha\\omega_0\\). The coefficients of \\(z(t)\\) are the same as \\(x(t)\\).\nMultiplication. The coefficients of \\[z(t) = x(t) \\cdot y(t) \\mbox{ are } \\sum\\limits_{m = -\\infty}^{\\infty} a_m\\cdot b_{k-m}\\] the discrete convolution of the individual signals’ coefficients.\nConjugate Symmetry. The coefficients of \\[z(t) = x^*(t) = \\Re{x(t)} - j\\Im{x(t)} \\mbox{ are } a_{-k}^*\\] A consequence of this property and the time-reversal property is that real, even signals have real, even \\(a_k\\); and real, odd signals have purely imaginary, odd \\(a_k\\) (see the examples above).\nParseval’s Relation. The power of the signal with Fourier series coefficients \\[\\frac{1}{T_0} \\int_{T_0} |x(t)|^2\\;dt = \\sum\\limits_{k = -\\infty}^{\\infty} |a_k|^2\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CT Fourier Series</span>"
    ]
  },
  {
    "objectID": "15-dtfs.html",
    "href": "15-dtfs.html",
    "title": "15  DT Fourier Series",
    "section": "",
    "text": "15.1 Synthesis and Analysis Equation\nRecall the complex exponential \\(z^n\\) is the Eigenfunction of DT LTI systems. If we can decompose an input into a (possibly infinite) sum of such signals, we can easily determine the output using the superposition principle. In this section we consider the decomposition when the input is periodic, called the DT Fourier Series (DTFS). The DTFS is similar, but not identical to the CTFS. Notably, the approximation requires only a finite number of terms, there are no convergence issues, and the resulting spectrum is a periodic function.\nRecall a DT signal \\(x[n]\\) is periodic, with fundamental frequency \\(\\omega_0 = \\frac{2\\pi}{N}\\) rad/sec, if \\(x[n] = x[n+kN]\\) for integer multiple \\(k\\) and fundamental period \\(N \\in \\mathbb{Z}\\). As we shall see, in this case the complex base of the Eigenfunction becomes \\(z_k = e^{jk\\omega_0}\\), and the decomposition is a finite sum. This gives the input-output relationship for a stable DT LTI system as \\[x[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n} \\;\\longrightarrow\\;  y[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} H\\left(e^{j k\\omega_0}\\right) a_k e^{jk\\omega_0 n}\\] where \\(H\\left(e^{j k\\omega_0}\\right)\\) are the Eigenvalues or DT frequency response. We now turn to how to find the coefficients \\(a_k\\).\nSimilar to the CTFS we wish to show that any periodic DT signal can be represented by the sum of complex exponentials whose frequencies are harmonics of the fundamental. This differs from the CTFS in that there are only \\(N\\) distinct harmonics, so that the sum is over a finite range \\[\\boxed{x[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n} }\\] where \\(N\\) is the period and \\(N_0\\) is any starting index for the sum. Note the course text defines \\(&lt;N&gt; = \\{ N_0, N_0 + 1,  \\cdots (N_0 + N-1) \\}\\). This is called the synthesis equation of the DT Fourier series.\nOne approach to find the coefficients \\(a_k\\) is to note that there are a finite number of terms in the summation and the signal has a finite number of values over one period. This gives a system of \\(N\\) linear equations in \\(N\\) unknowns (the \\(a_k\\)’s) \\[\\begin{aligned}\n  x[N_0] &= \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0N_0}\\\\\n  x[N_0+1] &= \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0(N_0+1)}\\\\\n  \\vdots\\hspace{2em} &= \\hspace{2em}\\vdots\\\\\n  x[N_0+N-1] &= \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0(N_0+N-1)}\\\\\n\\end{aligned}\\] which can be solved to find the coefficients using linear algebra.\nAnother approach is similar to that taken when deriving the CT Fourier Series. Beginning with the synthesis equation \\[x[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n}\\] we multiply both sides by \\(e^{-j\\frac{2\\pi r}{N} n}\\) for \\(r \\in \\mathbb{Z}\\) and sum over \\(N\\) terms \\[\\sum\\limits_{n = N_0}^{N_0 + N-1} x[n]e^{-j\\frac{2\\pi r}{N} n} = \\sum\\limits_{n = N_0}^{N_0 + N-1} \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n}e^{-j\\frac{2\\pi r}{N} n}\\] We then interchange to order of summation on the right-hand-side \\[\\sum\\limits_{n = N_0}^{N_0 + N-1} x[n]e^{-j\\frac{2\\pi r}{N} n} =  \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k \\underbrace{\\sum\\limits_{n = N_0}^{N_0 + N-1} e^{jk\\omega_0n}e^{-j\\frac{2\\pi r}{N} n}}\\] Since \\(\\omega_0 = \\frac{2\\pi}{N}\\), the bracketed term is\nand the right-hand side is \\[\\sum\\limits_{k = N_0}^{N_0 + N-1} a_k N \\delta[(k-r) + mN] = Na_r\\] since \\(N_0 &lt; mN &lt; N_0 + 1\\) for some \\(m\\). Solving for \\(a_r\\) gives the analysis equation of the DT Fourier series: \\[\\boxed{a_r = \\frac{1}{N} \\sum\\limits_{n = N_0}^{N_0+N-1} x[n]e^{-j\\frac{2\\pi}{N} r n }}\\] where the summation can be over any interval of length \\(N\\) and the symbol for the subscript (integer \\(r\\)) is arbitrary. The DT Fourier Series coefficients are also called the spectrum of the signal. In general the \\(a_k\\) are complex. Note the spectrum is periodic in \\(N\\). The function of \\(k\\), \\(|a_k|\\) is called the amplitude spectrum. The function of \\(k\\), \\(\\angle a_k\\) is called the phase spectrum. When plotting the coefficients it is common to plot the amplitude and phase spectrum together over a single interval of length \\(N\\) (since it is periodic).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DT Fourier Series</span>"
    ]
  },
  {
    "objectID": "15-dtfs.html#synthesis-and-analysis-equation",
    "href": "15-dtfs.html#synthesis-and-analysis-equation",
    "title": "15  DT Fourier Series",
    "section": "",
    "text": "Example\n\n\nConsider the periodic DT signal \\(x[n] = \\cdots -1, 1, -1, 1, -1, 1, \\cdots\\) where \\(x[0] = 1\\). The period is \\(N=2\\) so that \\(\\omega_0 = \\pi\\). If we let \\(N_0 = 0\\), the system of equations is\n\\[\\begin{aligned}\n    x[0] &= \\sum\\limits_{k = 0}^{1} a_k = a_0 + a_1 = 1\\\\\n    x[1] &= \\sum\\limits_{k = 0}^{1} a_k e^{jk\\pi} = a_0 - a_1 = -1  \n\\end{aligned}\\]\nwhich has the solution \\(a_0 = 0\\) and \\(a_1 = 1\\) and \\(x[n] = e^{j\\pi n}\\).\n\n\n\n\n\\[\\begin{aligned}\n\\sum\\limits_{n = N_0}^{N_0 + N-1} e^{j(k-r)\\frac{2\\pi}{N} n} &= \\begin{cases}\n  N & \\mbox{ if } k-r = 0, \\pm N, \\pm 2N, \\cdots\\\\\n  0 & \\mbox{ else }\n\\end{cases}\\\\\n&= N\\delta[(k-r) + mN] \\mbox{ for arbitrary } m \\in \\mathbb{Z}\n\\end{aligned}\\]\n\n\n\nExample\n\n\nA simple way to construct a DT periodic signal is to use the modulus \\(\\%\\) operator. For example, \\[x[n] = \\gamma^{n \\% N} \\mbox{ for any } \\gamma \\in \\mathbb{C}\\] is periodic in \\(N\\), e.g. \\(x[n] = \\left(\\frac{1}{2}\\right)^{n \\% 4}\\)\nTODO graphics/dtfs_example1\nThe synthesis equation is given by \\[x[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n}\\] Where the coefficients are found using the analysis equation. Let \\(N_0 = 0\\) arbitrarily, then\n\\[\\begin{aligned}\n    a_k &= \\frac{1}{N} \\sum\\limits_{n = 0}^{N-1} x[n]e^{-j\\frac{2\\pi}{N} k n }\\\\\n    &= \\frac{1}{N} \\sum\\limits_{n = 0}^{N-1} \\gamma^n e^{-j\\frac{2\\pi}{N} k n }\\\\\n    &= \\frac{1}{N} \\sum\\limits_{n = 0}^{N-1} \\left(\\gamma e^{-j\\frac{2\\pi}{N} k}\\right)^n\\\\\n    a_k &= \\frac{1}{N} \\frac{1-\\left(\\gamma e^{-j\\frac{2\\pi}{N} k}\\right)^N}{1-\\left(\\gamma e^{-j\\frac{2\\pi}{N} k}\\right)}\n  \n\\end{aligned}\\]\nWe can plot the spectrum of this signal (using for example Matlab)\ngamma = 0.5;\nN = 4;\nk = -10:10;\na = (1-(gamma*exp(-j*2*pi*k/N)).^N)./(N*(1-gamma*exp(-j*2*pi*k/N)));\n\nh0 = subplot(2,1,1);\nh1 = stem(k, abs(a));\nh2 = xlabel('k');\nh3 = ylabel('|a(k)|');\nh4 = title('Amplitude Spectrum');\nh5 = subplot(2,1,2);\nh6 = stem(k, angle(a));\nh7 = xlabel('k');\nh8 = ylabel('Angle a(k)');\nh9 = title('Phase Spectrum');\nGiving the amplitude and phase spectrum plot\nTODO graphics/dtfs_example1.png\n\n\n\n\nExample\n\n\nFind the DTFS of \\(x[n] = \\cos[\\tfrac{\\pi}{4}n]\\). Note \\(N=8\\) and \\(\\omega_0 = \\tfrac{\\pi}{4}\\). Using Euler’s formula \\[x[n] = \\frac{1}{2}e^{j\\frac{\\pi}{4}n} + \\frac{1}{2}e^{-j\\frac{\\pi}{4}n}\\] The synthesis equation is \\[x[n] = \\sum\\limits_{k = 0}^{N-1} a_k e^{jk\\omega_0n} = a_0 + a_1e^{j\\frac{\\pi}{4}n}+ a_2e^{j\\frac{2\\pi}{4}n} + \\cdots + a_7e^{j\\frac{7\\pi}{4}n}\\] Comparing to the expansion above and noting that \\(e^{-j\\frac{\\pi}{4}n} = e^{j\\frac{7\\pi}{4}n}\\) we see that \\[a_k = \\begin{cases}\n    \\frac{1}{2} & k=1\\\\\n    \\frac{1}{2} & k=7\\\\\n    0 & \\mbox{else}\n  \\end{cases}\\] for \\(k \\in [0,7]\\) and \\(a_k = a_{k\\%8}\\) for all \\(k\\).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DT Fourier Series</span>"
    ]
  },
  {
    "objectID": "15-dtfs.html#properties-of-the-dt-fourier-series",
    "href": "15-dtfs.html#properties-of-the-dt-fourier-series",
    "title": "15  DT Fourier Series",
    "section": "15.2 Properties of the DT Fourier Series",
    "text": "15.2 Properties of the DT Fourier Series\nGiven two signals \\(x[n]\\) and \\(y[n]\\) periodic in \\(N\\) with \\(\\omega_0 = \\frac{2\\pi}{N}\\), having DT Fourier coefficients \\(a_k\\) and \\(b_k\\) respectively.\n\nLinearity. The coefficients of the signal \\[z[n] = Ax[n] + By[n] \\mbox{ for constants } A,B\\] are \\(Aa_k + Bb_k\\)\nIndex Shifting. The coefficients of \\[z[n] = x[n-n_0] \\mbox{ are } e^{-jk\\omega_0 n_0}a_k\\] that is, it adds a phase shift.\nFrequency Shift. The coefficients of \\[z[n] = x[n]e^{jm\\omega_0n} \\mbox{ are } a_{k-m}\\]\nIndex Reversal. The coefficients of \\[z[n] = x[-n] \\mbox{ are } a_{-k}\\]\nMultiplication. The coefficients of \\[z[n] = x[n] \\cdot y[n] \\mbox{ are } \\sum\\limits_{m = N_0}^{N_0 + N -1} a_m\\cdot b_{k-m}\\] the discrete convolution of the individual signals’ coefficients.\nConvolution. The coefficients of \\[z[n] = x[n] * y[n] \\mbox{ are } N a_k b_k\\]\nConjugate Symmetry. The coefficients of \\[z[n] = x^*[n] = \\Re{x[n]} - j\\Im{x[n]} \\mbox{ are } a_{-k}^*\\] A consequence of this property is that real, even signals have real, even \\(a_k\\); and real, odd signals have purely imaginary, odd \\(a_k\\). Thus if \\(x[n]\\) is real \\(|a_k|\\) is an even periodic function of \\(k\\) and \\(\\angle a_k\\) is an odd periodic function of \\(k\\).\nParseval’s Relation. The power of the signal with Fourier series coefficients is \\[\\frac{1}{N} \\sum\\limits_{n = N_0}^{N_0 + N -1} |x[n]|^2\\;dt = \\sum\\limits_{k = N_0}^{N_0+N-1} |a_k|^2\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DT Fourier Series</span>"
    ]
  },
  {
    "objectID": "15-dtfs.html#comparison-of-ct-and-dt-fourier-series",
    "href": "15-dtfs.html#comparison-of-ct-and-dt-fourier-series",
    "title": "15  DT Fourier Series",
    "section": "15.3 Comparison of CT and DT Fourier Series",
    "text": "15.3 Comparison of CT and DT Fourier Series\nA summary of the CT and DT Fourier Series is as follows.\nIn CT, a periodic signal \\(x(t)\\) can be decomposed as a countably infinite combination of complex sinusoids at harmonic frequencies of the fundamental. The Fourier series coefficients are a discrete signal that is a-periodic.\n\\[x(t) \\approx \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\, e^{j k\\omega_0 t}\n\\hspace{2em}\na_k = \\frac{1}{T_0} \\int\\limits_{0}^{T_0} x(t)e^{-jk\\omega_0 t} \\; dt\\]\nIn DT, a periodic signal \\(x[n]\\) can be decomposed as a finite combination of complex sinusoids at harmonic frequencies of the fundamental. The Fourier series coefficients are a discrete signal that is periodic.\n\\[x[n] = \\sum\\limits_{k = N_0}^{N_0 + N-1} a_k e^{jk\\omega_0n}\n\\hspace{2em}\na_k = \\frac{1}{N} \\sum\\limits_{n = N_0}^{N_0+N-1} x[n]e^{-j\\frac{2\\pi}{N} k n }\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>DT Fourier Series</span>"
    ]
  },
  {
    "objectID": "16-ctft.html",
    "href": "16-ctft.html",
    "title": "16  CT Fourier Transform",
    "section": "",
    "text": "16.1 Synthesis and Analysis Equation\nRecall the complex exponential \\(e^{st}\\) for \\(s\\in\\mathbb{C}\\) is the Eigenfunction of CT LTI systems. If we can decompose an input into a (possibly infinite) sum of such signals, we can easily determine the output using the superposition principle. In this section we consider the decomposition when the input is aperiodic, called the CT Fourier Transform (CTFT).\nIn contrast to the CT Fourier series, in this case the complex exponent of the Eigenfunction becomes \\(s = j\\omega\\) a continuous variable, and the decomposition is an uncountably infinite sum (integral). This gives the input-output relationship for a stable LTI system as \\[x(t) = \\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{\\infty} X(j\\omega) \\, e^{j \\omega t}\\; d\\omega \\;\\longrightarrow\\; y(t) = \\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{\\infty} H(j\\omega) X(j\\omega) \\, e^{j \\omega t}\\; d\\omega\\] where \\(H(j \\omega)\\) are the Eigenvalues, again called the frequency response. We now turn to determining under what circumstances the decomposition exists and how to find the function \\(X(j\\omega)\\).\nNote: The difference in notation between \\(X(\\omega)\\) and \\(X(j\\omega)\\) is superficial. They generally are the same function. The latter just emphasizes that \\(s \\rightarrow j\\omega\\). For example \\[H(j\\omega) = \\frac{1}{1+(j\\omega)^2} = \\frac{1}{1-\\omega^2} = H(\\omega)\\] are the same function since \\(j^2 = -1\\).\nConsider the aperiodic signal \\[x(t) = \\begin{cases}\n  p(t) & A &lt; t &lt; B\\\\\n  0 & \\text{else}\n\\end{cases}\\] and it’s periodic extension with fundamental frequency \\(\\omega_0 = \\frac{2\\pi}{T_0}\\) \\[x_p(t) = \\sum\\limits_{m = -\\infty}^{\\infty} x(t-mT_0)\\] where \\(T_0 &gt; B-A\\). For example:\nThe CT Fourier series coefficients are \\[\\begin{aligned}\n  a_k &= \\frac{1}{T_0} \\int\\limits_{T_0} x_p(t) e^{-jk\\omega_0 t}\\; dt\\\\\n  &= \\frac{1}{T_0} \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-jk\\omega_0 t}\\; dt \\mbox{ since } x(t) = 0 \\mbox{ outside the interval } (A,B)\\\\\n\\end{aligned}\\] Define the CT Fourier Transform of \\(x(t)\\) as \\[\\boxed{X(\\omega) = \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt}\\] so that \\[a_k = \\frac{1}{T_0} X(k\\omega_0)\\] are samples of \\(X(\\omega)\\) spaced at frequencies \\(\\omega_0\\). By the CT Fourier series synthesis equation \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} \\frac{1}{T_0} X(k\\omega_0) e^{jk\\omega_0 t}\\] Now, let \\(T_0 \\rightarrow \\infty\\) so that the periodic copies move toward \\(\\infty\\) and \\(x_p(t) \\rightarrow x(t)\\). At the same time the frequency sample spacing becomes infinitesimal and \\[X(k\\omega_0) e^{jk\\omega_0 t} \\rightarrow X(\\omega) e^{j\\omega t}\\; d\\omega\\] To give the Inverse Fourier Transform \\[\\boxed{x(t) = \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(\\omega)e^{j\\omega t}\\; d\\omega}\\] This gives the Fourier Transform Pair: \\[\\underbrace{X(\\omega) = \\mathcal{F}\\{x(t)\\} = \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt}_{\\text{Forward Transform / Analysis Equation}}\n\\hspace{3em}\n\\underbrace{x(t) = \\mathcal{F}^{-1}\\{X(\\omega)\\} = \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(\\omega)e^{j\\omega t}\\; d\\omega}_{\\text{Inverse Transform / Synthesis Equation}}\\] The forward transform decomposes \\(x(t)\\) into an infinite number of complex sinusoids. The inverse transform synthesizes a signal as an infinite sum of the sinusoids. It is an example of an Integral Transform. Note the signal \\(x(t)\\) and \\(X(\\omega)\\) are the same signal, just represented in different domains, the time-domain and frequency-domain respectively.\nSimilar to the CT Fourier series, the function \\(X(\\omega)\\) is called the spectrum of the signal \\(x(t)\\). The magnitude spectrum is the function \\(|X(\\omega)|\\) and the phase spectrum is the function \\(\\angle X(\\omega)\\). It is common to plot the spectrum as the combination of the magnitude and phase spectrum.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "16-ctft.html#synthesis-and-analysis-equation",
    "href": "16-ctft.html#synthesis-and-analysis-equation",
    "title": "16  CT Fourier Transform",
    "section": "",
    "text": "Example of a finite-length signal and its periodic extension.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "16-ctft.html#examples-of-the-ct-fourier-transform",
    "href": "16-ctft.html#examples-of-the-ct-fourier-transform",
    "title": "16  CT Fourier Transform",
    "section": "16.2 Examples of the CT Fourier Transform",
    "text": "16.2 Examples of the CT Fourier Transform\n\n\n16.2.1 Fourier Transform of the impulse.\n\n\nConsider the signal \\(x(t) = \\delta(t)\\). The Fourier transform is\n\\[\\begin{aligned}\n    X(\\omega) &= \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{-\\infty}^{\\infty} \\delta(t) e^{-j\\omega t}\\; dt\\\\\n    &= e^{-j\\omega (0)} \\mbox{ by the sifting property}\\\\\n    &= 1\n  \n\\end{aligned}\\]\n\n\n\n\n16.2.2 Fourier transform of the causal real exponential.\n\n\nConsider the signal \\(x(t) = e^{at}u(t)\\) for \\(a\\in \\mathbb{R}\\). The Fourier transform is\n\\[\\begin{aligned}\n    X(\\omega) &= \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{0}^{\\infty}  e^{at} \\, e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{0}^{\\infty}  e^{(a-j\\omega) t}\\; dt\\\\\n    &= \\frac{1}{a-j\\omega } e^{(a-j\\omega) t} \\Big|_{0}^{\\infty}\\\\\n    &= \\frac{1}{a-j\\omega } \\left[ \\lim_{T\\rightarrow\\infty} e^{(a-j\\omega) T} - \\underbrace{e^{(a-j\\omega) (0)}}_{1}\\right]\n  \n\\end{aligned}\\]\nThis example raises the question, of when does the Fourier Transform exist? Note if \\(a &lt; 0\\) then the limit above converges to zero, otherwise the integral diverges. In the former case we say the Fourier transform exists, and in the latter that it does not. Thus\n\\[X(\\omega) = \\frac{-1}{a-j\\omega } = \\frac{1}{j\\omega-a} \\mbox{ for } a &lt; 0\\;.\\]\nNote when \\(a &lt; 0\\), \\(x(t)\\) is an energy signal. A sufficient, but not necessary condition for the Fourier transform to exist is that the signal be an energy signal.\nFor this example, let’s examine the spectrum, noting \\[|X(\\omega)| = \\frac{1}{(a^2 + \\omega^2)^\\frac{1}{2}} \\hspace{2em}\\mbox{and}\\hspace{2em} \\angle X(\\omega) = -\\arctan\\left( \\frac{\\omega}{-a}\\right)\\] plotted below for \\(a = -1\\).\nTODO\n\n\n\n\n16.2.3 Fourier transform of a complex sinusoid.\n\n\nConsider the signal \\(x(t) = e^{j\\omega_0 t}\\) for \\(\\omega_0\\in \\mathbb{R}\\). The Fourier transform is\n\\[\\begin{aligned}\n    X(\\omega) &= \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{-\\infty}^{\\infty}  e^{j\\omega_0 t} \\, e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{-\\infty}^{\\infty}  e^{-j(\\omega_0-\\omega) t}\\; dt\n  \n\\end{aligned}\\]\nFor \\(\\omega \\neq \\omega_0\\) this integral evaluates to\n\\[\\begin{aligned}\n    X(\\omega) &= \\int\\limits_{-\\infty}^{\\infty}  \\cos((\\omega-\\omega_0) t)\\; dt + j \\int\\limits_{-\\infty}^{\\infty}  \\sin((\\omega-\\omega_0) t)\\; dt\\\\\n    &= 0  \n\\end{aligned}\\]\nsince the average value of a sinusoid is zero. When \\(\\omega = \\omega_0\\) this integral diverges \\[\\int\\limits_{-\\infty}^{\\infty}  e^{-j(\\omega_0-\\omega) t}dt = \\int\\limits_{-\\infty}^{\\infty}  e^{-j(0) t} dt= \\int\\limits_{-\\infty}^{\\infty} dt = \\infty\\] What signal is zero everywhere, but infinite at one point (I am hand-waving a bit here)? The delta function \\[X(\\omega) = A\\delta(\\omega-\\omega_0) \\mbox{ for some constant } A.\\] To find the constant we can use the inverse transform\n\\[\\begin{aligned}\n    x(t) &= \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(\\omega)e^{j\\omega t}\\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty}  A\\delta(\\omega-\\omega_0) e^{j\\omega t}\\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} A e^{j\\omega_0 t}\\\\\n    &= e^{j\\omega_0 t}\n  \n\\end{aligned}\\]\nwhich implies \\(A = 2\\pi\\).\n\n\n\n\n16.2.4 Fourier transform of a real sinusoid.\n\n\nConsider the signal \\(x(t) = \\cos(\\omega_0 t)\\) for \\(\\omega_0\\in \\mathbb{R}\\). The Fourier transform can be found using the result in the previous example by noting\n\\[\\begin{aligned}\n    X(\\omega) &= \\int\\limits_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\; dt\\\\\n    &= \\int\\limits_{-\\infty}^{\\infty}  \\cos(\\omega_0 t) \\, e^{-j\\omega t}\\; dt\\\\\n    &= \\frac{1}{2}\\int\\limits_{-\\infty}^{\\infty}   e^{j\\omega_0 t} \\, e^{-j\\omega t}\\; dt + \\frac{1}{2}\\int\\limits_{-\\infty}^{\\infty} e^{-j\\omega_0 t} \\, e^{-j\\omega t}\\; dt\\\\\n    &= \\frac{1}{2} 2\\pi \\delta(\\omega-\\omega_0) + \\frac{1}{2} 2\\pi \\delta(\\omega+\\omega_0)\\\\\n    &= \\pi \\delta(\\omega-\\omega_0) + \\pi \\delta(\\omega+\\omega_0)\n  \n\\end{aligned}\\]\nThis example highlights that the cosine signal is composed of exactly two frequencies.\n\n\n\n\n16.2.5 Inverse Fourier transform of a band-limited signal.\n\n\nConsider the signal \\[X(\\omega) = \\begin{cases}\n    1 & |\\omega| &lt; \\omega_0\\\\\n    0 & \\text{else}\n  \\end{cases}\\] The Inverse Fourier transform is\n\\[\\begin{aligned}\n    x(t) & = \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(\\omega)e^{j\\omega t}\\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\int\\limits_{-\\omega_0}^{\\omega_0} e^{j\\omega t}\\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\frac{1}{jt} \\left[ e^{j\\omega_0 t} - e^{-j\\omega_0 t}\\right]\\\\\n    &= \\frac{1}{\\pi t} \\left[ \\frac{1}{2j}e^{j\\omega_0 t} - \\frac{1}{2j} e^{-j\\omega_0 t}\\right]\\\\\n    &= \\frac{1}{\\pi t} \\sin(\\omega_0 t)\\\\\n    &= \\frac{\\omega_0}{\\pi} \\frac{\\sin(\\omega_0 t)}{\\omega_0 t}\\\\\n    &= \\frac{\\omega_0}{\\pi} \\mbox{sinc}(\\omega_0 t)\n  \n\\end{aligned}\\]\nwhere \\(\\mbox{sinc}()\\) is the (unnormalized) sinc function.\n\n\nTable A.1 lists several CT Fourier transform results.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "16-ctft.html#existence-of-the-ct-fourier-transform",
    "href": "16-ctft.html#existence-of-the-ct-fourier-transform",
    "title": "16  CT Fourier Transform",
    "section": "16.3 Existence of the CT Fourier Transform",
    "text": "16.3 Existence of the CT Fourier Transform\nThe example of the real exponential above showed that for the Fourier transform to exist, the Fourier (analysis) integral must exist. Similar to the Fourier series some mild conditions, called the Dirichlet conditions, are a sufficient prerequisite for the Fourier transform of a signal \\(x(t)\\) to exist:\n\n\\(x(t)\\) is absolutely integrable \\[x(t) = \\int\\limits_{-\\infty}^{\\infty} |x(t)|\\; dt &lt; \\infty\\]\n\\(x(t)\\) has a finite number of minima and maxima over any finite interval\n\\(x(t)\\) has a finite number of finite-valued discontinuities over any finite interval\n\nThese conditions are not necessary however, and we can extend the Fourier transform to a broader class of signals, if we allow delta functions in the transform, as in the cosine example above.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "16-ctft.html#properties-of-the-ct-fourier-transform",
    "href": "16-ctft.html#properties-of-the-ct-fourier-transform",
    "title": "16  CT Fourier Transform",
    "section": "16.4 Properties of the CT Fourier Transform",
    "text": "16.4 Properties of the CT Fourier Transform\nThere are several useful properties of the CT Fourier Transform that, when combined with a table of transforms (see Table 4.2, page 329 of OW), allow us to take the Fourier transform of wide array of signals, and one, the convolution property, that allows us to determine the output of a system in the frequency domain easily. We state these here without proof in rough order of usefulness. See the course text for detailed derivations.\nWe use the notation \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) to indicate the signals are related by a Fourier Transform pair.\n\nLinearity: if \\(x_1(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_1(\\omega)\\) and \\(x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_2(\\omega)\\) then \\[ax_1(t) + bx_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} aX_1(\\omega) + bX_2(\\omega)\\]\nConvolution: if \\(x_1(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_1(\\omega)\\) and \\(x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_2(\\omega)\\) then \\[x_1(t) * x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_1(\\omega)X_2(\\omega)\\] Note in particular if one signal is the system input and the other is the impulse response, the output is the product of the Fourier transforms of each, where the Fourier transform of \\(h(t)\\) is \\(H(\\omega)\\), the Eigenvalue or frequency response.\nDifferentiation: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[\\frac{dx}{dt}(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} j\\omega X(\\omega)\\] This allows us to easily determine the Eigenvalues/Frequency Response from a stable differential equation.\nMultiplication: if \\(x_1(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_1(\\omega)\\) and \\(x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_2(\\omega)\\) then \\[x_1(t) \\cdot x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{2\\pi} X_1(\\omega)*X_2(\\omega)\\] where \\(X_1(\\omega)*X_2(\\omega)\\) is convolution in the frequency domain \\[X_1(\\omega)*X_2(\\omega) = \\int\\limits_{-\\infty}^{\\infty} X_1(\\gamma)\\cdot X_2(\\omega-\\gamma)\\;d\\gamma\\]\nTime-Shift: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[x(t-t_0) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)e^{-j\\omega t_0}\\]\nFrequency-Shift: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[e^{j\\omega_0 t}x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega-\\omega_o)\\]\nConjugate Symmetry: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[x^*(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X^*(-\\omega)\\] This implies that if \\(x(t)\\) is real, then the magnitude spectrum is an even function, and the phase spectrum is an odd function.\nIntegration: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[\\int\\limits_{-\\infty}^t x(\\tau)\\; d\\tau \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{j\\omega} X(\\omega) + \\pi X(0) \\delta(\\omega)\\]\nTime and Frequency Scaling: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then if \\(a\\) is a real constant \\[x(at) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{|a|} X\\left(\\frac{\\omega}{a}\\right)\\]\nParseval’s Relation: if \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(\\omega)\\) then \\[\\int\\limits_{0-\\infty}^{\\infty} |x(t)|^2\\; dt =  \\frac{1}{2\\pi}\\int\\limits_{0-\\infty}^{\\infty} |X(\\omega)|^2\\;d\\omega\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "16-ctft.html#ct-fourier-transform-of-a-periodic-signal",
    "href": "16-ctft.html#ct-fourier-transform-of-a-periodic-signal",
    "title": "16  CT Fourier Transform",
    "section": "16.5 CT Fourier Transform of a Periodic Signal",
    "text": "16.5 CT Fourier Transform of a Periodic Signal\nEven though the Fourier transform was derived in the case of an a-periodic signal, the linearity property of the transform, combined with one of our examples above shows us that we can take the Fourier Transform of a periodic signal. Consider a periodic signal with Fourier series expansion \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k e^{jk\\omega_0 t}\\] Taking the Fourier Transform \\[\\mathcal{F}\\{x(t)\\} = \\mathcal{F}\\left\\{\\sum\\limits_{k = -\\infty}^{\\infty} a_k e^{jk\\omega_0 t}\\right\\} = \\sum\\limits_{k = -\\infty}^{\\infty} a_k \\mathcal{F}\\{e^{jk\\omega_0 t}\\} = \\sum\\limits_{k = -\\infty}^{\\infty} a_k 2\\pi \\delta(\\omega-k\\omega_0)\\] Thus the discrete Fourier series coefficients become the weights of the corresponding delta functions centered at the harmonic frequency.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>CT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "17-dtft.html",
    "href": "17-dtft.html",
    "title": "17  DT Fourier Transform",
    "section": "",
    "text": "17.1 Analysis and Synthesis Equations\nRecall the complex exponential \\(z^{n}\\) for \\(z\\in\\mathbb{C}\\) is the Eigenfunction of DT LTI systems. If we can decompose an input into a (possibly infinite) sum of such signals, we can easily determine the output using the superposition principle. In this section we consider the decomposition when the input is aperiodic, called the DT Fourier Transform (DTFT).\nIn contrast to the DT Fourier series, in this case the complex exponent of the Eigenfunction becomes \\(z = e^{j\\omega}\\) a continuous variable, and the decomposition is an uncountably infinite sum (integral). This gives the input-output relationship for a stable DT LTI system as \\[x[n] = \\frac{1}{2\\pi}\\int\\limits_{2\\pi} X\\left(e^{j\\omega}\\right) \\, e^{j \\omega n}\\; d\\omega \\;\\longrightarrow\\; y[n] = \\frac{1}{2\\pi}\\int\\limits_{2\\pi} H\\left(e^{j\\omega}\\right) X\\left(e^{j\\omega}\\right) \\, e^{j \\omega n}\\; d\\omega\\] where \\(H\\left(e^{j \\omega}\\right)\\) are the Eigenvalues, again called the frequency response. We now turn to determining under what circumstances the decomposition exists and how to find the function \\(X\\left(e^{j\\omega}\\right)\\).\nNote: The notation \\(X\\left(e^{j\\omega}\\right)\\) can be confusing. It just emphasizes that \\(z \\rightarrow e^{j\\omega}\\). The expressions are functions of the independent variable \\(\\omega\\).\nConsider the Fourier series of \\(x[n]\\), a periodically extended finite-length DT signal \\(\\tilde{x}[n]\\), e.g.\nTODO graphics/dt-derivation.pdf\nwhere \\(\\tilde{x}[n]\\) is zero outside the range \\([N_1,N_2]\\). Since \\(x[n] = \\tilde{x}[n]\\) over the interval \\(-N_1\\) to \\(N_2\\) \\[a_k = \\frac{1}{N}\\sum\\limits_{n = -N_1}^{N_2} \\tilde{x}[n] e^{-j\\frac{2\\pi}{N}kn} = \\frac{1}{N}\\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\frac{2\\pi}{N}kn}\\] Define the function \\(X\\left(e^{j\\omega}\\right) = \\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\omega n}\\), then \\[a_k = \\frac{1}{N} X\\left(e^{jk\\omega_0}\\right)\\] are samples of \\(X\\left(e^{j\\omega}\\right)\\) at locations that are multiples of \\(\\omega_0 = \\frac{2\\pi}{N}\\). Substituting back into the synthesis equation \\[\\tilde{x}[n] = \\sum\\limits_{k = -N_1}^{N_2} a_k e^{j\\frac{2\\pi}{N}kn} = \\sum\\limits_{k = -N_1}^{N_2} \\frac{1}{N} X\\left(e^{jk\\omega_0}\\right) e^{jk\\omega_0 n}\\] Now note that \\(N = \\frac{2\\pi}{\\omega_0}\\) so that \\[\\tilde{x}[n] = \\frac{1}{2\\pi} \\sum\\limits_{k = -N_1}^{N_2} X\\left(e^{jk\\omega_0}\\right) e^{jk\\omega_0 n} \\; \\omega_0\\] Now let \\(N \\rightarrow \\infty\\).\nThis is shown graphically in the figure below. As \\(N\\) approaches infinity the sampling of the unit circle becomes infinite, and the summation approaches an integral.\nTODO graphics/dt-fourier-limit.pdf\nThis gives the DT Fourier Transform Pair. The Analysis Equation or Forward Transform is: \\[X\\left(e^{j\\omega}\\right) = \\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\omega n}\\] Note \\(X\\left(e^{j\\omega}\\right)\\) must be a periodic function with period \\(2\\pi\\). The Synthesis Equation or Inverse Transform is: \\[x[n] = \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\omega}\\right) e^{j\\omega n} \\; d\\omega\\] where the integral is over any \\(2\\pi\\) period of \\(X\\).\nTable [table:dtft] lists several DT Fourier Transform results.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>DT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "17-dtft.html#analysis-and-synthesis-equations",
    "href": "17-dtft.html#analysis-and-synthesis-equations",
    "title": "17  DT Fourier Transform",
    "section": "",
    "text": "\\[\\begin{aligned}\n  \\lim_{N\\rightarrow \\infty} \\tilde{x}[n] &= \\lim_{N\\rightarrow \\infty} \\frac{1}{2\\pi} \\sum\\limits_{k = -N_1}^{N_2} X\\left(e^{jk\\omega_0}\\right) e^{jk\\omega_0 n} \\; \\omega_0\\\\\n  x[n] &= \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\omega}\\right) e^{j\\omega n} \\; d\\omega\n\\end{aligned}\\]\n\n\n\n\n\nExample\n\n\nLet \\(x[n] = \\delta[n]\\)\n\\[\\begin{aligned}\n    X\\left(e^{j\\omega}\\right) &= \\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\omega n}\\\\\n    &= \\sum\\limits_{n = -\\infty}^{\\infty} \\delta[n] e^{-j\\omega n}\\\\\n    &= e^{-j\\omega (0)}\\\\\n    &= 1\n  \n\\end{aligned}\\]\n\n\n\n\nExample\n\n\nLet \\(x[n] = \\left( \\gamma \\right)^n\\; u[n]\\)\n\\[\\begin{aligned}\n    X\\left(e^{j\\omega}\\right) &= \\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\omega n}\\\\\n    &= \\sum\\limits_{n = 0}^{\\infty} \\left( \\gamma \\right)^n \\; e^{-j\\omega n}\\\\\n    &= \\sum\\limits_{n = 0}^{\\infty} \\left( \\gamma e^{-j\\omega} \\right)^n\n  \n\\end{aligned}\\]\nUsing the geometric series \\(\\sum\\limits_{n = 0}^{\\infty} z^n = \\frac{1}{1-z}\\) for \\(|z| &lt; 1\\) gives: \\[X\\left(e^{j\\omega}\\right) = \\sum\\limits_{n = 0}^{\\infty} \\left( \\gamma e^{-j\\omega} \\right)^n = \\frac{1}{1-\\gamma e^{-j\\omega}} = \\frac{e^{j\\omega}}{e^{j\\omega} - \\gamma}\\] If \\(\\mid\\gamma e^{-j\\omega}\\mid &lt; 1\\) or equivalently \\(\\mid \\gamma \\mid &lt; 1\\).\n\\[\\left( \\gamma \\right)^n\\; u[n] \\; \\stackrel{\\mathcal{F}}{\\longrightarrow} \\; \\frac{1}{1-\\gamma e^{-j\\omega}}\\] Below is a plot of the original signal and the magnitude and phase spectrum when \\(\\gamma = \\tfrac{1}{2}\\).\nTODO graphics/dtft-example1-x.pdf\nTODO graphics/dtft-example1-Xmag.pdf\nTODO graphics/dtft-example1-Xarg.pdf\n\n\n\n\nExample\n\n\nLet \\[X\\left(e^{j\\omega}\\right) = \\left\\{ \\begin{array}{lc}\n    1 & |\\omega -2\\pi k| &lt; \\omega_c\\\\\n    0 & \\text{else}\n  \\end{array}\n  \\right.\n  \\; \\text{for}\\; k\\in\\mathbb{Z} \\;\\text{and}\\; \\omega_c &lt; \\pi\\]\n\\[\\begin{aligned}\n    x[n] &= \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\omega}\\right) e^{j\\omega n} \\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\int\\limits_{-\\omega_c}^{\\omega_c}  e^{j\\omega n} \\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\frac{1}{jn} e^{j\\omega n} \\Bigg|_{-\\omega_c}^{\\omega_c}\\\\\n    &= \\frac{1}{\\pi n} \\left( \\frac{1}{2j} e^{j\\omega_c n} - \\frac{1}{2j} e^{-j\\omega_c n} \\right)\\\\\n    &= \\frac{1}{\\pi n} \\sin(\\omega_c n) = \\frac{\\omega_c}{\\pi}\\text{sinc}(\\omega_c n)\n  \n\\end{aligned}\\]\n\n\n\n\nExample\n\n\nLet \\[X\\left(e^{j\\omega}\\right) = \\sum\\limits_{k = -\\infty}^{\\infty} \\delta(\\omega-\\omega_0 -2\\pi k)\\] for \\(-\\pi &lt; \\omega_0 &lt; \\pi\\)\nTODO graphics/dtft-periodic-ex.pdf\n\\[\\begin{aligned}\n    x[n] &= \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\omega}\\right) e^{j\\omega n} \\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} \\int\\limits_{-\\pi}^{\\pi} \\delta(\\omega-\\omega_0)e^{j\\omega n} \\; d\\omega\\\\\n    &= \\frac{1}{2\\pi} e^{j\\omega_0 n}\n  \n\\end{aligned}\\]",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>DT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "17-dtft.html#existence-of-the-dt-fourier-transform",
    "href": "17-dtft.html#existence-of-the-dt-fourier-transform",
    "title": "17  DT Fourier Transform",
    "section": "17.2 Existence of the DT Fourier Transform",
    "text": "17.2 Existence of the DT Fourier Transform\nThe example of the exponential \\(x[n] = \\left(\\gamma\\right)^n\\,u[n]\\) above showed that for the DT Fourier transform to exist, the Fourier (analysis) sum must exist. Similar to the CT Fourier transform, a mild conditions is a sufficient prerequisite for the Fourier transform of a signal \\(x[n]\\) to exist: it must be absolutely summable \\[\\sum\\limits_{n = -\\infty}^{\\infty} |x[n]| &lt; \\infty\\]\nThis conditions is not necessary however, and we can extend the Fourier transform to a broader class of signals, if we allow delta functions in the transform, as in the sinusoidal examples above.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>DT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "17-dtft.html#properties-of-the-dt-fourier-transform",
    "href": "17-dtft.html#properties-of-the-dt-fourier-transform",
    "title": "17  DT Fourier Transform",
    "section": "17.3 Properties of the DT Fourier Transform",
    "text": "17.3 Properties of the DT Fourier Transform\nThere are several useful properties of the DT Fourier Transform that, when combined with a table of transforms (see Table 5.2, page 392 of OW), allow us to take the Fourier transform of wide array of signals, and one, the convolution property, that allows us to determine the output of a system in the frequency domain easily. We state these here without proof in rough order of usefulness. See the course text for detailed derivations.\nWe use the following notation \\[\\mathcal{F}\\left\\{ x[n] \\right\\} = X\\left(e^{j\\omega}\\right) = \\sum\\limits_{n = -\\infty}^{\\infty} x[n] e^{-j\\omega n}\\] \\[\\mathcal{F}^{-1}\\left\\{ X\\left(e^{j\\omega}\\right) \\right\\} = x[n] =  \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\omega}\\right) e^{j\\omega n} \\; d\\omega\\] \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\;  X\\left(e^{j\\omega}\\right)\\] Important: \\(X\\left(e^{j\\omega}\\right)\\) is periodic in \\(2\\pi\\) such that \\[X\\left(e^{j(\\omega + 2\\pi k)}\\right) =  X\\left(e^{j\\omega}\\right) \\;\\text{for}\\; k \\in \\mathbb{Z}\\]\n\nLinearity Property. Let \\(x_1[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X_1\\left(e^{j\\omega}\\right)\\) and \\(x_2[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X_2\\left(e^{j\\omega}\\right)\\) then for \\(a,b\\in\\mathbb{C}\\) \\[a x_1[n] + b x_2[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; a X_1\\left(e^{j\\omega}\\right) + b X_2\\left(e^{j\\omega}\\right)\\] Example:\n\\[\\begin{aligned}\n\\mathcal{F}\\left\\{ 2\\left( \\frac{1}{2}\\right)^nu[n] -5 \\left( -\\frac{1}{4}\\right)^nu[n] \\right\\} &= \\frac{2}{1-\\frac{1}{2}e^{-j\\omega}} - \\frac{5}{1+\\frac{1}{4}e^{-j\\omega}}\n\\end{aligned}\\]\nTime-shift Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[x[n-n_0] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; e^{-j\\omega n_0} X\\left(e^{j\\omega}\\right)\\] Example: \\[\\mathcal{F}\\left\\{ \\delta[n-5] \\right\\} = e^{-j5\\omega}\\]\nFrequency Shift Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[e^{j\\omega_0 n} x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j(\\omega-\\omega_0)}\\right)\\] Example: \\[\\mathcal{F}^{-1}\\left\\{ \\frac{1}{1-\\frac{1}{2}e^{-j\\omega}e^{j\\frac{\\pi}{20}}} \\right\\} = e^{j\\frac{\\pi}{20} n} \\left( \\frac{1}{2}\\right)^n u[n]\\]\nConjugation Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[x^*[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X^{*}\\left(e^{-j\\omega}\\right)\\] Thus, if \\(x[n]\\) is real \\(X\\left(e^{j\\omega}\\right)\\) has conjugate symmetry \\[X\\left(e^{-j\\omega}\\right) = X^*\\left(e^{j\\omega}\\right)\\] and the magnitude spectrum is an even function and the phase spectrum is an odd function.\nDifferencing and Accumulation Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[x[n] - x[n-1] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right) - e^{-j\\omega} X\\left(e^{j\\omega}\\right) = \\left(1-e^{-j\\omega}\\right)X\\left(e^{j\\omega}\\right)\\] and \\[\\sum\\limits_{m = -\\infty}^{n} x[m] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; \\frac{1}{1-e^{-j\\omega}}X\\left(e^{j\\omega}\\right) + \\pi X\\left(e^{j0}\\right)\\sum\\limits_{k = -\\infty}^{\\infty} \\delta(\\omega - 2\\pi k)\\]\nTime Expansion Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[x_{(k)}[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{jk\\omega}\\right)\\] where \\[x_{(k)}[n] = \\left\\{ \\begin{array}{lc}\n    x[n/k] & \\text{if}\\; n = \\; \\text{multiple of}\\; k\\\\\n    0 & \\text{if}\\; n \\neq \\; \\text{multiple of}\\; k\\\\\n  \\end{array}\n\\right.\\]\n\nTODO graphics/dt=transform-property-expand.pdf)\n\nFrequency Differentiation Property Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[n\\, x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; j \\frac{d}{d\\omega} X\\left(e^{j\\omega}\\right)\\] Example:\n\\[\\begin{aligned}\n    \\mathcal{F}\\left\\{ n\\left( \\frac{1}{8}\\right)^n u[n] \\right\\} &= j \\frac{d}{d\\omega} \\left\\{\\frac{1}{1-\\frac{1}{8}e^{-j\\omega}} \\right\\}\\\\\n    &= j \\frac{-\\left(-\\frac{1}{8} (-j)e^{-j\\omega}\\right)}{\\left( 1-\\frac{1}{8}e^{-j\\omega} \\right)^2}\\\\\n    &= \\frac{\\frac{1}{8}e^{-j\\omega}}{\\left( 1-\\frac{1}{8}e^{-j\\omega} \\right)^2}\n\n\\end{aligned}\\]\nParseval’s Relation. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] then \\[\\sum\\limits_{n = -\\infty}^{\\infty} |x[n]|^2 \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\;\n  \\frac{1}{2\\pi} \\int_{2\\pi} \\left| X\\left(e^{j\\omega}\\right) \\right|^2 \\; d\\omega\\] The energy is also the integral over one period of the DTFT magnitude squared.\nConvolution Property. Recall for a DT LTI system with impulse response \\(h[n]\\) the output is \\[y[n] = h[n]*x[n]\\] In the frequency domain this is equivalent to \\[Y\\left(e^{j\\omega}\\right) = H\\left(e^{j\\omega}\\right) \\, X\\left(e^{j\\omega}\\right)\\] As in CT systems, convolution in the discrete-time domain is equivalent to multiplication in the frequency domain. Example: suppose a DT system has impulse response \\[h[n] = \\left( \\gamma_1^n + \\gamma_2^n \\right)\\, u[n]\\] and the input is \\(x[n] = n\\, \\gamma_3^n\\, u[n]\\) where \\(|\\gamma_1| &lt; 1\\), \\(|\\gamma_2| &lt; 1\\), \\(|\\gamma_3| &lt; 1\\). The output in the frequency domain is\n\\[\\begin{aligned}\n    Y\\left(e^{j\\omega}\\right) &= H\\left(e^{j\\omega}\\right) \\, X\\left(e^{j\\omega}\\right)\\\\\n    &= \\left[ \\frac{1}{1-\\gamma_1 e^{-j\\omega}} + \\frac{1}{1-\\gamma_2 e^{-j\\omega}} \\right] \\frac{\\gamma_3 e^{-j\\omega}}{\\left( 1-\\gamma_3 e^{-j\\omega} \\right)^2}\\\\\n    &= \\frac{\\gamma_3 e^{-j\\omega}}{\\left(1-\\gamma_1 e^{-j\\omega}\\right)\\left( 1-\\gamma_3 e^{-j\\omega} \\right)^2} + \\frac{\\gamma_3 e^{-j\\omega}}{\\left(1-\\gamma_2 e^{-j\\omega}\\right)\\left( 1-\\gamma_3 e^{-j\\omega} \\right)^2}\n\n\\end{aligned}\\]\nMultiplication (modulation) Property. Let \\[x[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; X\\left(e^{j\\omega}\\right)\\] and \\[y[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; Y\\left(e^{j\\omega}\\right)\\] then \\[x[n]\\,y[n] \\; \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\; \\frac{1}{2\\pi} \\int_{2\\pi} X\\left(e^{j\\theta}\\right)\\, Y\\left(e^{j(\\omega-\\theta)}\\right)\\; d\\theta\\]",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>DT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "17-dtft.html#dt-fourier-transform-of-a-periodic-signal",
    "href": "17-dtft.html#dt-fourier-transform-of-a-periodic-signal",
    "title": "17  DT Fourier Transform",
    "section": "17.4 DT Fourier Transform of a Periodic Signal",
    "text": "17.4 DT Fourier Transform of a Periodic Signal\nThe DTFS allows us to write any periodic function with period \\(N\\) as \\[x[n] = \\sum\\limits_{k = N_0}^{N_0 + N -1} a_k e^{j\\frac{2\\pi}{N}kn}\\] taking the DT Fourier Transform \\[X\\left(e^{j\\omega}\\right) = \\sum\\limits_{k = N_0}^{N_0 + N -1} a_k \\mathcal{F}\\left\\{e^{j\\frac{2\\pi}{N}kn}\\right\\}\\] Using the previously derived transform shows, similar to CT, the DT Fourier Transform of a periodic signal is \\[X\\left(e^{j\\omega}\\right) = \\sum\\limits_{k = -\\infty}^{\\infty} 2\\pi a_k \\delta\\left(\\omega - \\frac{2\\pi k}{N}\\right)\\]\nExample \\[x[n] = \\cos\\left(\\frac{2\\pi}{10} n\\right) = \\frac{1}{2}e^{j\\frac{2\\pi}{10} n} + \\frac{1}{2}e^{-j\\frac{2\\pi}{10} n}\\] Using the previous transform \\[X\\left(e^{j\\omega}\\right) =  \\sum\\limits_{k = -\\infty}^{\\infty} \\pi \\delta\\left(\\omega - \\frac{2\\pi}{10} -2\\pi k\\right) + \\pi \\delta\\left(\\omega + \\frac{2\\pi}{10} -2\\pi k\\right)\\] Which looks like\nTODO graphics/dtft-periodic-ex2",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>DT Fourier Transform</span>"
    ]
  },
  {
    "objectID": "18-ct-fr.html",
    "href": "18-ct-fr.html",
    "title": "18  CT Frequency Response",
    "section": "",
    "text": "18.1 Determining the frequency response (FR) of a CT system\nIn this lecture we are going to focus on the frequency response and highlight its importance in linear systems theory.\nThe frequency response of a CT LTI system can be thought of as arising in several equivalent ways. What follows is a common, but not exhaustive, list of ways the frequency response can be derived from other representations.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>CT Frequency Response</span>"
    ]
  },
  {
    "objectID": "18-ct-fr.html#determining-the-frequency-response-fr-of-a-ct-system",
    "href": "18-ct-fr.html#determining-the-frequency-response-fr-of-a-ct-system",
    "title": "18  CT Frequency Response",
    "section": "",
    "text": "Using the Eigenvalues / Transfer Function\nRecall if we apply the Eigenfunction \\(e^{st}\\) for the complex frequency \\(s \\in \\mathbb{C}\\) as the input to a LTI system, the output is the Eigenfunction scaled by the Eigenvalue (transfer function) \\(H(s)\\) for values of \\(s\\) in the region of convergence, where \\[H(s) = \\int\\limits_{-\\infty}^{\\infty} h(t) e^{-st}\\; dt \\; .\\] is the bilateral Laplace transform of the impulse response.\nTODO 18-ct-tf.pdf\nIf a system is stable, then the region of convergence includes the imaginary axis \\(s = j\\omega\\). In that case, evaluating the Eigenvalues on the imaginary axis \\(s = j\\omega\\) gives the CT frequency response \\(H(j\\omega)\\). This converts from a function of a complex variable, \\(s\\), to one of a real variable \\(\\omega\\).\n\n\nExample\n\n\nConsider a system with Eigenvalues (transfer function) \\[H(s) = \\frac{2}{s+5}\\mbox{ for } \\Re{s} &gt; -5\\] Determine the frequency response of the system, if possible.\nSolution: We first need to check if the system is stable using the region-of-convergence. Since the real part of the region of convergence includes the imaginary axis (\\(\\Re s = 0\\)), the system is stable. To find the frequency response we substitute \\(s = j\\omega\\) to give \\[H(j\\omega) = \\frac{2}{j\\omega+5}\\]\n\n\n\n\n\nExample\n\n\nConsider an apparently similar system with Eigenvalues \\[H(s) = \\frac{2}{s-5}\\mbox{ for } \\Re{s} &gt; 5\\] Determine the frequency response of the system, if possible.\nSolution: Again, we first need to check of the system is stable using the region-of-convergence. Since the real part of the region of convergence does not include the imaginary axis (\\(\\Re s = 0\\)), the system is unstable. Thus, the frequency response does not exist.\n\n\n\n\n\nUsing the CTFT\nAnother way we can view the frequency response is as the CT Fourier Transform of the impulse response. If the system is stable, then the impulse response is absolutely integrable, and the Fourier transform exists giving \\(H(j\\omega) = \\mathcal{F}\\left\\{h(t)\\right\\}\\). This is connected to the transfer function by noting the bilateral Laplace transform and the Fourier Transform are identical under the substitution \\(s = j\\omega\\), which is allowed if the system is stable.\n\n\nExample\n\n\nSuppose the impulse response of a CT LTI system is given by \\[h(t) = \\left(e^{-t}-e^{-6t}\\right)u(t)\\] Determine the frequency response of the system, if possible.\nSolution: If the system is stable, the Fourier transform of the impulse response exists. Since \\[\\int\\limits_{0}^{\\infty} \\left| e^{-t}-e^{-6t} \\right| \\; dt &lt; \\int\\limits_{0}^{\\infty} e^{-t} \\; dt &lt; \\infty\\] the system is stable and the Fourier Transform exists, giving \\[H(j\\omega) = \\mathcal{F}\\left\\{ \\left(e^{-t}-e^{-6t}\\right)u(t) \\right\\} = \\mathcal{F}\\left\\{ \\left(e^{-t}u(t) \\right\\} - \\mathcal{F}\\left\\{e^{-6t}\\right)u(t) \\right\\} = \\frac{1}{j\\omega + 1} - \\frac{1}{j\\omega + 6} = \\frac{7}{6-\\omega^2 + j7\\omega}\\]\n\n\n\n\n\nDirectly from a LCCDE\nBy the convolution theorem of the CTFT, the frequency response is the ratio of the output to input in the frequency domain, i.e. \\[H(j\\omega) = \\frac{Y(j\\omega)}{X(j\\omega)}\\] We can easily determine this ratio from the LCCDE representation of the system using the derivative property of the Fourier Transform. Recall this property states if \\(\\mathcal{F}\\{x(t)\\} = X(j\\omega)\\) then \\[\\mathcal{F}\\left\\{\\frac{d^n x}{dt^n}(t) \\right\\} = (j\\omega)^n  X(j\\omega) \\; .\\]\nIf the system is stable (and thus the frequency response exists) then all roots of the characteriztic equation \\(Q(D)\\) have real parts that are less than zero. If the system is stable we can take the Fourier transform of each term of the LCCDE using the derivative property, then algebrically solve for the ratio of output to input. Note this provides a signifigant savings in analysis effort since we do not have to first find the impulse response, then take its Fourier transform to arrive at the frequency response (although that approach is still valid).\n\n\nExample\n\n\nConsider a system decribed by the LCCDE \\[\\frac{d^2y}{dt^2}(t) + 15\\frac{dy}{dt}(t) + 50y(t) = 10x(t)\\] Determine the frequency response of the system, if possible.\nSolution: We first need to check for stability. The characteristic equation is \\(Q(D) = D^2 + 15D + 50\\) which has two real roots \\(-10\\) and \\(-5\\). Since both are less than zero, the system is stable. Next we take the Fourier transform of both sides and apply the derivative property \\[(j\\omega)^2Y(j\\omega) + 15(j\\omega) Y(j\\omega) + 50Y(j\\omega) = 10X(j\\omega)\\] and rearrange to get the frequency response \\[H(j\\omega) = \\frac{Y(j\\omega)}{X(j\\omega)} = \\frac{10}{(j\\omega)^2 + 15(j\\omega) + 50} = \\frac{10}{50-\\omega^2 + j15\\omega}\\]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>CT Frequency Response</span>"
    ]
  },
  {
    "objectID": "18-ct-fr.html#magnitude-phase-representation-of-the-ctfr",
    "href": "18-ct-fr.html#magnitude-phase-representation-of-the-ctfr",
    "title": "18  CT Frequency Response",
    "section": "18.2 Magnitude-phase representation of the CTFR",
    "text": "18.2 Magnitude-phase representation of the CTFR\nNote that any complex valued function can be expressed in polar form using the magnitude and phase. Specifically the input and output can be put into this form \\[X(j\\omega) = |X(j\\omega)|e^{\\angle X(j\\omega)}\\] \\[Y(j\\omega) = |Y(j\\omega)|e^{\\angle Y(j\\omega)}\\]\nBy the convolution theorem then \\[H(j\\omega) = \\frac{Y(j\\omega)}{X(j\\omega)} = \\frac{|Y(j\\omega)|e^{\\angle Y(j\\omega)}}{|X(j\\omega)|e^{\\angle X(j\\omega)}} = \\frac{|Y(j\\omega)|}{|X(j\\omega)|}e^{\\angle Y(j\\omega) - \\angle X(j\\omega)} = |H(j\\omega)|e^{\\angle H(j\\omega)}\\] Thus we see that \\[|H(j\\omega)| = \\frac{|Y(j\\omega)|}{|X(j\\omega)|}\\] and \\[\\angle H(j\\omega) = \\angle Y(j\\omega) - \\angle X(j\\omega)\\] This is the magnitude and phase representation of the frequency response.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>CT Frequency Response</span>"
    ]
  },
  {
    "objectID": "18-ct-fr.html#ctfr-acting-on-sinusoids",
    "href": "18-ct-fr.html#ctfr-acting-on-sinusoids",
    "title": "18  CT Frequency Response",
    "section": "18.3 CTFR acting on sinusoids",
    "text": "18.3 CTFR acting on sinusoids\nThe advantage of the magnitude and phase representation of the frequency response, is the ease with which we can find the output due to a sinusoidal input. If we apply a sinusoidal input \\(x(t) = A e^{j\\omega t}\\), the output is a the same sinusoid scaled by the frequency response \\(y(t) = H(j\\omega) A e^{j\\omega t}\\).\nTODO 18-ct-fr.pdf)\nNow using the magnitude and phase representation \\[y(t) = H(j\\omega) A e^{j\\omega t} = |H(j\\omega)|e^{\\angle H(j\\omega)} A e^{j\\omega t} = A |H(j\\omega)| e^{j\\omega t + \\angle H(j\\omega)}\\] Thus we can interpret the frequency response as telling us how the input sinsusoids are scaled in magnitude and phase shifted as they pass through the system.\nBy the linearity property this extends to real sinusoidal inputs since \\[\\begin{aligned}\n  x(t) &\\longrightarrow y(t)\\\\\n  \\sin(\\omega t) &\\longrightarrow \\frac{1}{2j}|H(j\\omega)| e^{j\\omega t + \\angle H(j\\omega)} - \\frac{1}{2j}|H(j\\omega)| e^{-j\\omega t + \\angle H(j\\omega)}\\\\\n  \\sin(\\omega t) &\\longrightarrow |H(j\\omega)|\\sin(\\omega t + \\angle H(j\\omega))  \n\\end{aligned}\\] and \\[\\begin{aligned}\n  x(t) &\\longrightarrow y(t)\\\\\n  \\cos(\\omega t) &\\longrightarrow \\frac{1}{2}|H(j\\omega)| e^{j\\omega t + \\angle H(j\\omega)} + \\frac{1}{2}|H(j\\omega)| e^{-j\\omega t + \\angle H(j\\omega)}\\\\\n  \\cos(\\omega t) &\\longrightarrow |H(j\\omega)|\\cos(\\omega t + \\angle H(j\\omega))  \n\\end{aligned}\\]\nAlso by the linearity property this analysis extends to the CT Fourier representation of a signal (an infinite sum of sinusoids): \\[x(t) = \\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{\\infty} X(j \\omega) \\, e^{j \\omega t}\\; d\\omega \\;\\longrightarrow\\; y(t) = \\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{\\infty} H(j \\omega) X(j \\omega) \\, e^{j \\omega t}\\; d\\omega = \\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{\\infty} \\left| H(j \\omega)\\right| X(j \\omega) \\, e^{j \\omega t + \\angle H(j \\omega)}\\; d\\omega\\]\nThus we arrive at the reason for the name Frequency Response – it specifies the the response of a stable system to any linear combination of sinusoidal inputs, i.e. any signal with a Fourier Transform.\n\n18.3.1 Bode plots\nWe can visualize the frequency response as a plot of the real and imaginary part, or, of the magnitude and phase. Since the magnitude and phase allow us to directly see the system behavior at a given frequency, those plots are much more useful.\nRather than simply plot the magnitude and phase as a function of \\(\\omega\\), it is common to change the abscissa (horizontal / \\(\\omega\\)-axis) to be on a logarithmic scale and so only plot the positive frequency portion of the spectrum (recall if the signal is real, the frequency response is even, so no information is lost). This is because the frequency response for physically realizable systems changes slowly as a function of frequency. Plotting on a log-scale compresses this information horizontally so that we can see how a wide range of frequency content is scaled. When plotting the magnitude spectrum it is also common to make the ordinate (vertical / gain axis) to be in decibels (dB). This is because of Weber’s law, which states the humans perceive a doubling in strength of stimulus, when it is actually a ten-fold increase. Thus the magnitude of the frequency response in dB is \\(20 \\log_{10} |H(j\\omega)|\\). When the frequency response is plotted this particular way we get what is called a Bode plot (after the engineer Hendrik Wade Bode, an important figure in the development of control theory).\nYou will likely encounter Bode plots at several points in your career, so it is important to understand them well enough to create them on your own using software and read them. Also data-sheets and other documentation for CT devices generally use a Bode plot rather than giving an explicit mathematical model of the frequency response. It is also instructive to learn how to plot them manually (which was the traditional way to do it) since it gives you insight that can help with reverse engineering a model, however we do not cover this in detail in this course. Note that we will plot the spectrum as a function of frequency in units of rad/s, but it is also common to see it plotted in units of Hz. Take care to read the horizontal axis label as mixing up the two is a common source of error.\n\n\nExample\n\n\nConsider a frequency response given by \\[H(j\\omega) = \\frac{20000}{(j\\omega)^2 + 300(j\\omega) + 20000}\\] The following Matlab code shows you how to plot the spectrum as a Bode plot (with some extra code to make it look nicer). You should read the documentation for the bode command in Matlab. It is also easy to just compute the magnitude and phase yourself.\n    H = tf([20000],[1,300,20000]);\n    [mag,ph,w] = bode(H);\n\n    % Create a nice bode plot \n    hFig = figure();\n    hold on;\n\n    subplot(2,1,1);\n    hm = semilogx(w,20*log10(squeeze(mag)));\n    grid on;\n    hTitle  = title ('Frequency Response');\n    hYLabel1 = ylabel('Magnitude (dB)');\n    set(gca, 'FontSize', 14, 'YTick', -60:10:20, ...\n        'Box', 'off', 'LineWidth', 2);\n\n    subplot(2,1,2);\n    hp = semilogx(w,squeeze(ph*(pi/180)));\n    grid on;\n    hYLabel2 = ylabel('Phase (radians)');\n    hXLabel = xlabel('Frequency (rad/s)');\n    set(gca, 'FontSize', 14, 'Box', 'off', 'LineWidth', 2);\n\n    set(hm, 'linewidth', 2);\n    set(hp, 'linewidth', 2);\n    set([hXLabel, hYLabel1, hYLabel2]  , ...\n         'FontSize'   , 14          );\n    set( hTitle                    , ...\n         'FontSize'   , 14          , ...\n         'FontWeight' , 'bold'      );\nThis gives the following plot\nTODO lecture20_1.png\n\n\nTo read a Bode plot to see the behavior of the system at a given frequency, one need only read the values off the plot and convert from dB to a unit-less gain. A common mistake is to not realize the horizontal axis is logarithmic.\n\n\nExample\n\n\nSuppose you are given the Bode plot (only) from the previous example and are asked what the output of the system is when the input is \\(x(t) = \\cos(2\\pi 32 t)\\), i.e. a sinusoid at 32 Hz.\nSolution: First we determine the frequency in rad/s, \\(\\omega = 2\\pi 32 \\approx 200\\) rad/s. We go to that frequency on the Bode plot and read off a value of about \\(-10\\) dB for the magnitude and about \\(-1.9\\) rad for the phase. To convert back from dB \\[|H(200)| = 10^{\\frac{-10}{20}} \\approx 0.3\\] so the output would be \\[y(t) \\approx 0.3\\cos(2\\pi 32 t - 1.9)\\]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>CT Frequency Response</span>"
    ]
  },
  {
    "objectID": "18-ct-fr.html#ctfr-of-first-and-second-order-systems",
    "href": "18-ct-fr.html#ctfr-of-first-and-second-order-systems",
    "title": "18  CT Frequency Response",
    "section": "18.4 CTFR of first and second order systems",
    "text": "18.4 CTFR of first and second order systems\nTODO",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>CT Frequency Response</span>"
    ]
  },
  {
    "objectID": "19-dt-fr.html",
    "href": "19-dt-fr.html",
    "title": "19  DT Frequency Response",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>DT Frequency Response</span>"
    ]
  },
  {
    "objectID": "20-ct-filters.html",
    "href": "20-ct-filters.html",
    "title": "20  Frequency Selective Filters in CT",
    "section": "",
    "text": "20.1 Ideal Filters\nRecall the response of stable CT LTI systems to periodic inputs. Given a stable LTI system with frequency response \\(H(j\\omega)\\) \\[x(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k e^{jk\\omega_0 t} \\longrightarrow y(t) = \\sum\\limits_{k = -\\infty}^{\\infty} a_k H(jk\\omega_0) e^{jk\\omega_0 t}\\]\nNote the output is equivalent to a signal with Fourier series coefficients \\(b_k = a_k H(jk\\omega_0)\\). That is the Fourier coefficients are scaled by the frequency response at the harmonic frequency \\(k\\omega_0\\).\nSimilarly for aperiodic signals, given a stable LTI system with frequency response \\(H(j\\omega)\\) \\[x(t) = \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(j\\omega) e^{j\\omega t}\\; d\\omega \\longrightarrow y(t) = \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} X(j\\omega) H(j\\omega) e^{j\\omega t}\\; d\\omega\\]\nNote the output is equivalent to a signal with Fourier Transform \\(Y(j\\omega) = X(j\\omega) H(jk\\omega_0)\\). That is the Fourier transform at each continuous frequency \\(\\omega\\) is scaled by the frequency response at that frequency.\nWe can use this behavior to our advantage. In many applications we want to modify the values of \\(a_k\\) or \\(X(j\\omega)\\) selectively, passing them unmodified, increasing (amplifying) them, or decreasing (attenuating) them. This is accomplished by designing a frequency response. Such systems are called frequency selective filters and come in 4 basic types:\nWhile the design of such filters is outside the scope of this course, you are now equipped to understand and apply them based on your knowledge of the Fourier methods covered over the past several weeks.\nThe above filter types each have an ideal (although unrealizable) form.\nLow-pass filters remove frequency content above a threshold, \\(\\omega_c\\), called the cutoff frequency. They have an ideal frequency response \\[H(j\\omega) = \\left\\{ \\begin{array}{lc}\n  1 & -\\omega_c &lt; \\omega &lt; \\omega_c\\\\\n  0 & \\text{else}\n\\end{array}\n\\right.\\] with magnitude and phase plot\nTODO lowpass-ideal.pdf\nThe range of frequencies below \\(|\\omega_c|\\) are called the pass-band. The range of frequencies above \\(|\\omega_c|\\) are called the stop-band.\nHigh-pass filters remove frequency content below the cutoff frequency \\(\\omega_c\\). They have an ideal frequency response \\[H(j\\omega) = \\left\\{ \\begin{array}{lc}\n  0 & -\\omega_c &lt; \\omega &lt; \\omega_c\\\\\n  1 & \\text{else}\n\\end{array}\n\\right.\\] with magnitude and phase plot\nTODO highpass-ideal.pdf\nThe range of frequencies above \\(|\\omega_c|\\) are called the pass-band. The range of frequencies below \\(|\\omega_c|\\) are called the stop-band.\nBandpass filters remove frequency content outside a band of frequencies called the pass-band. They have an ideal frequency response \\[H(j\\omega) = \\left\\{ \\begin{array}{lc}\n  1 & -\\omega_0 - \\frac{B}{2} &lt; \\omega &lt; -\\omega_0+\\frac{B}{2}\\\\[1em]\n  1 & \\omega_0 -\\frac{B}{2} &lt; \\omega &lt; \\omega_0+\\frac{B}{2}\\\\[1em]\n  0 & \\text{else}\n\\end{array}\n\\right.\\] where \\(\\omega_0\\) is the center frequency and \\(B\\) is the bandwidth. The frequencies outside this range are in the stop-band. The magnitude and phase plot looks like\nTODO bandpass-ideal.pdf\nFinally, notch or bandstop filters remove frequency content inside a band of frequencies (the stop band) defined by the center frequency \\(\\omega_0\\) and bandwidth \\(B\\). The ideal frequency response is \\[H(j\\omega) = \\left\\{ \\begin{array}{lc}\n  0 & -\\omega_0 - \\frac{B}{2} &lt; \\omega &lt; -\\omega_0+\\frac{B}{2}\\\\[1em]\n  0 & \\omega_0 -\\frac{B}{2} &lt; \\omega &lt; \\omega_0+\\frac{B}{2}\\\\[1em]\n  1 & \\text{else}\n\\end{array}\n\\right.\\] with magnitude and phase plot\nTODO bandstop-ideal.pdf\nOften the bandstop filter has a very narrow bandwidth, thus it \"notches\" out a frequency component of the input signal. A related filter with many notches at multiple frequencies (often harmonics) is called a comb filter.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Frequency Selective Filters in CT</span>"
    ]
  },
  {
    "objectID": "20-ct-filters.html#practical-filters",
    "href": "20-ct-filters.html#practical-filters",
    "title": "20  Frequency Selective Filters in CT",
    "section": "20.2 Practical Filters",
    "text": "20.2 Practical Filters\nIdeal CT filters cannot be implemented in practice because they are non-causal and thus physically impossible. To see why consider the impulse response of the ideal low-pass filter:\n\\[h(t) = \\mathcal{F}^{-1} \\left\\{ H(j\\omega) \\right\\} = \\frac{1}{2\\pi} \\int\\limits_{\\-\\omega_c}^{\\omega_c} e^{j\\omega t} \\; d\\omega = \\frac{1}{\\pi t}\\sin(\\omega_c t)\\] which has nonzero values for \\(t &lt; 0\\), and thus corresponds to a non-casual system. Ideal filters also have zero phase which cannot be achieved in practice.\nPractical filters are described by a frequency response that is a ratio of two polynomials in \\(j\\omega\\), i.e. \\[H(j\\omega) = \\frac{K \\cdot(j\\omega - \\beta_1)\\cdot(j\\omega - \\beta_2)\\cdots (j\\omega - \\beta_M)}{(j\\omega - \\alpha_1)\\cdot(j\\omega - \\alpha_2)\\cdots (j\\omega - \\alpha_N)}\\] where \\(K\\) is a constant that controls the gain at DC, and the zero or more complex coefficients \\(\\beta_k\\) and the one or more complex coefficients \\(\\alpha_k\\) are called the zeros and poles of the filter respectively. Such systems correspond to differential equations as we have covered before and are physically realizable as circuits if all poles and zeros are real or come in conjugate pairs. The processes of designing filters consists of choosing the poles and zeros, or equivalently choosing the coefficients of the numerator and dominator polynomials. This is covered in ECE 3704, ECE 4624, and other upper-level courses.\nPractical filters differ from ideal filters in that they cannot be zero over any finite range of frequencies and cannot transition discontinuously between stop and pass bands. Instead they must vary over the bands and transition smoothly, with a degree of variation and sharpness that is a function of the order of the filter and the exact form of the frequency response polynomials. Thus practical filters are described by additional parameters that define the stop and pass-bands.\nThe overall gain of the filter is the magnitude of the frequency response at a frequency that depends on the filter type, zero for a low-pass filter and the center frequency for a band-pass filter. The pass-band is defined by the frequency at which the magnitude of the frequency response drops below the overall gain, often -3dB = \\(\\frac{\\sqrt{2}}{2}\\). The stop-band is defined similarly, as the frequency at which the magnitude of the frequency response drops further below the overall gain, often -20dB = 0.1 or -40dB = 0.01. The transition bandwidth is defined as the difference in the stop-band and pass-band frequencies. The pass-band ripple is defined as the maximum deviation from the overall gain, over the pass-band.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Frequency Selective Filters in CT</span>"
    ]
  },
  {
    "objectID": "20-ct-filters.html#first-order-and-second-order-systems-as-filters",
    "href": "20-ct-filters.html#first-order-and-second-order-systems-as-filters",
    "title": "20  Frequency Selective Filters in CT",
    "section": "20.3 First-order and second-order systems as filters",
    "text": "20.3 First-order and second-order systems as filters\nGiven the equivalence of LTI systems and linear, constant-coefficient differential equations, block diagrams, impulse responses, and frequency responses, filters can be represented in any of these ways.\nWe have covered extensively first-order and second-order CT systems and seen how they can be represented variously as circuits, differential equations, block diagrams, and as frequency responses. We now see how they can describe simple filters and serve as building blocks for higher-order filters.\n\nConsider a low-pass filter with the desired characteristics of having a pass-band of -3dB at 1kHz, and a stop-band of -20dB at 10kHz. Suppose this is implemented as a first-order \"Butterworth\" filter, which can be realized by an RC circuit.\n\n\n(5,2.5) node[op amp, yscale=-1] (opamp1) (0,3) to[R,l=\\(R\\),o-] (opamp1.+) (3,0) to[C, l=\\(C\\)] (3,3) (0,0) to[short,o-o] (7,0) (0,3) to[open, v=\\(x(t)\\)] (0,0) (7.5,2.5) to[open, v=\\(y(t)\\)] (7.5,0) (opamp1.-) |- (5,1) -| (opamp1.out) (opamp1.out) to[short,-o] (7,2.5);\n\n\nwhere \\(R=99.2k\\Omega\\) and \\(C=1.6\\)nF. This is equivalent to the differential equation \\[\\frac{dy}{dt}(t) + a y(t) = a x(t)\\] where \\(a=\\frac{1}{RC}\\), or the block diagram\n\n= [draw, fill=gray!20, rectangle, minimum height=2em, minimum width=2em] = [draw, fill=gray!20, circle, node distance=1cm] = [coordinate] = [coordinate] = [pin edge=to-,thin,black]\n\nThis system has the impulse response \\[h(t) = ae^{-at}u(t)\\] and the frequency response \\[H(j\\omega) = \\frac{a}{j\\omega + a}\\] If we plot the frequency response as a Bode plot, we see the DC gain is 0dB, and the response passes through -3dB and -20dB at the expected frequencies \\(2\\pi*1000 \\approx 6.3\\times 10^3\\) rad/s and \\(2\\pi*10000 \\approx 6.3\\times 10^4\\) rad/s. Thus the transition bandwidth is 9kHz.\n\n\n\n\nimage\n\n\n\n\n\nSuppose we wish to sharpen the transition band for the previous example so that has a pass-band of -3dB at 1kHz, and a narrower stop-band of -20dB at 5kHz. This requires a second-order filter, and can be realized by a circuit called the Sallen-Key.\n\n\n(7,3.5) node[op amp] (opamp1) (0,4) to[R,l=\\(R_1\\),o-] (2,4) (2,4) to[short] (2,5) (2,5) to[C,l=\\(C_1\\)] (8.2,5) (8.2,5) to[short] (opamp1.out) (2,4) to[R, l=\\(R_2\\)] (4,4) (4,4) to[C, l=\\(C_2\\)] (4,0) (0,0) to[short,o-o] (12,0) (4,4) to[short] (opamp1.-) (opamp1.+) to[short] (5.8,1.75) (5.8,1.75) to[short] (8.2,1.75) (opamp1.out) to[short] (8.2,1.75) (opamp1.out) to[short, -o] (12,3.5) (0,4) to[open, v=\\(x(t)\\)] (0,0) (12,3.5) to[open, v=\\(y(t)\\)] (12,0);\n\n\nwhere \\(R_1=74.2k\\Omega\\), \\(R_2=91.3M\\Omega\\), \\(C_1=1.6\\)nF and \\(C_2=160\\)pF. This is equivalent to the differential equation \\[\\frac{d^2y}{dt^2}(t) + 2\\alpha \\frac{dy}{dt}(t) + \\omega_0^2 y(t) = \\omega_0^2 x(t)\\] where \\[\\alpha = \\frac{R_1+R_2}{2R_1 R_2 C_1} \\;\\text{ and }\\; \\omega_0^2 = \\frac{1}{R_1 R_2 C_1 C_2}\\] or the block diagram\n\n= [draw, fill=gray!20, rectangle, minimum height=2em, minimum width=2em] = [draw, fill=gray!20, circle, node distance=1cm] = [coordinate] = [coordinate] = [pin edge=to-,thin,black]\n\nThis system has the frequency response \\[H(j\\omega) = \\frac{\\omega_0^2}{\\omega_0^2-\\omega^2 + j2\\alpha\\omega}\\]\nIf we plot the frequency response as a Bode plot using the resistor and capacitor values above, we see the DC gain is 0dB, and the response passes through -3dB at the expected frequency \\(2\\pi*1000 \\approx 6.3\\times 10^3\\) rad/s. At the frequency \\(2\\pi*5000 \\approx 3.14\\times 10^4\\) rad/s the response passes through about -28dB. Thus this circuit has a transition bandwidth even narrower than that designed (it is slightly better).\n\n\n\n\nimage\n\n\n\nNote the price we pay for this decreased transition bandwidth is a larger phase shift (and two more components).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Frequency Selective Filters in CT</span>"
    ]
  },
  {
    "objectID": "20-ct-filters.html#higher-order-filters",
    "href": "20-ct-filters.html#higher-order-filters",
    "title": "20  Frequency Selective Filters in CT",
    "section": "20.4 Higher-Order Filters",
    "text": "20.4 Higher-Order Filters\nWe can continue to increase the steepness of the passband to stop-band transitions by increasing the order of the filter. This is typically accomplished using a serial connection of systems, called stages in filter parlance, where each stage is a first-order or second-order system.\nRecall in a series connection of systems the overall impulse response is the convolution of the individual responses. If we assume each stage is stable then, by the convolution property, the overall frequency response is given by the product of their individual frequency responses.\n\n\n\n\\[H(j\\omega) = \\frac{Y(j\\omega)}{X(j\\omega)} = H_1(j\\omega)\\cdot H_2(j\\omega)\\]\nWriting each response in polar form \\[H_1(j\\omega)\\cdot H_2(j\\omega) = |H_1(j\\omega)|\\cdot |H_2(j\\omega)| e^{\\angle H_1(j\\omega)+ \\angle H_2(j\\omega)}\\] we note that the magnitudes multiply and the phases add. That means we can use additional stages to reinforce the attenuation of previous stages. Note this requires in the circuit that the stages be impedance isolated, thus the use of the opamps at the end of CT filters. Again the price we pay for increasing the order of the filter and decreasing the transition frequency is increased phase shift in the signal.\nMatlab code for plotting the first-order example Bode plot:\nR = 99.2e3;\nC = 1.6e-9;\na = 1/(R*C);\n\nH = tf([a],[1,a]);\n[mag,ph,w] = bode(H);\n\n% Create a nice bode plot \nhFig = figure();\nhold on;\n\nsubplot(2,1,1);\nhm = semilogx(w,20*log10(squeeze(mag)));\ngrid on;\nhTitle  = title ('Frequency Response - first order');\nhYLabel1 = ylabel('Magnitude (dB)');\nset(gca, 'FontSize', 14, 'YTick', -60:10:20, ...\n    'Box', 'off', 'LineWidth', 2);\n\nsubplot(2,1,2);\nhp = semilogx(w,squeeze(ph*(pi/180)));\ngrid on;\nhYLabel2 = ylabel('Phase (radians)');\nhXLabel = xlabel('Frequency (rad/s)');\nset(gca, 'FontSize', 14, 'Box', 'off', 'LineWidth', 2);\n\nset(hm, 'linewidth', 2);\nset(hp, 'linewidth', 2);\nset([hXLabel, hYLabel1, hYLabel2]  , ...\n     'FontSize'   , 14          );\nset( hTitle                    , ...\n     'FontSize'   , 14          , ...\n     'FontWeight' , 'bold'      );\nMatlab code for plotting the second-order example Bode plot:\nR1 = 74.2e3;\nR2 = 1.33e6;\nC1 = 1.6e-9;\nC2 = 160e-12;\n\na = (R1+R2)/(R1*R2*C1);\nb = 1/(R1*R2*C1*C2);\n\nH = tf([b],[1,a,b]);\n[mag,ph,w] = bode(H);\n\n% Create a nice bode plot \nhFig = figure();\nhold on;\n\nsubplot(2,1,1);\nhm = semilogx(w,20*log10(squeeze(mag)));\ngrid on;\nhTitle  = title ('Frequency Response - second order');\nhYLabel1 = ylabel('Magnitude (dB)');\nset(gca, 'FontSize', 14, 'YTick', -90:10:20, ...\n    'Box', 'off', 'LineWidth', 2);\n\nsubplot(2,1,2);\nhp = semilogx(w,squeeze(ph*(pi/180)));\ngrid on;\nhYLabel2 = ylabel('Phase (radians)');\nhXLabel = xlabel('Frequency (rad/s)');\nset(gca, 'FontSize', 14, 'Box', 'off', 'LineWidth', 2);\n\nset(hm, 'linewidth', 2);\nset(hp, 'linewidth', 2);\n\nset([hXLabel, hYLabel1, hYLabel2]  , ...\n     'FontSize'   , 14          );\nset( hTitle                    , ...\n     'FontSize'   , 14          , ...\n     'FontWeight' , 'bold'      );",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Frequency Selective Filters in CT</span>"
    ]
  },
  {
    "objectID": "21-dt-filters.html",
    "href": "21-dt-filters.html",
    "title": "21  Frequency Selective Filters in DT",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Frequency Selective Filters in DT</span>"
    ]
  },
  {
    "objectID": "22-dft.html",
    "href": "22-dft.html",
    "title": "22  Discrete Fourier Transform",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete Fourier Transform</span>"
    ]
  },
  {
    "objectID": "23-sampling.html",
    "href": "23-sampling.html",
    "title": "23  Sampling CT Signals",
    "section": "",
    "text": "23.1 Sampling Theory\nUp until now in the course we have focused on either CT or DT signals and systems. Practical systems though often are hybrid and require conversion between DT and CT signals. For example a CT audio signal might be converted to a DT audio signal for storage and/or transmission, and at a later time or location converted back to a CT signal for playback through a speaker.\nIt is also common to design a CT system and then implement it as a DT system. Advantages of this approach are e.g. such implementations are less susceptible to component variations, require no tuning a build time, are easier to change (firmware or software update), easier to prototype, and more easily use encryption.\nIn this lecture we focus on sampling of CT signals to produce a DT signal \\(x[n] = x(nT)\\) with sample index \\(n\\) and sample time \\(T\\). In the next lecture we consider the case of converting from a DT to CT signal.\nThe process of sampling is to produce a DT signal \\(x[n]\\) from a CT signal \\(x(t)\\) by sampling time at regular intervals \\(T\\in \\mathbb{R}^+\\) called the sample-time, or equivalently sampling at a frequency of \\(\\tfrac{1}{T}\\) Hz or \\(\\tfrac{2\\pi}{T}\\) rad/s. Mathematically this is simple to express in the time domain as \\(x[n] = x(nT)\\), however we seek a system that can perform this task.\nRecall the impulse train is the periodic signal \\[x_1(t) = \\sum\\limits_{n=-\\infty}^{\\infty} \\delta(t-nT_0)\\] with period \\(T_0\\) and frequency \\(\\omega_0 = \\tfrac{2\\pi}{T_0}\\). The exponential CT Fourier series of the impulse train is given by \\[x_1(t) = \\sum\\limits_{n=-\\infty}^{\\infty} a_n e^{j\\tfrac{2\\pi}{T_0}nt}\\] where the Fourier series coefficients are \\[a_n = \\frac{1}{T_0} \\int\\limits_{-\\frac{T_0}{2}}^{\\frac{T_0}{2}} \\delta(t) e^{-jn\\omega_0 t} \\; dt = \\frac{1}{T_0}\\] Now, lets take the Fourier Transform of the Fourier series representation \\[\\begin{aligned}\nX_1(j\\omega) &= \\int\\limits_{-\\infty}^{\\infty} \\sum\\limits_{n=-\\infty}^{\\infty} \\frac{1}{T_0} e^{j\\tfrac{2\\pi}{T_0}nt}e^{j\\omega t}\\; dt\\\\\n&= \\frac{1}{T_0} \\sum\\limits_{n=-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} e^{j\\tfrac{2\\pi}{T_0}nt}e^{j\\omega t}\\; dt\\\\\n&= \\frac{2\\pi}{T_0} \\sum\\limits_{n=-\\infty}^{\\infty} \\delta(\\omega - \\omega_0 n)\n\\end{aligned}\\] also an impulse train in the frequency domain. Now suppose we have another signal \\(x_2(t)\\) and we multiply \\(x_1(t)\\) and \\(x_2(t)\\) to get a signal \\(y(t)\\). \\[y(t) = x_1(t) \\cdot x_2(t) = \\sum\\limits_{n=-\\infty}^{\\infty} x_2(t) \\delta(t-nT_0) = \\sum\\limits_{n=-\\infty}^{\\infty} x_2(nT_0) \\delta(t-nT_0)\\] Since \\(y(t)\\) is non-zero only at the locations of the delta functions, we can treat \\(y(nT_0) = x_2(nT_0)\\) as the DT signal \\(x_2[n]\\). This is illustrated below\nTODO samplinf_timedomain.pdf\nEquivalently in the frequency domain the modulation theorem gives \\[y(t) = x_1(t) \\cdot x_2(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{2\\pi} X_1(j\\omega) * X_2(j\\omega) = Y(j\\omega)\\] Lets do the convolution \\[\\begin{aligned}\n  Y(j\\omega) &= \\frac{1}{2\\pi} X_1(j\\omega) * X_2(j\\omega)\\\\\n  &=  \\frac{1}{2\\pi} \\left[  \\frac{2\\pi}{T_0} \\sum\\limits_{n=-\\infty}^{\\infty} \\delta(\\omega - \\omega_0 n) \\right] * X_2(j\\omega)\\\\\n  &= \\frac{1}{2\\pi} \\int\\limits_{-\\infty}^{\\infty}  \\frac{2\\pi}{T_0} \\sum\\limits_{n=-\\infty}^{\\infty} \\delta(\\omega - \\omega^\\prime - \\omega_0 n) X_2(j\\omega^\\prime) \\; d\\omega^\\prime\\\\\n  &= \\frac{1}{T_0} \\sum\\limits_{n=-\\infty}^{\\infty} X_2(j(\\omega - n\\omega_0))\n\\end{aligned}\\] Thus the sampling process in the frequency domain causes periodic replication of the Fourier transform of the signal being sampled, \\(x_2(t)\\), which are sometimes called images. This signal \\(Y(j\\omega)\\) is periodic in \\(\\omega_0 = \\tfrac{2\\pi}{T_0}\\) radians per second and corresponds to the DT Fourier Transform of \\(x_2[n]  \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_2\\left(e^{j\\omega}\\right)\\), which is periodic in \\(2\\pi\\) radians per sample time.\nTo help us visualize this, suppose that the signal \\(x_2(t)  \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X_2(j\\omega)\\) is band-limited to \\(B\\) Hz, that is \\(X_2(j\\omega) = 0\\) for all frequencies outside the band \\(-2\\pi B &lt; \\omega &lt; 2\\pi B\\). This is shown schematically as the magnitude spectrum below:\nTODO bandlimited.pdf\nAfter sampling (\\(y(t) = x_1(t)\\cdot x_2(t)\\)) and assuming \\(\\omega_0 &gt; 4\\pi B\\) the spectrum of the sampled signal is:\nTODO bandlimitedsampled1.pdf\nIf instead \\(\\omega_0 &lt; 4\\pi B\\) the images overlap and we get aliasing, where high frequency content gets added to the lower frequency content. This is shown below with the lighter lines showing the images and the heavier line showing their sum.\nTODO bandlimitedsampled2.pdf\nAs we will see next time, to reconstruct the signal \\(x_2[n]\\) back to \\(x_2(t)\\) we need to ensure that \\(\\omega_0 &gt; 4\\pi B\\) rad/s or equivalently \\(f_0 &gt; 2 B\\) Hz, which requires the sample time \\(T_0 &lt; \\tfrac{1}{2B}\\) seconds. This is called the Nyquist sample rate/frequency.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sampling CT Signals</span>"
    ]
  },
  {
    "objectID": "23-sampling.html#sampling-theory",
    "href": "23-sampling.html#sampling-theory",
    "title": "23  Sampling CT Signals",
    "section": "",
    "text": "Consider a signal representing a musical chord (an additive mixture of three notes) \\[x(t) = \\sin(2\\pi\\cdot (261) t) + \\sin(2\\pi\\cdot (329) t) + \\sin(2\\pi\\cdot (392) t)\\] Suppose it is sampled at a frequency of \\(f_0 = 1\\) kHz. Then there is no aliasing into the frequency range \\((0, 500)\\) Hz. After reconstruction \\(x(t)\\) would be unmodified. Suppose instead it is sampled at \\(f_0 = 500\\) Hz. Then the signal component at \\(261\\) Hz aliases to \\(239 = 500-261\\) Hz, the signal component at \\(329\\) Hz aliases to \\(171 = 500-329\\) Hz, and the signal component at \\(392\\) Hz aliases to \\(108 = 500-392\\) Hz. When reconstructed, the signal now has an additional 3 tones mixed in at audible frequencies, but do not correspond to (Western) musical notes, i.e. \\[x(t) = \\sin(2\\pi\\cdot (108) t) + \\sin(2\\pi\\cdot (171) t) + \\sin(2\\pi\\cdot (239) t) + \\sin(2\\pi\\cdot (261) t) + \\sin(2\\pi\\cdot (329) t) + \\sin(2\\pi\\cdot (392) t)\\]",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sampling CT Signals</span>"
    ]
  },
  {
    "objectID": "23-sampling.html#practical-sampling",
    "href": "23-sampling.html#practical-sampling",
    "title": "23  Sampling CT Signals",
    "section": "23.2 Practical Sampling",
    "text": "23.2 Practical Sampling\nSampling in practice requires addressing three issues. First, we cannot generate the impulse train, but can only approximate it. Second, digital signals must have a fixed bit width so we have to convert the real signal value to a quantized one. Lastly, since in general we have no control over the input signal means we need to ensure the signal is approximately band-limited before sampling.\n\n23.2.1 Sample and Hold\nSampling is typically accomplished using a circuit called a sample-and-hold, schematically illustrated below.\nTODO smaple_hold.pdf\nThe CT signal is applied to the input of the first op-amp buffer. The output of this first buffer is switched into a charging capacitor for the sample time, then disconnected (high impedance) at regular intervals for the hold time, typically using a MOSFET switch. The effect is the capacitor is charged to the current value of \\(x(t)\\) during the sample-time, which it maintains during the hold-time, the value of which is bufered by the second op-amp. This can be mathematically modeled as a pulse train with a width equal to the sample time rather than as an impulse train.\n\n\n23.2.2 Quantization\nTo quantize the signal after the sample-and-hold into \\(N\\) bits, several strategies can be used. One popular approach is called successive approximation, illustrated below\nTODO sar.pdf\nThe current quantized digital value is held in a counter connected to a clock signal. The direction of the counter (up or down) is controlled by a comparator connected to the output of the sample and hold and the current counter output and a digital-to-analog converter (DAC, usually a resistor ladder) that converts it back to an analog value. If the DAC value is less than the held value, the counter counts up, if the DAC value is greater than the held value the counter counts down. In this fashion the counter output tracks the held value after a settling time required for convergence, at which point the counter value is clocked into a register for storage.\n\n\n23.2.3 Anti-aliasing\nBefore the sample and hold we need to include a filter to limit the bandwidth. This can be accomplished by a CT low-pass filter called an anti-aliasing filter whose cutoff frequency in the ideal case is \\(\\omega_c = 2\\pi B\\). As we saw in lecture 24 ideal filters cannot be implemented, thus we specify the anti-aliasing filter as a pass-band gain/frequency and a stop-band gain/frequency. Since the transition band is non-zero for a practical filter, this means we have to either lower the pass-band relative to the ideal or increase the sample rate. In the best case, the filter should have a stop-band frequency at half the sampling frequency with the order of the filter and pass-band frequency adjusted as needed. Alternatively the gain that defines the stop-band can be relaxed. This gives a desired frequency response magnitude that looks like the following.\nTODO antialias.pdf\nThe bold dotted line shows the maximum frequency response of the first image.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sampling CT Signals</span>"
    ]
  },
  {
    "objectID": "24-recon.html",
    "href": "24-recon.html",
    "title": "24  Reconstructing CT Signals",
    "section": "",
    "text": "24.1 Reconstruction Theory\nIn the previous lecture we focused on sampling of CT signals to produce a DT signal \\(x[n] = x(nT)\\) with sample index \\(n\\) and sample time \\(T\\). In this lecture we consider reconstruction, converting from a DT signal \\(x[n]\\) to a CT signal \\(x(t)\\) using a sample time \\(T\\) as the spacing between samples. Ideally a conversion from \\(x(t)\\) to \\(x[n]\\) and back again would result in identical signal.\nGiven a DT signal \\(x[n]\\) and a sample spacing \\(T\\), we can define a corresponding CT signal as \\[x_p(t) = \\sum\\limits_{n = -\\infty}^{\\infty} x[n] \\, \\delta(t-nT)\\] the impulse train with each impulse weighted by the DT signal.\nCT signal reconstruction can be viewed from two different (but equivalent) perspectives. In the time domain perspective, the CT signal \\(x(t)\\) corresponding to a DT signal \\(x[n]\\) can be viewed as interpolation, where the values of the CT signal are equal to the DT signal at intervals of the sample time, i.e. \\(x(nT) = x[n]\\), and in between the value of \\(x(t)\\) is interpolated. If the interpolation is of zero-order, the value at \\(x(nT)\\) is held constant until \\(x(nT+T)\\). This is called a zero-order hold, and can mathematically modeled as convolution of the weighted impulse train with a pulse \\(p(t) = u(t) - u(t-T)\\) whose width is the sample time, called the interpolation function. \\[y(t) = p(t)*x_p(t)\\] This is illustrated below\nTODO zero-order-hold-interp\nThe zero-order hold is not a very accurate representation of a band-limited signal. So, what interpolation function is optimal?\nTo answer this question we can turn to the alternative perspective on reconstruction, that of the frequency domain. Recall the sampled signal \\(x(nT)\\) in the frequency domain can be viewed as the summation of the Fourier transform of \\(x(t)\\), \\(X(j\\omega)\\), and periodic replicas or images centered at multiples of the sampling frequency. If we assume the original signal was band-limited and sampled appropriately (using the Nyquist criteria), then if we ideal low-pass filter the sampled signal we will preserve the central portion of the Fourier spectrum that corresponds to the original signal, and chop off the images. For this reason the reconstruction filter is also called an anti-imaging filter.\nTODO bandlimitedreconstructed1.pdf\nRecall filtering is multiplication the frequency domain and convolution in the time domain, so the optimal interpolation function corresponds to the impulse response of the ideal low-pass filter with cutoff frequency \\(\\omega_c = 2\\pi B\\), a sinc function.\n\\[h(t) = \\mathcal{F}^{-1} \\left\\{ H(j\\omega) \\right\\} = \\frac{1}{2\\pi} \\int\\limits_{-\\omega_c}^{\\omega_c} e^{j\\omega t} \\; d\\omega = \\frac{1}{\\pi t}\\sin(\\omega_c t)\\]\nThus the ideal ideal interpolation function is the sinc function, and reconstruction is low-pass filtering of the weighted impulse train \\(x_p(t)\\) 1.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reconstructing CT Signals</span>"
    ]
  },
  {
    "objectID": "24-recon.html#practical-reconstruction",
    "href": "24-recon.html#practical-reconstruction",
    "title": "24  Reconstructing CT Signals",
    "section": "24.2 Practical Reconstruction",
    "text": "24.2 Practical Reconstruction\nAs we have seen before we cannot physically represent the impulse train nor the ideal low-pass filter. Thus practical reconstruction uses an approximation of the ideal reconstruction filter by a digital-to-analog converter (DAC), followed by a causal (and thus physically possible) low-pass filter.\n\n24.2.1 Zero-order hold using an R-2R ladder\nA zero-order hold DAC can be implemented by a circuit called a resistor ladder. Consider a digital output with \\(N\\) bits and a reference voltage \\(V_{ref}\\) (for example an 8-bit output port on a micro-controller using CMOS 3.3v logic).\nIf this port is connected to a resistor network consisting of resistor values \\(R\\) and \\(2R\\) as follows\nTODO resistor-ladder.pdf\nthen depending on the bit pattern at the output port \\(V\\), the output of the buffer op-amp will be \\[V_o = V_{ref}\\frac{V}{2^N}\\]\nIf the port value is changed every sample time \\(T\\), then the resister ladder and buffer op-amp combine to implement a zero-order hold circuit.\n\n\n24.2.2 Reconstruction(anti-imaging) filter\nThe zero-order hold is followed by the reconstruction (anti-imaging) filter which low-pass filters the output and smooths-out the jumps from value to value.\nTODO sinc-interp.pdf\nIn general the reconstruction filter is of a similar, or identical form to the anti-aliasing filter.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reconstructing CT Signals</span>"
    ]
  },
  {
    "objectID": "24-recon.html#footnotes",
    "href": "24-recon.html#footnotes",
    "title": "24  Reconstructing CT Signals",
    "section": "",
    "text": "As an aside this also gives an intuitive view of convolution with an impulse train, as interpolation↩︎",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reconstructing CT Signals</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html",
    "href": "26-formulas-tables.html",
    "title": "Appendix A — Useful Mathematical Definitions and Tables",
    "section": "",
    "text": "A.1 Definition of modulus for integers\nLet \\(n\\in\\mathbb{Z}\\) and \\(N \\in \\mathbb{N}\\). The mod operator \\[n \\% N = \\left\\{ \\begin{array}{cc}\n  \\text{remainder}\\left(\\frac{n}{N}\\right) & n \\geq 0\\\\\n  N - \\text{remainder}\\left(\\frac{|n|}{N}\\right) & n &lt; 0\n  \\end{array}\n\\right.\\] where \\(\\text{remainder}\\) is the remainder after dividing \\(n\\) by \\(N\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Useful Mathematical Definitions and Tables</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:ctconv",
    "href": "26-formulas-tables.html#table:ctconv",
    "title": "Appendix A — Useful Mathematical Definitions and Tables",
    "section": "A.2 Table of Representative Convolution Integrals",
    "text": "A.2 Table of Representative Convolution Integrals\n\nCT Convolution Table\n\n\n\\(x_1(t)\\)\n\\(x_2(t)\\)\n\\(x_1(t) * x_2(t)\\)\n\n\n\n\n\\(u(t)\\)\n\\(e^{a t}u(t)\\)\n\\(\\frac{1-e^{a t}}{-a}u(t)\\)\n\n\n\\(u(t)\\)\n\\(u(t)\\)\n\\(tu(t)\\)\n\n\n\\(e^{a_1 t}u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{e^{a_1 t}-e^{a_2 t}}{a_1 - a_2}u(t)\\) for \\(a_1 \\neq a_2\\)\n\n\n\\(e^{a t}u(t)\\)\n\\(e^{a t}u(t)\\)\n\\(te^{a t}u(t)\\)\n\n\n\\(te^{a_1 t}u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{e^{a_2 t}-e^{a_1 t} + (a_1-a_2)te^{a_1 t}}{(a_1 - a_2)^2}u(t)\\) for \\(a_1 \\neq a_2\\)\n\n\n\\(e^{a_1 t}\\cos(\\beta t + \\theta)u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{\\cos(\\theta - \\phi)e^{a_2 t} - e^{a_1 t}\\cos(\\beta t + \\theta - \\phi)}{\\sqrt{(a_1 + a_2)^2 + \\beta^2}}u(t)\\)\n\n\n\n\n\\(\\phi = \\arctan\\left( \\frac{-\\beta}{a_1 + a_2}\\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Useful Mathematical Definitions and Tables</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:dtconv",
    "href": "26-formulas-tables.html#table:dtconv",
    "title": "Appendix A — Useful Mathematical Definitions and Tables",
    "section": "A.3 Table of Representative Convolution Sums",
    "text": "A.3 Table of Representative Convolution Sums\n\nDT Convolution Table\n\n\n\\(x_1[n]\\)\n\\(x_2[n]\\)\n\\(x_1[n] * x_2[n]\\)\n\n\n\n\n\\(u[n]\\)\n\\(u[n]\\)\n\\((n+1)u[n]\\)\n\n\n\\(\\gamma^{n}u[n]\\)\n\\(u[n]\\)\n\\(\\frac{1-\\gamma^{n+1}}{1-\\gamma}u[n]\\)\n\n\n\\(\\gamma_1^{n}u[n]\\)\n\\(\\gamma_2^{n}u[n]\\)\n\\(\\frac{\\gamma_1^{n+1}-\\gamma_2^{n+1}}{\\gamma_1-\\gamma_2}u[n]\\) for \\(\\gamma_1 \\neq \\gamma_2\\)\n\n\n\\(\\gamma^{n}u[n]\\)\n\\(\\gamma^{n}u[n]\\)\n\\((n+1)\\gamma^{n}u[n]\\)\n\n\n\\(|\\gamma_1|^{n}\\cos\\left(\\beta n + \\theta \\right)u[n]\\)\n\\(|\\gamma_2|^{n}u[n]\\)\n\\(\\frac{1}{R}\\left[ |\\gamma_1|^{n+1}\\cos\\left( \\beta (n+1) + \\theta - \\phi\\right) - |\\gamma_2|^{n+1}\\cos\\left( \\theta - \\phi\\right)\\right]u[n]\\)\n\n\n\n\n\\(R = \\left[ |\\gamma_1|^2 + |\\gamma_2|^2 -2|\\gamma_1||\\gamma_2|\\cos(\\beta)\\right]^{\\frac{1}{2}}\\)\n\n\n\n\n\\(\\phi = \\arctan\\left( \\frac{|\\gamma_1|\\sin(\\beta)}{|\\gamma_1|\\cos(\\beta) - |\\gamma_2|} \\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Useful Mathematical Definitions and Tables</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:ctft",
    "href": "26-formulas-tables.html#table:ctft",
    "title": "Appendix A — Useful Mathematical Definitions and Tables",
    "section": "A.4 Table of Representative CT Fourier Transform Pairs",
    "text": "A.4 Table of Representative CT Fourier Transform Pairs\n\n\n\nTable A.1: CT Fourier Transform Table\n\n\n\n\n\n\\(x(t)\\)\n\\(X(j\\omega)\\)\n\n\n\n\n\\(1\\)\n\\(2\\pi\\delta(\\omega)\\)\n\n\n\\(\\delta(t)\\)\n\\(1\\)\n\n\n\\(u(t)\\)\n\\(\\pi\\delta(\\omega) + \\frac{1}{j\\omega}\\)\n\n\n\\(e^{-at}u(t)\\)\n\\(\\frac{1}{a + j\\omega}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(te^{-at}u(t)\\)\n\\(\\frac{1}{\\left(a + j\\omega\\right)^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(e^{j\\omega_0 t}\\)\n\\(2\\pi\\delta(\\omega-\\omega_0)\\)\n\n\n\\(\\cos(\\omega_0 t)\\)\n\\(\\pi\\left[ \\delta(\\omega-\\omega_0) + \\delta(\\omega+\\omega_0)\\right]\\)\n\n\n\\(\\sin(\\omega_0 t)\\)\n\\(j\\pi\\left[ \\delta(\\omega+\\omega_0) - \\delta(\\omega-\\omega_0)\\right]\\)\n\n\n\\(e^{-at}\\cos(\\omega_0 t)u(t)\\)\n\\(\\frac{a+j\\omega}{(a+j\\omega)^2 + \\omega_0^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(e^{-at}\\sin(\\omega_0 t)u(t)\\)\n\\(\\frac{\\omega_0}{(a+j\\omega)^2 + \\omega_0^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(\\delta(t - t_0)\\)\n\\(e^{-j t_0 \\omega}\\)\n\n\n\\(K_0\\)\n\\(2 K_0 \\pi \\delta(\\omega)\\)\n\n\n\\(e^{-a|t|}\\), \\(\\text{Re}\\{a\\} &gt; 0\\)\n\\(\\frac{2a}{a^2 + \\omega^2}\\)\n\n\n\\(u(t + T) - u(t - T)\\)\n\\(2T \\frac{\\sin{(\\omega T)}}{\\omega T}\\)\n\n\n\\(\\frac{\\sin{({W}t)}}{W t}\\)\n\\(\\frac{\\pi}{W} [u(\\omega + W) - u(\\omega - W)]\\)\n\n\n\\(e^{-\\frac{t^2}{2 \\sigma^2}}\\)\n\\(\\sigma \\sqrt{2 \\pi} e^{-\\frac{\\sigma^2 \\omega^2}{2}}\\)\n\n\n\\(\\sum\\limits_{k=-\\infty}^{\\infty} a_k e^{j k \\omega_0 t}\\)\n\\(2 \\pi \\sum\\limits_{k=-\\infty}^{\\infty} a_k \\delta{(\\omega - k \\omega_0)}\\)\n\n\n\\(\\sum\\limits_{n=-\\infty}^{\\infty} \\delta(t - nT)\\)\n\\(\\omega_0 \\sum\\limits_{k=-\\infty}^{\\infty} \\delta{(\\omega - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{T}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Useful Mathematical Definitions and Tables</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:dtft",
    "href": "26-formulas-tables.html#table:dtft",
    "title": "Appendix A — Useful Mathematical Definitions and Tables",
    "section": "A.5 Table of Representative DT Fourier Transform Pairs",
    "text": "A.5 Table of Representative DT Fourier Transform Pairs\n\nDT Fourier Transform Table\n\n\n\\(x[n]\\)\n\\(X(e^{j\\omega})\\)\n\n\n\n\n\\(\\delta[n]\\)\n\\(1\\)\n\n\n\\(\\delta[n - n_0]\\)\n\\(e^{-j \\omega n_0}\\)\n\n\n\\(u[n]\\)\n\\(\\frac{1}{1 - e^{-j \\omega}} + \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta(\\omega - 2 k \\pi)\\)\n\n\n\\(K_0\\)\n\\(2 K_0 \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta(\\omega - 2 k \\pi)\\)\n\n\n\\(a^n u[n]\\), \\(|a| &lt; 1\\)\n\\(\\frac{1}{(1 - ae^{-j \\omega)}}\\)\n\n\n\\(n a^n u[n]\\), \\(|a| &lt; 1\\)\n\\(\\frac{a e^{-j \\omega}}{(1 - a e^{-j \\omega})^2}\\)\n\n\n\\(a^{|n|}\\), \\(|a| &lt; 1\\)\n\\(\\frac{1-a^2}{1 - 2 a \\cos{\\omega} + a^2}\\)\n\n\n\\(e^{j \\omega_0 n}\\)\n\\(2 \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta{(\\omega - \\omega_0 - 2 k \\pi)}\\)\n\n\n\\(\\cos{(\\omega_0 n)}\\)\n\\(\\pi \\sum\\limits_{k = - \\infty}^{\\infty} [\\delta(\\omega + \\omega_0 - 2 k \\pi) + \\delta(\\omega - \\omega_0 - 2 k \\pi)]\\)\n\n\n\\(\\sin{(\\omega_0 n)}\\)\n\\(j \\pi \\sum\\limits_{k = - \\infty}^{\\infty} [\\delta(\\omega + \\omega_0 - 2 k \\pi) - \\delta(\\omega - \\omega_0 - 2 k \\pi)]\\)\n\n\n\\(u[n + n_0] - u[n - n_0]\\)\n\\(\\frac{\\sin{(\\omega (n_0 + 0.5))}}{\\sin{0.5 \\omega}}\\)\n\n\n\\(\\frac{\\sin{({W}n)}}{\\pi n}\\)\n\\(\\sum\\limits_{k = - \\infty}^{\\infty} X_1(\\omega - 2 k \\pi), X_1(\\omega) = \\begin{array}{cc} 1 & 0 \\le |\\omega| \\le W, 0 &lt; W &lt; \\pi\\\\ 0 & W &lt; |\\omega| \\le \\pi,  0 &lt; W &lt; \\pi \\end{array}\\)\n\n\n\\(\\sum\\limits_{k=n_0}^{n_0+N-1} a_k e^{j k \\omega_0 n}\\)\n\\(2 \\pi \\sum\\limits_{k=-\\infty}^{\\infty} a_k \\delta{(\\omega - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{N}\\),\n\n\n\\(\\sum\\limits_{k=-\\infty}^{\\infty} \\delta[n - kN]\\)\n\\(\\omega_0 \\sum\\limits_{k=-\\infty}^{\\infty} \\delta{(n - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{N}\\),",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Useful Mathematical Definitions and Tables</span>"
    ]
  },
  {
    "objectID": "image-descriptions.html",
    "href": "image-descriptions.html",
    "title": "Appendix B — Detailed Image Descriptions",
    "section": "",
    "text": "B.1 Figure 14.1\nDescription of Figure 14.1\nThis figure contains two vertically arranged stem plots sharing the same discrete horizontal axis k from -10 to 10. Top plot — Amplitude Spectrum: Largest value at k=0 (≈0.333), with decreasing symmetric values for larger |k|. Bottom plot — Phase Spectrum: All phases are zero radians.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Detailed Image Descriptions</span>"
    ]
  },
  {
    "objectID": "image-descriptions.html#sec-14-2",
    "href": "image-descriptions.html#sec-14-2",
    "title": "Appendix B — Detailed Image Descriptions",
    "section": "B.2 Figure 14.2",
    "text": "B.2 Figure 14.2\nDescription of Figure 14.2\nThis figure shows TODO",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Detailed Image Descriptions</span>"
    ]
  }
]