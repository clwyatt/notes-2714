[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Notes for ECE 2714: Signals and Systems",
    "section": "",
    "text": "About the Notes\nThis is a set of supplementary notes and examples for ECE 2714 in the Bradley Department of Electrical and Computer Engineering at Virginia Tech.\nSee a mistake? file an issue. This helps improve the notes.",
    "crumbs": [
      "About the Notes"
    ]
  },
  {
    "objectID": "index.html#about-the-notes",
    "href": "index.html#about-the-notes",
    "title": "Supplementary Notes for ECE 2714: Signals and Systems",
    "section": "",
    "text": "License\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\nUpdate History\nThis book is continually updated as new content becomes available and errata corrected.\n\nAugust 2025: Conversion of Chapter 6 complete.\nJuly 2025: Conversion of Chapters 2-5 complete.\nJune 2025: Conversion of Chapter 1 complete.\nFeb 2025: Conversion from LaTeX pdf to accessible html started.",
    "crumbs": [
      "About the Notes"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "To the student:\nThis is a set of supplementary notes and examples for ECE 2714. It is not a replacement for the textbook, but can act as a reference and guide your reading. These notes are not comprehensive – often additional material and insights are covered during class.\nThis material is well covered in the official course text “Oppenheim, A. V., Willsky, A. S., and Nawab, S. H. Signals and Systems, Prentice Hall Pearson, 1996.” (abbreviated OW). This is an older, but very good book. However there are many, many texts that cover the same material. Engaged reading a textbook is one of the most important things you can do to learn this material. Again, these notes should not be considered a replacement for a textbook.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#to-the-instructor",
    "href": "preface.html#to-the-instructor",
    "title": "Preface",
    "section": "To the instructor:",
    "text": "To the instructor:\nThese notes are simply a way to provide some consistency in topic coverage and notation between and within semesters. Feel free to share these with your class but you are under no obligation to do so. There are many alternative ways to motivate and develop this material and you should use the way that you like best. This is just how I do it.\nEach chapter corresponds to a “Topic Learning Objective” and would typically be covered in one class meeting on a Tuesday-Thursday or Monday-Wednesday schedule. Note CT and DT topics are taught interleaved rather than in separate blocks. This gets the student used to going back and forth between the two signal and system types. We introduce time-domain topics first, followed by (real) frequency domain topics, using complex frequency domain for sinusoidal analysis only and as a bridge. Detailed analysis and application of Laplace and Z-transforms is left to ECE 3704.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements:",
    "text": "Acknowledgements:\nThe development of this course has been, and continues to be, a team effort. Dr. Mike Buehrer was instrumental in the initial design and roll-out of the course. Dr. Mary Lanzerotti has helped enormously with the course organization and academic integrity. All the instructors thus far: Drs. Buehrer, Safaai-Jazi, Lanzerotti, Kekatos, Poon, Xu, and Talty, have shaped the course in some fashion.\n\nC.L. Wyatt\nMay 7, 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Course Introduction",
    "section": "",
    "text": "1.1 Example Signals and Systems\nThe concepts and techniques in this course are probably the most useful in engineering. A signal is a function of one or more independent variables conveying information about a physical (or virtual) phenomena. A system may respond to signals to produce other signals, or produce signals directly.\nThis course is about the mathematical models and related techniques for the design and understanding of systems as signal transformations. We focus on a broadly useful class of systems, known as linear, time-invariant systems. You will learn about:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#example-signals-and-systems",
    "href": "01-intro.html#example-signals-and-systems",
    "title": "1  Course Introduction",
    "section": "",
    "text": "Example\n\n\nElectrical Circuits. This is a Sallen-Key filter, a second-order system commonly use to select frequencies from a signal:\n\n\n\nA circuit that implements a Sallen-Key filter.\n\n\nThere are two signals we can easily identify, the input signal as the voltage applied across \\(x(t)\\), and the output voltage measured across \\(y(t)\\). We build on your circuits course by viewing this circuit as an implementation of a more abstract linear system. We see how it can be viewed as a frequency selective filter. We will see how to answer questions such as: how do we choose the values of the resistors and capacitors to select the frequencies we are interested in? and how do we determine what those frequencies are?\n\n\n\n\nExample\n\n\nRobotic Joint. This is a Linear, Time-Invariant model of a DC motor, a mixture of electrical and mechanical components.\n\n\n\nA model of a DC motor.\n\n\nHow do we convert the motor into a servo for use in a robotic joint? What are its characteristics (e.g. how fast can it move)?\n\n\n\n\nExample\n\n\nAudio Processing. Suppose you record an interview for a podcast, but during an important part of the discussion, the HVAC turns on and there is an annoying noise in the background.\n\n\n\nA plot of a noisy signal in the time domain.\n\n\nHow could you remove the noise minimizing distortion to the rest of the audio?\n\n\n\n\nExample\n\n\nCommunications. Consider a wireless sensor, that needs to transmit to a base station, e.g. a wireless mic system.\n\n\n\nDiagram illustrating a wireless transmitter and reciever.\n\n\nHow should the signal be processed so it can be transmitted? How should the received signal be processed?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#types-of-problems",
    "href": "01-intro.html#types-of-problems",
    "title": "1  Course Introduction",
    "section": "1.2 Types of Problems",
    "text": "1.2 Types of Problems\nApplications of this material occur in all areas of science and engineering. When we have a measured output but are unsure what combination of inputs and system components could have produced it, we have a modeling problem.\n\n\n\nA Modeling Problem\n\n\nModels are the bedrock of the scientific method and are required to apply the concepts of this course to engineering problems.\nWhen we know the input and the system description and desire to know the output we have an analysis problem.\n\n\n\nAn Analysis Problem\n\n\nAnalysis problems are the kind you have encountered most often already. For example, given an electrical circuit and an applied voltage or current, what are the voltages and currents across and through the various components.\nWhen we know either the input and desired output and seek the system to perform this transformation,\n\n\n\nAn System Identification Problem\n\n\nor we know the system description and output and desire the input that would generate the output,\n\n\n\nAn Input Identification Problem\n\n\nwe have a design problem or identification problem.\nThis course focuses on modeling and analysis with applications to electrical circuits and devices for measurement and control of the physical world and is broadly applicable to all ECE majors. Some Examples:\n\nControls, Robotics, & Autonomy: LTI systems theory forms the basis of perception and control of machines.\nCommunications & Networking: LTI systems theory forms the basis of transmission and reception of signals, e.g. AM and FM radio.\nMachine Learning: LTI systems are often used to pre-process samples or to create basis functions to improve learning.\nEnergy & Power Electronic Systems: linear circuits are often modeled as LTI systems.\n\nSubsequent courses, e.g. ECE 3704, focus more on analysis and design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-objectives",
    "href": "01-intro.html#learning-objectives",
    "title": "1  Course Introduction",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\nThe learning objectives (LOs) for the course are:\n\nDescribe a given system using a block-level description and identify the input/output signals.\nMathematically model continuous and discrete linear, time-invariant systems using differential and difference equations respectively.\nAnalyze the use of filters and their interpretation in the time and frequency domains and implement standard filters in hardware and/or software.\nApply computations of the four fundamental Fourier transforms to the analysis and design of linear systems.\nCommunicate solutions to problems and document projects within the domain of signals and systems through formal written documents.\n\nThese are broken down further into the following topic learning objectives (TLOs). The TLOs generally map onto one class meeting but are used extensively in later TLOs.\nTLO 1: Course introduction (OW Forward and §1.0)\nTLO 2: Continuous-time (CT) signals (OW §1.1 through 1.4 and 2.5): A continuous-time (CT) signal is a function of one or more independent variables conveying information about a physical phenomena. This lecture gives an introduction to continuous-time signals as functions. You learn how to characterize such signals in a number of ways and are introduced to two very important signals: the unit impulse and the complex exponential.\nTLO 3: Discrete-time (DT) signals (OW §1.1 through 1.4)\nTLO 4: CT systems as linear constant coefficient differential equations (OW §2.4.1)\nTLO 5: DT systems as linear constant coefficient difference equations (OW §2.4.2)\nTLO 6: Linear time invariant CT systems (OW §1.5, 1.6, 2.3)\nTLO 7: Linear time invariant DT systems (OW §1.5, 1.6, 2.3)\nTLO 8: CT convolution (OW §2.2)\nTLO 9: DT convolution (OW §2.1)\nTLO 10: CT block diagrams (OW §1.5.2 and 2.4.3)\nTLO 11: DT block diagrams (OW §1.5.2 and 2.4.3)\nTLO 12: Eigenfunctions of CT systems (OW §3.2 and 3.8)\nTLO 13: Eigenfunctions of DT systems (OW §3.2 and 3.8)\nTLO 14: CT Fourier Series representation of signals (OW §3.3 through 3.5)\nTLO 15: DT Fourier Series representation of signals (OW §3.6 and 3.7)\nTLO 16: CT Fourier Transform (OW §4.0 through 4.7)\nTLO 17: DT Fourier Transform (OW §5.0 though 5.8)\nTLO 18: CT Frequency Response (OW §6.1, 6.2, 6.5)\nTLO 19: DT Frequency Response (OW §6.1, 6.2, 6.6)\nTLO 20: Frequency Selective Filters in CT (OW §3.9, 3.10, 6.3, 6.4)\nTLO 21: Frequency Selective Filters in DT (OW §3.11, 6.3, 6.4)\nTLO 22: The Discrete Fourier Transform\nTLO 23: Sampling (OW §7.1, 7.3, 7.4)\nTLO 24: Reconstruction (OW §7.2)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#graphical-outline",
    "href": "01-intro.html#graphical-outline",
    "title": "1  Course Introduction",
    "section": "1.4 Graphical Outline",
    "text": "1.4 Graphical Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html",
    "href": "02-ct-signals.html",
    "title": "2  Continuous-Time Signals",
    "section": "",
    "text": "2.1 Signals as Functions\nA continuous-time (CT) signal is a function of one or more independent variables conveying information about a physical phenomena. This lecture gives an introduction to continuous-time signals as functions. You learn how to characterize such signals in a number of ways and are introduced to two very important signals: the unit impulse and the complex exponential.\nIn order to reason about signals mathematically we need a representation or model. Signals are modeled as functions, mappings between sets \\[\nf: A \\rightarrow B\n\\] where \\(A\\) is a set called the domain and \\(B\\) is a set called the range.\nThe most basic classification of signals depends on the sets that makeup the domain and co-domain. We will be interested in two versions of the domain, the reals denoted \\(\\mathbb{R}\\) and the integers denoted \\(\\mathbb{Z}\\). We will be interested in two versions of the co-domain, the reals \\(\\mathbb{R}\\) and the set of complex numbers \\(\\mathbb{C}\\).\nSome other possibilities:\nThe co-domain can also be complex.\nSince the domains \\(\\mathbb{R}\\) and \\(\\mathbb{Z}\\) are usually interpreted as time, we will call these time-domain signals. In the time-domain, when the co-domain is \\(\\mathbb{R}\\) we call these real signals. All physical signals are real. However complex signals will become important when we discuss the frequency domain.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#signals-as-functions",
    "href": "02-ct-signals.html#signals-as-functions",
    "title": "2  Continuous-Time Signals",
    "section": "",
    "text": "Example\n\n\nAnalog Signal: If the function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), we call this an analog or real, continuous-time signal, e.g. a voltage at time \\(t \\in \\mathbb{R}\\), \\(v(t)\\). We will write these as \\(x(t)\\), \\(y(t)\\), etc. The units of \\(t\\) are seconds. Figure 2.1 shows some graphical representations, i.e. plots.\n\n\n\n\n\n\n\n\nFigure 2.1: Example plots of analog signals.\n\n\n\n\n\n\n\n\n\nExample\n\n\nReal, Discrete-time Signal: If the function \\(f: \\mathbb{Z} \\rightarrow \\mathbb{R}\\), we call this a real, discrete-time signal, e.g. the temperature every day at noon. We will write these as \\(x[n]\\), \\(y[n]\\), etc. Note \\(n\\) is dimensionless. Figure 2.2 shows some graphical representations.\n\n\n\n\n\n\n\n\nFigure 2.2: Example plots of real-valued, discrete-time signals.\n\n\n\n\n\n\n\n\n\n\\(f: \\mathbb{R} \\rightarrow \\mathbb{Z}\\), digital, continuous-time signals, e.g. the output of a general purpose pin on a microcontroller\n\\(f: \\mathbb{Z} \\rightarrow \\mathbb{Z}\\), digital, discrete-time signals, e.g. the signal on a computer bus\n\n\n\n\\(f: \\mathbb{R} \\rightarrow \\mathbb{C}\\), complex-valued, continuous-time signals, e.g. \\[\nx(t) = e^{j\\omega t} = \\cos(\\omega t) + j\\sin(\\omega t)\n\\]\n\\(f: \\mathbb{Z} \\rightarrow \\mathbb{C}\\), complex-valued, discrete-time signals, e.g. \\[\nx[n] = e^{j\\omega n} = \\cos(\\omega n) + j\\sin(\\omega n)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#primitive-models",
    "href": "02-ct-signals.html#primitive-models",
    "title": "2  Continuous-Time Signals",
    "section": "2.2 Primitive Models",
    "text": "2.2 Primitive Models\nWe mathematically model signals by combining elementary/primitive functions, for example:\n\npolynomials: \\(x(t) = t\\), \\(x(t) = t^2\\), etc.\ntransendental functions: \\(x(t) = e^t\\), \\(x(t) = \\sin(t)\\), \\(x(t) = \\cos(t)\\), etc.\npiecewise functions, e.g. \\[\n   x(t) = \\left\\{  \\begin{array}{cl}\n     f_1(t) & t &lt; 0\\\\\n     f_2(t) & t \\geq 0\\\\\n   \\end{array}\\right.\n\\]\n\n\n\nExample\n\n\nModeling a Switch: Consider a mathematical model of a switch, which moves positions at time \\(t = 0\\).\n\n\n\n\n\n\nFigure 2.3: Single pole, single throw switch connected to a unit DC source.\n\n\n\nWe use this model so much we give it it’s own name and symbol: Unit Step, \\(u(t)\\)\n\\[\nu(t) = \\left\\{  \\begin{array}{cl}\n        0 & t &lt; 0\\\\\n        1 & t \\geq 0\\\\\n      \\end{array}\\right.\n\\] so a mathematical model of the switch circuit above would be \\(x(t) = V u(t)\\).\nNote: some texts define the step function at \\(t=0\\) to be \\(1\\) or \\(\\frac{1}{2}\\). It is typically plotted like so:\n\n\n\n\n\n\nFigure 2.4: Plot of the unit step function. It turns on at the time origin and stays on forever.\n\n\n\n\n\n\n\nExample\n\n\nPure audio tone at “middle C”. A signal modeling the air pressure of a specific tone might be\n\\[\n  x(t) = \\sin\\left(2\\pi (261.6) t\\right)\n\\]\n\n\n\n\nExample\n\n\nChord. The chord “G”, an additive mixture of tones at G, B, and D and might be modeled as\n\\[\nx(t) = \\sin\\left(2\\pi (392) t\\right) + \\sin\\left(2\\pi (494) t\\right) + \\sin\\left(2\\pi (293) t\\right)\n\\]\nThis example shows we can use addition to build-up signals to approximate real signals of interest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#basic-transformations",
    "href": "02-ct-signals.html#basic-transformations",
    "title": "2  Continuous-Time Signals",
    "section": "2.3 Basic Transformations",
    "text": "2.3 Basic Transformations\nWe can also apply transformations to signals to increase their modeling flexibility.\n\nmagnitude scaling \\[\nx_2(t) = a x_1(t)\n\\] for \\(a \\in \\mathbb{R}\\).\nderivatives \\[\nx_2(t) = x_1^\\prime(t) = \\frac{d x_1}{dt}(t)\n\\]\nintegrals \\[\nx_2(t) = \\int\\limits_{-\\infty}^t x_1(\\tau) \\; d\\tau\n\\]\nsums \\[\ny(t) = \\sum\\limits_{i} x_i(t)\n\\] an important example we will see is the CT Fourier series.\n\nmultiplication (modulation) \\[\ny(t) = x_1(t) x_2(t)\n\\] For example amplitude modulation \\(y(t) = x(t)\\sin(\\omega_0 t)\\)\ntime shift \\[\nx_2(t) = x_1(t+\\tau)\n\\]\n\nif \\(\\tau &lt;0\\) it is called a delay\nif \\(\\tau &gt;0\\) it is called an advance\n\ntime scaling \\[\n  x_2(t) = x_1\\left(\\frac{t}{\\tau}\\right)\n  \\]\n\nif \\(\\tau &gt;1\\) increasing \\(\\tau\\) expands in time, slows down the signal\nif \\(0 &lt; \\tau &lt; 1\\) decreasing \\(\\tau\\) contracts in time, speeds up the signal\nif \\(-1 &lt; \\tau &lt;0\\) time reverses and increasing \\(\\tau\\) contracts in time, speeding up the signal\nif \\(\\tau &lt; -1\\) time reverses and decreasing \\(\\tau\\) expands in time, slows down the signal\n\nCommon uses are time reversal, \\(x_2(t) = x_1(-t)\\), and changing the frequency of of sinusoids.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#characterization-of-signals",
    "href": "02-ct-signals.html#characterization-of-signals",
    "title": "2  Continuous-Time Signals",
    "section": "2.4 Characterization of Signals",
    "text": "2.4 Characterization of Signals\nThere are a few basic ways of characterizing signals.\n\n\nDefinition\n\n\nCausal CT Signal. A CT signal is if \\(x(t) = 0\\) \\(\\forall t &lt; 0\\).\nAnti-Causal CT Signal. A CT signal is or acausal if \\(x(t) = 0\\) \\(\\forall t \\geq 0\\).\n\n\nA signal can be written as the sum of a causal and anti-causal signal.\n\n\nDefinition\n\n\nPeriodic Signals. A CT signal is if \\(x(t) = x(t + T)\\) \\(\\forall t\\) for a fixed parameter \\(T \\in \\mathbb{R}\\) called the .\n\n\nThe simplest periodic signals are those based on the sinusoidal functions.\n\n\nDefinition\n\n\nEven Signal. A CT signal is if \\(x(t) = x(-t)\\) \\(\\forall t\\).\nOdd Signal.  A CT signal is if \\(x(t) = -x(-t)\\) \\(\\forall t\\).\n\n\nAny CT signal can be written in terms of an even and odd component \\[\nx(t) = x_e(t) + x_o(t)\n\\] where \\[\n\\begin{array}{ll}\n  x_e(t) &= \\frac{1}{2}\\left\\{x(t) + x(-t)\\right\\} \\\\\n  & \\\\\n  x_o(t) &= \\frac{1}{2}\\left\\{x(t) - x(-t)\\right\\}\n\\end{array}\n\\]\n\n\nDefinition\n\n\nEnergy of a CT Signal. The energy of a CT signal \\(x(t)\\) is defined as a measure of the function \\[\n  E_x = \\lim_{T\\rightarrow\\infty} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n  \\]\n\n\n\n\nDefinition\n\n\nPower of a CT Signal. The power of a CT signal is the energy averaged over an interval as that interval tends to infinity. \\[\n  P_x = \\lim_{T\\rightarrow\\infty} \\frac{1}{2T} \\int_{-T}^T \\lvert x(t)\\rvert^2 dt \\; .\n  \\]\n\n\nSignals can be characterized based on their energy or power:\n\nSignals with finite, non-zero energy and zero power are called energy signals.\nSignals with finite, non-zero power (and by implication infinite energy) are called power signals.\n\nNote, these categories are non-exclusive, some signals are neither energy or power signals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#unit-impulse-function",
    "href": "02-ct-signals.html#unit-impulse-function",
    "title": "2  Continuous-Time Signals",
    "section": "2.5 Unit Impulse Function",
    "text": "2.5 Unit Impulse Function\nAn important CT signal is the unit impulse function, also called the “delta” \\(\\delta\\) function for the symbol traditionally used to define it. Applying this signal to a system models a “kick” to that system. For example, consider striking a tuning fork. The reason this signal is so important is that it will turn out that the response of the system to this input tells us all we need to know about a linear, time-invariant system!\n\n\nExample\n\n\nCT Impulse Function. The CT impulse function is not really a function at all, but a mathematical object called a “distribution”. Some equivalent definitions:\n\\[\n\\delta(t) = \\lim_{\\epsilon \\rightarrow 0}\\left\\{\n\\begin{array}{ll}\n  \\frac{1}{2\\epsilon} & |t| &lt; \\epsilon\\\\\n  0 & \\text{else}\n\\end{array}\n\\right.\n\\]\n\\[\n\\delta(t) = \\lim_{\\epsilon \\rightarrow 0} \\frac{1}{\\sqrt{2\\pi}\\epsilon} e^{-\\frac{t^2}{2\\epsilon^2}}\n\\] Note the area under each definition is always one.\n\n\nIn practice we can often use the following definition and some properties, without worrying about the distribution functions. \\[\n\\delta(t) = \\left\\{\n\\begin{array}{ll}\n  0 & t \\neq 0\\\\\n  \\infty & t = 0\n\\end{array}\n\\right.\n\\] which we draw as a vertical arrow in plots:\n\n\n\n\n\n\nFigure 2.5: Plot of the CT delta function.\n\n\n\nNote the height of the arrow is arbitrary. Often in the case of a non-unit impulse function the area is written in parenthesis near the arrow tip.\nThe following properties of the impulse function will be used often.\n\nThe area under the unit impulse is unity since by definition \\[\n\\int\\limits_{-\\infty}^{\\infty} \\delta(t) \\; dt = 1\n\\]\nSampling property: \\(x(t)\\delta(t-t_0) = x(t_0)\\delta(t-t_0)\\)\nSifting Property: \\[\n\\int\\limits_{a}^{b} x(t)\\delta(t-t_0) \\; dt = x(t_0)\n\\] for any \\(a &lt; t_0 &lt; b\\).\n\nWe previously defined the unit step function. The impulse can be defined in terms of the step: \\[\n\\delta(t) = \\frac{du}{dt}\n\\] and vice-versa \\[\nu(t) = \\int\\limits_{-\\infty}^{t} \\delta(\\tau) \\; d\\tau\n\\] using the notion of distributions, e.g.\n\\[\nu(t) = \\int\\limits_{-\\infty}^{t} \\delta(\\tau) \\; d\\tau = \\lim_{\\epsilon \\rightarrow 0} \\int\\limits_{-\\infty}^{t} \\frac{1}{\\sqrt{2\\pi}\\epsilon} e^{-\\frac{\\tau^2}{2\\epsilon^2}} \\; d\\tau = \\lim_{\\epsilon \\rightarrow 0} \\frac{1}{2}\\left(1+\\text{erf}\\left( \\frac{t}{\\sqrt{2}\\epsilon}\\right)\\right)\n\\]\nThe step and impulse function are related, but in many cases finding the response of a system to a step input is easier.\nWe can apply additional transformations to the impulse and step functions to get other useful signals, e.g.\n\nramp \\[\nr(t) = \\int\\limits_{-\\infty}^{t} u(\\tau) \\; d\\tau = tu(t)\n\\]\ncausal pulse of width \\(\\epsilon\\) \\[\np(t) = u(t) - u(t-\\epsilon)\n\\]\nnon-causal pulse of width \\(2\\epsilon\\) \\[\n    p(t) = u(t+\\epsilon) - u(t-\\epsilon)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#ct-complex-exponential",
    "href": "02-ct-signals.html#ct-complex-exponential",
    "title": "2  Continuous-Time Signals",
    "section": "2.6 CT Complex Exponential",
    "text": "2.6 CT Complex Exponential\nOne of the most important signals in systems theory is the complex exponential: \\[\nx(t) = C\\, e^{a t}\n\\] where the parameters \\(C, a \\in \\mathbb{C}\\) in general.\nWhen \\(C\\) and \\(a\\) are both real (\\(\\Im(C) = \\Im(a) = 0\\)), we have the familiar exponential. When \\(a &gt; 0\\) and \\(C &gt; 0\\), \\(x(t) = C e^{a t}\\) looks like:\n\n\n\n\n\n\nFigure 2.6: Plot of the expoential function with real, positive parameter.\n\n\n\nWhen \\(a &lt; 0\\) and \\(C &gt; 0\\), \\(x(t) = C e^{a t}\\) looks like:\n\n\n\n\n\n\nFigure 2.7: Plot of the expoential function with real, negative parameter.\n\n\n\nIf \\(C &lt; 0\\) the signals reflect about the time axis.\nTo get the pure sinusoidal case, let \\(C \\in \\mathbb{R}\\) and \\(a\\) be purely imaginary: \\(a = j\\omega_0\\): \\[\nx(t) = Ce^{j\\omega_0 t}\n\\] where \\(\\omega_0\\) is the frequency (in radians/sec). This is called the complex sinusoid.\nBy Euler’s identity: \\[\ne^{j\\omega_0 t} = \\cos(\\omega_0 t) + j\\sin(\\omega_0 t)\n\\] and \\[\n\\Re(x(t)) = \\cos(\\omega_0 t) = \\frac{1}{2}\\left( e^{j\\omega_0 t} + e^{-j\\omega_0 t} \\right)\n\\]\n\\[\n\\Im(x(t)) = \\sin(\\omega_0 t) = \\frac{1}{2j}\\left( e^{j\\omega_0 t} - e^{-j\\omega_0 t} \\right)\n\\] are both real sinusoids.\nNote that the sinusoids are periodic. Recall a signal \\(x(t)\\) is periodic with period \\(T\\) if \\[\nx(t) = x(t+T) \\; \\forall t\n\\] In the case of the complex sinusoid \\[\nCe^{j\\omega_0 t} = Ce^{j\\omega_0 (t+T)}= Ce^{j\\omega_0 t}\\underbrace{e^{j\\omega_0 T}}_{\\text{must be 1}}\n\\]\n\nif \\(\\omega_0 = 0\\) this is true for all \\(T\\)\nif \\(\\omega_0 \\neq 0\\), then to be periodic \\(\\omega_0 T = 2\\pi m\\) for \\(m = \\pm 1, \\pm 2, \\cdots\\). The smallest \\(T\\) for which this is true is the fundamental period \\(T_0\\) \\[\nT_0 = \\frac{2\\pi}{|\\omega_0|}\n\\] or equivalently \\(\\omega_0 = \\frac{2\\pi}{T_0}\\)\n\nSome useful properties of sinusoids:\n\nIf \\(x(t)\\) is periodic with period \\(T\\) and \\(g\\) is any function then \\(g(x(t))\\) is periodic with period \\(T\\).\nIf \\(x_1(t)\\) is periodic with period \\(T_1\\) and \\(x_2(t)\\) is periodic with period \\(T_2\\), and if there exists positive integers \\(a,b\\) such that \\[\naT_1 = b T_2 = P\n\\] then \\(x_1(t) + x_2(t)\\) and \\(x_1(t)x_2(t)\\) are periodic with period \\(P\\)\n\nThe last property implies that both \\(T_1\\) and \\(T_2\\) must both be rational in \\(\\pi\\) or neither should be. For example\n\n\\(x(t) = \\sin(2\\pi t) + \\cos(5\\pi t)\\) is periodic\n\\(x(t) = \\sin(2 t) + \\cos(5 t)\\) is periodic\n\\(x(t) = \\sin(2\\pi t) + \\cos(5 t)\\) is not periodic\n\nWhen the parameter \\(C\\) is complex we get a phase shift. Again let \\(a = j\\omega_0\\). When \\(C\\) is complex we can write it as \\(C = Ae^{j\\phi}\\) where \\(A = |C|\\) and \\(\\phi = \\angle C\\). Then\n\\[\nx(t) = Ae^{j\\phi} e^{j\\omega_0 t} = Ae^{j(\\omega_0 t+\\phi)}\n\\] and \\[\n\\Re(x(t)) = A\\cos(\\omega_0 t+\\phi)\n\\]\n\\[\n\\Im(x(t)) = A\\sin(\\omega_0 t+\\phi)\n\\]\nSince \\(\\sin\\) is a special case of \\(\\cos\\), i.e. \\(\\cos(\\theta) = \\sin(\\theta + \\frac{\\pi}{2})\\), the general real sinusoid is\n\\[\nA\\cos(\\omega_0 t + \\phi)\n\\]\n\n\\(A\\) is called the amplitude\n\\(\\omega_0\\) is again the frequency in radians/sec.\n\\(\\phi\\) is called the phase shift and is related to a time shift \\(T_s\\) by \\[\n\\phi = \\omega_0T_s\n\\]\n\nFor example the signal graphically represented as follows\n\n\n\n\n\n\n\n\nFigure 2.8: Example plot of sinusoidal signal.\n\n\n\n\n\nhas the functional representation\n\\[\nx(t) = 2\\cos\\left(\\frac{\\pi}{2} (t+\\tfrac{1}{2}) \\right) =  2\\cos\\left(\\frac{\\pi}{2} t +\\frac{\\pi}{4} \\right)\n\\]\n\n2.6.1 Energy of CT complex sinusoid\nRecall the energy of a CT signal \\(x(t)\\) is\n\\[\n  E_x = \\lim_{T\\rightarrow\\infty} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n\\] Substituting \\(x(t) = e^{j\\omega_0 t}\\) and letting \\(T = N T_0\\) \\[\n    E_x = \\lim_{N\\rightarrow\\infty} \\int\\limits_{-N T_0}^{N T_0} \\underbrace{\\lvert e^{j\\omega_0 t} \\rvert^2}_{\\text{always 1}} \\; dt = \\lim_{N\\rightarrow\\infty} 2NT_0 = \\infty\n\\]\n\n\n2.6.2 Power of CT complex sinusoid\nRecall the power of a CT signal \\(x(t)\\) is \\[\n  P_x = \\lim_{T\\rightarrow\\infty} \\frac{1}{2T} \\int\\limits_{-T}^T \\lvert x(t) \\rvert^2 dt \\; .\n\\] Again, substituting \\(x(t) = e^{j\\omega_0 t}\\) and letting \\(T = N T_0\\) \\[\n  P_x = \\lim_{N\\rightarrow\\infty} \\frac{1}{2NT_0} \\int\\limits_{-N T_0}^{N T_0} \\underbrace{\\lvert e^{j\\omega_0 t} \\rvert^2}_{\\text{always 1}} \\; dt = \\lim_{N\\rightarrow\\infty} \\frac{1}{2NT_0} 2NT_0 = 1\n\\]\n\n\n2.6.3 Harmonics\nTwo CT complex sinusoids are harmonics of one another is both are periodic in \\(T_0\\). This occurs when\n\\[\n    x_k(t) = e^{jk\\omega_0 t} \\; \\text{for} \\; k = 0, \\pm 1, \\pm 2, \\cdots\n\\]\nThe term comes from music where the vibrations of a string instrument are modeled as a weighted combination of harmonic tones.\n\n\n2.6.4 Geometric interpretation of the Complex Exponential\nIn the general case we get a sinusoid signal modulated by an exponential. Let \\(C = Ae^{j\\phi}\\) and \\(a = r + j\\omega_0\\), then \\[\n  x(t) = C e^{a t} =  Ae^{j\\phi} e^{(r+j\\omega_0)t}\n\\] Expanding the terms and using Euler’s identity gives: \\[\nx(t) = \\underbrace{Ae^{rt}\\cos(\\omega_0 t+\\phi)}_{\\Re \\text{part}} + j \\underbrace{Ae^{rt}\\sin(\\omega_0 t+\\phi)}_{\\Im \\text{part}}\n\\] Each part is a real sinusoid whose amplitude is modulated by a real exponential.\nAn important visualization of the general case is to view the signal \\(x(t)\\) as a vector rotating counter-clockwise in the complex plane for positive \\(t\\).\n\n\n\n\n\n\nFigure 2.9: The CT complex sinusoid at a specific point in time.\n\n\n\nFor \\(r &lt; 0\\) the tip of the arrow traces out an inward spiral, whereas for \\(r &gt; 0\\) it traces out an outward spiral. For \\(r = 0\\) it traces out the unit circle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "02-ct-signals.html#example-problems",
    "href": "02-ct-signals.html#example-problems",
    "title": "2  Continuous-Time Signals",
    "section": "2.7 Example Problems",
    "text": "2.7 Example Problems\n\n2.7.1 \nConsider a signal described by the function \\[\n  x(t) = e^{-3t}\\sin(10\\pi t)u(t)\n   \\]\n\nDetermine the magnitude and phase of \\(x\\left( \\frac{1}{20}\\right)\\)\n\nSolution:\nSubstituting \\(t = \\frac{1}{20}\\) gives \\[\n  x\\left( \\frac{1}{20}\\right) = e^{-3\\frac{1}{20}}\\sin\\left(10\\pi \\frac{1}{20}\\right)u\\left( \\frac{1}{20}\\right) = e^{-\\frac{3}{20}} \\approx 0.86\n  \\] Since the signal is purely real and exponential is always positive, the magnitude is \\[\n  \\left|x\\left( \\frac{1}{20}\\right)\\right| = \\left| e^{-\\frac{3}{20}}\\right| =  e^{-\\frac{3}{20}}  \\approx 0.86\n  \\] and the phase is \\[\n  \\angle x\\left( \\frac{1}{20}\\right) = 0\n  \\]\n\nUsing Matlab, plot the signal \\(|x(t)|\\) between \\([-2, 2]\\). Give your code and embed the plot.\n\nSolution:\n% Solution to Example Problem 2.7.1b\n1t = -2:0.001:2;\n2x = exp(-3*t).*sin(10*pi*t).*heaviside(t);\n3hp = plot(t,abs(x));\ngrid on;\nxh = xlabel('t');\nyh = ylabel('x(t)');\nth = title('Plot for Example Problem 2.7.1b');\n\n% make the plot more readable\nset(gca, 'FontSize', 12, 'Box', 'off', 'LineWidth', 2);\nset(hp, 'linewidth', 2);\nset([xh, yh, th], 'FontSize', 12);\n\nset(gcf, 'PaperPositionMode', 'auto');\nprint -dpng example_2_7_1.png\n\n1\n\nCreate time slices from -2 seconds to 2 seconds in increments of 1 millisecond\n\n2\n\nCompute the signal value at each time slice\n\n3\n\nPlot the signal\n\n\n\n\n2.7.2 \nFind a solution to the differential equation \\[\n  \\frac{dy}{dt}(t) + 9y(t) = e^{-t}\n  \\] for \\(t \\geq 0\\), when \\(y(0) = 1\\).\nSolution: The homogeneous equation is \\[\n  \\frac{dy_h}{dt}(t) + 9y_h(t) = 0\n  \\] with initial condition \\(y_h(0) = 1\\). Its solution is of the form \\[\n  y_h(t) = C\\, e^{-9t}\n  \\] for constant \\(C\\). Using the initial condition \\[\n  y_h(0) = C\\, e^{-0} = C = 1\n  \\] gives \\[\n  y_h(t) = e^{-9t}\n  \\] The particular solution is of the form \\[\n  y_p(t) = C_1 e^{-t} + C_2 e^{-9t}\n  \\] Substitution and equating coefficients gives \\(C_1 = \\frac{1}{8}\\) and \\(C_2 = -\\frac{1}{8}\\). The total solution is the sum of the two solutions or \\[\n  y(t) = \\frac{1}{8} e^{-t} - \\frac{1}{8} e^{-9t} + e^{-9t} = \\frac{1}{8} e^{-t} + \\frac{7}{8} e^{-9t}\n  \\]\n\n\n2.7.3 \nFind a solution to the differential equation \\[\n  \\frac{dy}{dt}(t) + 9y(t) = e^{-t}\n  \\] for \\(t \\geq 0\\), when \\(y(0) = 1\\).\nSolution: The homogeneous equation is \\[\n  \\frac{dy_h}{dt}(t) + 9y_h(t) = 0\n  \\] with initial condition \\(y_h(0) = 1\\). Its solution is of the form \\[\n  y_h(t) = C\\, e^{-9t}\n  \\] for constant \\(C\\). Using the initial condition \\[\n  y_h(0) = C\\, e^{-0} = C = 1\n  \\] gives \\[\n  y_h(t) = e^{-9t}\n  \\] The particular solution is of the form \\[\n  y_p(t) = C_1 e^{-t} + C_2 e^{-9t}\n  \\] Substitution and equating coefficients gives \\(C_1 = \\frac{1}{8}\\) and \\(C_2 = -\\frac{1}{8}\\). The total solution is the sum of the two solutions or \\[\n  y(t) = \\frac{1}{8} e^{-t} - \\frac{1}{8} e^{-9t} + e^{-9t} = \\frac{1}{8} e^{-t} + \\frac{7}{8} e^{-9t}\n  \\]\n\n\n2.7.4 \nCompute the integral \\[\n    \\int\\limits_{-\\infty}^{\\infty} e^{-t^2} \\, \\delta(t-10)\\; dt\n    \\] where \\(\\delta(t)\\) is the delta function.\nSolution:\nUsing the sifting property of the delta function \\[\n  \\int\\limits_{a}^{b} f(t) \\, \\delta(t-t_0)\\; dt = f(t_0)\n  \\] for \\(a &lt; t_0 &lt; b\\), we get \\[\n  \\int\\limits_{-\\infty}^{\\infty} e^{-t^2} \\, \\delta(t-10)\\; dt = e^{-100} \\approx 0\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html",
    "href": "03-dt-signals.html",
    "title": "3  Discrete-Time Signals",
    "section": "",
    "text": "3.1 Primitive Models\nRecall from the previous chapter that a discrete-time (DT) signal is modeled as a function \\(f: \\mathbb{Z} \\rightarrow \\mathbb{C}\\). We will write these as \\(x[n]\\), \\(y[n]\\), etc. Note \\(n\\) is dimensionless. These are graphically plotted as stem or “lollipop” plots, as demonstrated in Chapter 2.\nSince the domain \\(\\mathbb{Z}\\) is usually interpreted as a time index, we will still call these time-domain signals. In the time-domain, when the co-domain is \\(\\mathbb{R}\\) we call these real DT signals. Unlike with CT signals there are no physical limitations requiring DT signals to be real, since in discrete hardware, a value at a given index can be a complex number, i.e. just a pair of numbers. However it is computationally advantageous to restrict ourselves to real arithmetic and such signals are often converted to or from CT signals, which do have to be real. For this reason, real DT signals dominate in models.\nAs with CT signals, we mathematically model DT signals by combining elementary/primitive functions, for example:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#primitive-models",
    "href": "03-dt-signals.html#primitive-models",
    "title": "3  Discrete-Time Signals",
    "section": "",
    "text": "polynomials: \\(x[n] = n\\), \\(x[n] = n^2\\), etc.\ntransendental functions: \\(x[n] = e^n\\), \\(x[n] = \\sin(n)\\), \\(x[n] = \\cos(n)\\), etc.\npiecewise functions, e.g. \\[\nx[n] = \\left\\{  \\begin{array}{cl}\nf_1[n] & n &lt; 0\\\\\nf_2[n] & n \\geq 0\\\\\n\\end{array}\\right.\n\\]\n\n\n\nDefinition\n\n\nThe DT counterpart of the CT step function is the DT Unit Step, \\(u[n]\\): \\[u[n] = \\left\\{  \\begin{array}{cl}\n    0 & n &lt; 0\\\\\n    1 & n \\geq 0\\\\\n  \\end{array}\\right.\\] Note, there are not continuity issues at \\(n=0\\) as DT functions have discrete domains.\n\n\n\n\nExample\n\n\nA sampled signal modeling the air pressure of a specific tone, sampled at 8kHz, might be \\[x[n] = \\sin\\left(2\\pi (261.6) \\tfrac{1}{8000} n\\right)\\] Such DT signals are commonly used in digital music generation, storage, and playback.\n\n\n\n\nExample\n\n\nSimilarly, the sampled chord \"G\", an additive mixture of tones at G, B, and D and might be modeled as \\[x[n] = \\sin\\left(2\\pi (392) \\tfrac{1}{8000} n\\right) + \\sin\\left(2\\pi (494) \\tfrac{1}{8000} n\\right) + \\sin\\left(2\\pi (293) \\tfrac{1}{8000} n\\right)\\] again sampled at 8kHz. This example shows we can use addition to build-up signals to approximate real signals of interest.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#basic-transformations",
    "href": "03-dt-signals.html#basic-transformations",
    "title": "3  Discrete-Time Signals",
    "section": "3.2 Basic Transformations",
    "text": "3.2 Basic Transformations\nSimilar to CT signals, we can also apply transformations to DT signals to increase their modeling flexibility.\n\nmagnitude scaling \\[x_2[n] = a x_1[n]\\] for \\(a \\in \\mathbb{R}\\).\ntime differences \\[x_2[n] = x_1[n] - x_1[n-1]\\]\nrunning sums \\[x_2[n] = \\sum\\limits_{m = -\\infty}^{n} x_1[m]\\]\nsums \\[y[n] = \\sum\\limits_{i} x_i[n]\\] an important example we will see is the DT Fourier series.\nmultiplication (modulation) \\[y[n] = x_1[n] x_2[n]\\]\ntime index shift \\[x_2[n] = x_1[n+m]\\]\n\nif \\(m &lt; 0\\) it is called a delay\nif \\(m &gt; 0\\) it is called an advance\n\ntime reversal \\[x_2[n] = x_1[-n]\\]\ndecimation \\[y[n] = x[m n]\\] for \\(m \\in \\mathbb{Z}^+\\).\n\ne.g. for \\(m=2\\) only keep every other sample\ne.g. for \\(m=3\\) only keep every third sample\netc.\n\ninterpolation \\[y[n] = \\left\\{  \\begin{array}{cl}\nx\\left[ \\frac{n}{m}\\right] & n = 0\\; , \\; \\pm m, , \\; \\pm 2m \\cdots\\\\\n0 & \\mbox{else}\n\\end{array}\\right.\\] When \\(m = 2\\) this inserts a zero sample between every sample of the signal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#characterization-of-signals",
    "href": "03-dt-signals.html#characterization-of-signals",
    "title": "3  Discrete-Time Signals",
    "section": "3.3 Characterization of Signals",
    "text": "3.3 Characterization of Signals\nThere are a few basic ways of characterizing DT signals.\n\n\nDefinition\n\n\nA DT signal is causal if \\(x[n] = 0\\) \\(\\forall n &lt; 0\\).\n\n\n\n\nDefinition\n\n\nA DT signal is anti-causal or acausal if \\(x[n] = 0\\) \\(\\forall n \\geq 0\\).\n\n\nA DT signal can be written as the sum of a causal and anti-causal signal.\nA DT signal is periodic if \\(x[n] = x[n + N] \\; \\forall n\\) for a fixed period \\(N \\in \\mathbb{Z}\\).\nA DT signal is even if \\(x[n] = x[-n] \\; \\forall n\\).\nA DT signal is odd if \\(x[n] = -x[-n] \\; \\forall n\\).\nAny DT signal can be written in terms of an even and odd component \\[x[n] = x_e[n] + x_o[n]\\] where \\[\\begin{array}{ll}\nx_e[n] &= \\frac{1}{2}\\left\\{x[n] + x[-n]\\right\\} \\\\\n& \\\\\nx_o[n] &= \\frac{1}{2}\\left\\{x[n] - x[-n]\\right\\}\n\\end{array}\\]\nAnalogous to CT signals, the energy of a DT signal is \\[E_x = \\lim_{N\\rightarrow\\infty} \\sum\\limits_{-N}^N \\lvert x[n]\\rvert^2 \\; .\\]\nAnd the power of a DT signal is the energy averaged over an interval as that interval tends to infinity.\n\\[P_x = \\lim_{N\\rightarrow\\infty} \\frac{1}{2N+1} \\sum\\limits_{-N}^N \\lvert x[n]\\rvert^2 \\; .\\]\nDT Signals with finite, non-zero energy and zero power are called energy signals. DT Signals with finite, non-zero power (and by implication infinite energy) are called power signals. These categories are non-exclusive, some signals are neither energy or power signals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#dt-unit-impulse-function",
    "href": "03-dt-signals.html#dt-unit-impulse-function",
    "title": "3  Discrete-Time Signals",
    "section": "3.4 DT Unit Impulse Function",
    "text": "3.4 DT Unit Impulse Function\nIn DT the unit impulse function, denoted \\(\\delta[n]\\) is defined as \\[\\delta[n] = \\left\\{\n\\begin{array}{ll}\n  1 & n = 0\\\\\n  0 & \\text{else}\n\\end{array}\n\\right.\\] Note this definition is straightforward compared to the CT impulse as there are no continuity issues and it is not defined in terms of a distribution. It is typically drawn as\n\n\n\n\n\n\n\n\nFigure 3.1: Plot of discrete-time delta function.\n\n\n\n\n\nSome useful properties of the DT impulse function are:\n\nEnergy is 1: \\(\\sum\\limits_{n=-\\infty}^{\\infty} \\delta[n] = 1\\)\nSampling: \\(x[n]\\delta[n-n_0] = x[n_0]\\delta[n-n_0]\\)\nSifting: \\(\\sum\\limits_{n=-\\infty}^{\\infty} x[n]\\delta[n-n_0] = x[n_0]\\)\n\nThe impulse can be defined in terms of the step: \\[\\delta[n] = u[n] - u[n-1]\\] and vice-versa \\[u[n] = \\sum\\limits_{m=-\\infty}^{n} \\delta[m]\\] or \\[u[n] = \\sum\\limits_{k=0}^{\\infty} \\delta[n-k]\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "03-dt-signals.html#dt-complex-exponential",
    "href": "03-dt-signals.html#dt-complex-exponential",
    "title": "3  Discrete-Time Signals",
    "section": "3.5 DT Complex Exponential",
    "text": "3.5 DT Complex Exponential\nThe DT Complex Exponential is defined in a similar fashion the the CT version, but with some important differences. The general DT complex exponential is given by the expression: \\[x[n] = Ce^{\\beta n}\\] where in general \\(C \\in \\mathbb{C}\\) and \\(\\beta \\in \\mathbb{C}\\). It is sometimes convenient (for reasons we will see later) to write this as \\[x[n] = C \\alpha^n\\] where \\(\\alpha = e^{j\\theta}\\) is a complex number \\(\\alpha = \\cos(\\theta) + j\\sin(\\theta)\\).\nWe now examine several special cases.\n\n3.5.1 DT Complex Exponential: real case\nLet \\(C\\) and \\(\\alpha\\) be real, then there are four intervals of interest:\n\n\\(\\alpha &gt; 1\\)\n\\(0 &lt; \\alpha &lt; 1\\)\n\\(-1 &lt; \\alpha &lt; 0\\)\n\\(\\alpha &lt; -1\\)\n\nEach of these are shown in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: DT Complex Exponential: real case, four intervals of interest.\n\n\n\n\n\n\n\n3.5.2 DT Complex Exponential: sinusoidal case\nLet \\(C = 1\\). When \\(\\beta\\) is purely imaginary, \\(\\beta = j\\omega_0\\) \\[x[n] = e^{j\\omega_0 n}\\]\nAs in CT, by Euler’s identity: \\[e^{j\\omega_0 n} = \\cos(\\omega_0 n) + j\\sin(\\omega_0 n)\\] and \\[\\Re(x[n]) = \\cos(\\omega_0 n) = \\frac{1}{2}\\left( e^{j\\omega_0 n} + e^{-j\\omega_0 n} \\right)\\] \\[\\Im(x[n]) = \\sin(\\omega_0 n) = \\frac{1}{2j}\\left( e^{j\\omega_0 n} - e^{-j\\omega_0 n} \\right)\\]\nThe energy and power are the same as for the CT complex sinusoid: \\(E_x = \\infty\\) and \\(P_x = 1\\).\n\n\n3.5.3 DT Complex Exponential: sinusoidal case with phase shift\nThe general DT sinusoid is\n\\[x[n] = A\\cos(\\omega_0 n + \\phi)\\]\n\n\\(A\\) is called the amplitude\n\\(\\phi\\) is called the phase shift\n\\(\\omega_0\\) is now in radians (assuming \\(n\\) is dimensionless)\n\n\n\n\n\n\n\n\n\n\nFor CT sinusoids as \\(\\omega_0\\) increases the signal oscillates faster and faster. However for DT sinusoids there is a \"fastest\" oscillation.\n\\[e^{j\\omega_0 n}\\rvert_{\\omega_0 = \\pi} = e^{j\\pi n} = (-1)^n\\]\n\n\n\n\n\n\n\n\n\n\n\n3.5.4 Properties of DT complex sinusoid\nIf we consider two frequencies: \\(\\omega_0\\) and \\(\\omega_0+2\\pi\\). In the first case: \\[x[n] = e^{j\\omega_0 n}\\] In the second case: \\[\\begin{array}{ll}\nx[n] &= e^{j(\\omega_0+2\\pi) n} \\\\\n&= \\underbrace{e^{j2\\pi n}}_{\\text{always 1}}\\; e^{j\\omega_0 n} \\\\\n&= e^{j\\omega_0 n}\n\\end{array}\\]\nThus the two are the same signal. This has important implications later in the course.\nAnother difference between CT and DT complex sinusoids is periodicity. Recall for a DT signal to be periodic with period \\(N\\) \\[x[n] = x[n+N] \\; \\forall n\\] Substituting the complex sinusoid \\[e^{j\\omega_0 n} = e^{j\\omega_0 (n+N)} = e^{j\\omega_0 n}e^{j\\omega_0 N}\\] requires \\(e^{j\\omega_0 N} = 1\\), which implies \\(\\omega_0 N\\) is a multiple of \\(2\\pi\\): \\[\\omega_0 N = 2\\pi m \\;\\;\\; m = \\pm 1, \\pm 2, \\cdots\\] or equivalently \\[\\frac{|\\omega_0|}{2\\pi} = \\frac{m}{N}\\] thus \\(\\omega_0\\) must be a rational multiple of \\(\\pi\\).\nTwo DT complex sinusoids are harmonics of one another is both are periodic in \\(N\\), i.e when\n\\[x_k(t) = e^{jk\\frac{2\\pi}{N} n} \\; \\text{for} \\; k = 0, \\pm 1, \\pm 2, \\cdots\\]\nThis implies there are only \\(N\\) distinct harmonics in DT.\n\n\n3.5.5 DT Complex Exponential: general case\nIn the general case we get a sinusoid signal modulated by an exponential. Let \\(C = Ae^{j\\phi}\\) and \\(\\beta = r + j\\omega_0\\), then \\[x[n] = C e^{\\beta n} =  Ae^{j\\phi} e^{(r+j\\omega_0)n}\\] Expanding the terms and using Euler’s identity gives:\n\\[x[n] = \\underbrace{Ae^{rn}\\cos(\\omega_0 n+\\phi)}_{\\Re \\text{part}} + j \\underbrace{Ae^{rn}\\sin(\\omega_0 n+\\phi)}_{\\Im \\text{part}}\\] Each part is a real sinusoid whose amplitude is modulated by a real exponential.\nThe visualization of the general case is to view the signal \\(x[n]\\) as a vector rotating through fixed angles in the complex plane.\n\n\n\n\n\n\nFigure 3.3: The DT complex sinusoid at a specific point in time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete-Time Signals</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html",
    "href": "04-ct-lccde.html",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "",
    "text": "4.1 Solving Linear, Constant Coefficient Differential Equations\nRecall a system is a transformation of signals, turning the input signal into the output signal. While this might seem like a new concept to you, you already know something about them from your differential equations course, i.e. MATH 2214 and your circuits course.\nFor example, consider the following circuit:\nwhere the switch moves position at \\(t = 0\\). The governing equation for the circuit when \\(t &lt; 0\\) is \\[\\frac{dV_c}{dt}(t) + \\frac{1}{RC}V_c(t) = 0\\] a homogeneous differential equation of first-order. From a DC analysis, the initial condition on the capacitor voltage is \\(V_C(0^-) = 0\\), so there is no current flowing prior to \\(t = 0\\) and the solution is \\(V_C(t) = 0\\) for \\(t &lt; 0\\).\nAfter the switch is thrown, the governing equation for the circuit when \\(t \\geq 0\\) is \\[\\frac{dV_c}{dt}(t) + \\frac{1}{RC}V_c(t) = \\frac{1}{RC}\\] Since the voltage across the capacitor cannot change instantaneously \\(V_C(0^-) = V_C(0^+) = 0\\), giving the auxillary condition necessary to solve this equation, which has the form \\[V_C(t) = A + Be^{-\\frac{1}{RC}t}\\] Using the auxillary condition we find \\[V_C(0) = A + Be^{-\\frac{1}{RC}0} = A + B = 0 \\mbox{ which implies } B = -A\\] Subsitution back into the differential equation and equating the coefficients gives \\(A = 1\\). Thus the voltage for \\(t \\geq 0\\) is \\[V_C(t) = 1 - e^{-\\frac{1}{RC}t}\\]\nSuppose we consider the voltage after the switch as the input signal \\(x(t)\\) to the system composed of the series RC. As we have seen previously a mathematical model of the switch is the unit step \\(x(t) = u(t)\\). Suppose we consider the capacitor voltage at the output of the system, so that \\(y(t) = V_C(t)\\). Then we can consider the system to be represented by the linear, constant-coefficient differential equation \\[\\frac{dy}{dt}(t) + \\frac{1}{RC}y(t) = \\frac{1}{RC}x(t)\\] where \\(x(t) = u(t)\\) and the solution \\(y(t)\\) is the step response \\[y(t) = \\left(1 - e^{-\\frac{1}{RC}t}\\right)u(t)\\]\nAs we will see later this representation of systems is central to the course, so we take some time here to review the solution of such equations.\nA linear, constant coefficient (LCC) differential equation is of the form \\[a_0\\, y + a_1\\, \\frac{dy}{dt} + a_2\\, \\frac{d^2y}{dt^2} + \\cdots + a_N\\, \\frac{d^Ny}{dt^N}  = b_0\\, x + b_1\\, \\frac{dx}{dt} + b_2\\, \\frac{d^2x}{dt^2} + \\cdots + b_M\\, \\frac{d^Mx}{dt^M}\\] which can be written compactly as \\[\\sum\\limits_{k = 0}^{N} a_k\\, \\frac{d^ky}{dt^k} = \\sum\\limits_{k = 0}^{M} b_k\\, \\frac{d^kx}{dt^k}\\]\nIt is helpful to clean up this notation using the derivative operator \\(D^n = \\frac{d^n}{dt^n}\\). For example \\(D^2y = \\frac{d^2y}{dt^2}\\) and \\(D^0 y= y\\). To give for form as \\[\\sum\\limits_{k = 0}^{N} a_k\\, D^k y = \\sum\\limits_{k = 0}^{M} b_k\\, D^k x\\]\nWe can factor out the derivative operators \\[a_0y + a_1Dy + a_2D^2y + \\cdots + a_ND^Ny  = b_0\\, x + b_1\\, Dx + b_2\\, D^2x + \\cdots + b_M\\, D^M x\\] \\[\\underbrace{\\left(a_0 + a_1D + a_2D^2 + \\cdots + a_ND^N\\right)}_{\\text{Polynomial in } D, Q(D)} y = \\underbrace{\\left(b_0 + b_1 D + b_2 D^2 + \\cdots + b_M D^M\\right)}_{\\text{Polynomial in } D, P(D)} x\\] to give:\n\\[Q(D)y = P(D)x\\] You learned how to solve these in differential equations (Math 2214) as \\[y(t) = y_\\text{h}(t) + y_\\text{p}(t)\\]\nThe term \\(y_\\text{h}(t)\\) is the solution of the homogeneous equation \\[Q(D)y = 0\\] Given the \\(N-1\\) auxillary conditions \\(y(t_0) = y_0\\), \\(Dy(t_0) = y_1\\), \\(D^2y(t_0) = y_2\\), up to \\(D^{N-1}y(t_0) = y_{N-1}\\).\nThe term \\(y_\\text{p}(t)\\) is the solution of the particular equation \\[Q(D)y = P(D)x\\] for a given \\(x(t)\\).\nRather than recapitulate the solution to \\(y_\\text{h}(t)\\) and \\(y_\\text{p}(t)\\) in the general case we focus on the homogeneous solution \\(y_\\text{h}(t)\\) only. The reason is that we will use the homogeneous solution to find the impulse response below and take a different approach to solving the general case for an arbitrary input using the impulse response and convolution (next week).\nTo solve the homogenous system:\nStep 1: Find the characteristic equation by replacing the derivative operators by powers of an arbitrary complex variable \\(s\\). \\[Q(D) = a_0 + a_1D + a_2D^2 + \\cdots + a_ND^N\\] becomes \\[Q(s) = a_0 + a_1s + a_2s^2 + \\cdots + a_Ns^N\\] a polynomial in \\(s\\) with \\(N\\) roots \\(s_i\\) for \\(i = 1, 2, \\cdots, N\\) such that \\[(s - s_1)(s-s_2)\\cdots(s-s_N) = 0\\]\nStep 2: Select the form of the solution, a sum of terms corresponding to the roots of the characteristic equation.\nStep 3: Solve for the unknown constants in the solution using the auxillary conditions.\nWe now examine two common special cases, when \\(N=1\\) (first-order) and when \\(N=2\\) (second-order).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html#solving-linear-constant-coefficient-differential-equations",
    "href": "04-ct-lccde.html#solving-linear-constant-coefficient-differential-equations",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "",
    "text": "For a real root \\(s_1\\in \\mathbb{R}\\) the term is of the form \\[C_1 e^{s_1 t}.\\]\nFor a pair of complex roots (they will always be in pairs) \\(s_{1,2} = a \\pm jb\\) the term is of the form \\[C_1 e^{s_1 t} + C_2 e^{s_2 t} = e^{a t}\\left(C_3\\cos(bt) + C_4\\sin(bt)\\right) = C_5 e^{a t}\\cos(bt + C_6).\\]\nFor a repeated root \\(s_1\\), repeated \\(r\\) times, the term is of the form \\[e^{s_1 t} (C_0 + C_1 t + \\cdots + C_{r-1} t^{r-1}).\\]\n\n\n\n\n4.1.1 First-Order Homogeneous LCCDE\nConsider the first order homogeneous differential equation \\[\\frac{dy}{dt}(t) + ay(t) = 0 \\mbox{ for } a \\in \\mathbb{R}\\] The characteristic equation is given by \\[s + a = 0\\] which has a single root \\(s_1 = -a\\). The solution is of the form \\[y(t) = Ce^{s_1 t} = Ce^{-a t}\\] where the constant \\(C\\) is found using the auxillary condition \\(y(t_0) = y_0\\).\n\n\nExample\n\n\nConsider the homogeneous equation \\[\\frac{dy}{dt}(t) + 3y(t) = 0 \\mbox{ where } y(0) = 10\\] The solution is \\[y(t) = Ce^{-3 t}\\] To find \\(C\\) we use the auxillary condition \\[y(0) = Ce^{-3 \\cdot 0} = C = 10\\] and the final solution is \\[y(t) = 10e^{-3 t}\\]\n\n\n\n\n4.1.2 Second-Order Homogeneous LCCDE\nConsider the second-order homogeneous differential equation \\[\\frac{d^2y}{dt^2}(t) + a\\frac{dy}{dt}(t) + by(t) = 0 \\mbox{ for } a,b \\in \\mathbb{R}\\] The characteristic equation is given by \\[s^2 + as + b = 0\\]\nLet’s look at several examples to illustrate the functional forms.\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 7\\frac{dy}{dt}(t) + 10y(t) = 0\\] The characteristic equation is given by \\[s^2 + 7s + 10 = 0\\] which has roots \\(s_1 = -2\\) and \\(s_2 = -5\\). Thus the form of the solution is \\[y(t) = C_1e^{-2t} + C_2e^{-5t}\\]\n\n\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 2\\frac{dy}{dt}(t) + 5y(t) = 0\\] The characteristic equation is given by \\[s^2 + 2s + 5 = 0\\] which has complex roots \\(s_1 = -1+j2\\) and \\(s_1 = -1-j2\\). Thus the form of the solution is \\[y(t) = e^{-t}\\left(C_1\\cos(2t) + C_2\\sin(2t)\\right)\\]\n\n\n\n\nExample\n\n\n\\[\\frac{d^2y}{dt^2}(t) + 2\\frac{dy}{dt}(t) + y(t) = 0\\] The characteristic equation is given by \\[s^2 + 2s + 1 = 0\\] which has a root \\(s_1 = -1\\) repeated \\(r=2\\) times. Thus the form of the solution is \\[y(t) = e^{-t}\\left(C_1 + C_2t\\right)\\]\n\n\nIn each of the above cases the constants, \\(C_1\\) and \\(C_2\\), are found using the auxillary conditions \\(y(t_0)\\) and \\(y\\prime(t_0)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "04-ct-lccde.html#finding-the-impulse-response-of-a-system-described-by-a-lccde",
    "href": "04-ct-lccde.html#finding-the-impulse-response-of-a-system-described-by-a-lccde",
    "title": "4  CT Systems as Linear Constant Coefficient Differential Equations",
    "section": "4.2 Finding the impulse response of a system described by a LCCDE",
    "text": "4.2 Finding the impulse response of a system described by a LCCDE\nAs we will see next week an important response of a system is the one that corresponds to an impulse input, i.e. the impulse response \\(y(t) = h(t)\\) when \\(x(t) = \\delta(t)\\). Thus we focus here on a recipe for solving LCCDEs for this special case when \\(M \\leq N\\). We will skip the derivation of why this works.\nOur goal is to find the solution to \\(Q(D)y = P(D)x\\) when \\(x(t)=\\delta(t)\\).\nStep 1: Rearrange the LCCDE so that \\(a_N = 1\\), i.e. divide through by \\(a_N\\) to put it into a standard form.\nStep 2: Let \\(y_h(t)\\) be the homogeneous solution to \\(Q(D)y_h = 0\\) for auxillary conditions \\[D^{N-1}y_h(0^+) = 1 \\; , \\; D^{N-2}y_h(0^+) = 0 \\; , \\; \\text{etc.} \\; y_h(0^+) = 0\\]\nStep 3: Assume a form for \\(h(t)\\) given by: \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0 \\text{ unless } N=M} + \\underbrace{\\left[ P(D)y_h\\right]}_{\\text{apply } P(D) \\text{ to } y_n(t)}u(t)\\]\nRecall from above the homogeneous solution depends on the roots of the characteristic equation \\(Q(D) = 0\\).\n\nroots are either real, or\nroots occur in complex conjugate pairs, or\nrepeated roots.\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[2\\frac{dy}{dt}(t) + 2y(t) = 2x(t)\\] In the standard for the LCCDE is \\[\\frac{dy}{dt}(t) + y(t) = x(t)\\] The characteristic equation is given by \\[s + 1 = 0\\] which has a single root \\(s_1 = -1\\). The solution is of the form \\[y_h(t) = Ce^{-t}\\] with the special auxillary condition \\(y(0) = 1\\), so that \\[y_h(t) = e^{-t}\\] Since \\(P(D) = 1\\) and \\(N = 1 \\neq M = 0\\) the impulse response is \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0} + \\left[ \\underbrace{P(D)}_{1}y_h(t)\\right]u(t) = e^{-t}u(t)\\]\n\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[\\frac{dy}{dt}(t) + y(t) = \\frac{dx}{dt}(t) + x(t)\\] It is already in the standard form. The homogeneous solution is the same as in Example 1, \\[y_h(t) = e^{-t}\\] however now \\(M = N = 1\\) with \\(b_1 = 1\\) and \\(P(D) = D+1\\). Thus, the impulse response is \\[h(t) = \\underbrace{b_N}_{=1}\\delta(t) + \\left[ \\underbrace{P(D)}_{D+1}y_h(t)\\right]u(t) = \\delta(t) + \\left\\{[D+1]e^{-t}\\right\\}u(t) = \\delta(t) + [- e^{-t} + e^{-t}]u(t) = \\delta(t)\\]\n\n\n\n\nExample\n\n\nFind the impulse response of the LCCDE \\[\\frac{d^2y}{dt^2}(t) + 7\\frac{dy}{dt}y(t) + 10y(t) = x(t)\\] It is already in the standard form. The characteristic equation is given by \\[s^2 + 7s + 10 = 0\\] which has roots \\(s_1 = -2\\) and \\(s_2 = -5\\). Thus the form of the solution is \\[y_h(t) = C_1e^{-2t} + C_2e^{-5t}\\] The special auxillary conditions are \\(y_h(0) = 0\\) and \\(y^\\prime_h(0) = 1\\). Using these conditions \\[y_h(0) = C_1e^{-2t} + C_2e^{-5t} |_{t = 0} = C_1 + C_2 = 0\\] \\[y^\\prime_h(0) = -2C_1e^{-2t} - 5C_2e^{-5t} |_{t = 0} = -2C_1 -5C_2 = 1\\] Solving for the constants gives \\(C_1 = \\frac{1}{3}\\) and \\(C_2 = -\\frac{1}{3}\\). Since \\(P(D) = 1\\) and \\(N = 2 \\neq M = 0\\) the impulse response is \\[h(t) = \\underbrace{b_N\\delta(t)}_{=0} + \\left[ \\underbrace{P(D)}_{1}y_h(t)\\right]u(t) = \\frac{1}{3} e^{-2t}u(t) - \\frac{1}{3} e^{-5t}u(t)\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CT Systems as Linear Constant Coefficient Differential Equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html",
    "href": "05-dt-lccde.html",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "",
    "text": "5.1 Definition of linear constant coefficient difference equation\nA difference equation is a relation among combinations of two DT functions and shifted versions of them. Similar to differential equations where the solution is a CT function, the solution to a difference equation is a DT function. For example: \\[y[n+1] + \\frac{1}{2}y[n] = x[n]\\] is a first order, linear, constant-coefficient difference equation. Given \\(x[n]\\) the solution is a function \\(y[n]\\). We can view this as a representation of a DT system, where \\(x[n]\\) is the input signal and \\(y[n]\\) is the output.\nThere is a parallel theory to differential equations for solving difference equations. However in this lecture we will focus specifically on the iterative solution of linear, constant-coefficient difference equations and the case when the input is a delta function, as this is all we need for this course.\nA linear, constant-coefficient, difference equation (LCCDE) comes in one of two forms.\nThe order of the system is given by \\(N\\). The delay and advance forms are equivalent because the equation holds for any \\(n\\), and we can move back and forth between them as needed by a constant index-shift.\nIt will be convenient to define the operator \\(E^m\\) as shifting a DT function by positive \\(m\\), i.e. \\(E^m x[n] = x[n+m]\\), and the operator \\(D^m\\) as shifting a DT function by negative \\(m\\), i.e. \\(D^m x[n] = x[n-m]\\). These are called the advance and delay operators respectively. Then, the advance form of the difference equation using this operator notation is \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\] \\[a_0 E^Ny + a_1E^{N-1}y + \\cdots a_N y = b_0 E^{N}x + \\cdots b_M E^{N-M}x\\] Factoring out the advance operators gives \\[\\underbrace{\\left(a_0E^N + a_1E^{N-1} + \\cdots a_N\\right)}_{Q(E)} y = \\underbrace{\\left(b_0 E^{N} + \\cdots b_M E^{N-M}\\right)}_{P(E)} x\\] or \\[Q(E)y[n] = P(E)x[n]\\]\nSimilarly, the delay form of the difference equation using this operator notation is \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] \\[a_0y[n] + a_1 Dy + \\cdots a_N D^N y = b_0 x + \\cdots b_MD^M x\\] Note: The DT delay operator \\(D\\) is similar, but not identical to the derivative operator \\(D\\) in CT.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#definition-of-linear-constant-coefficient-difference-equation",
    "href": "05-dt-lccde.html#definition-of-linear-constant-coefficient-difference-equation",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "",
    "text": "Delay form. \\[\\sum\\limits_{k = 0}^N a_k y[n-k] = \\sum\\limits_{k = 0}^M b_k x[n-k]\\] or \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\]\nAdvance form. Let \\(n\\rightarrow n+N\\), then the delay form becomes \\[\\sum\\limits_{k = 0}^N a_k y[n+N-k] = \\sum\\limits_{k = 0}^M b_k x[n+N-k]\\] or \\[a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = b_0 x[n+N] + \\cdots b_Mx[n+N-M]\\]\n\n\n\n\nExample\n\n\nThe delay form is \\[a_0y[n] + a_1 y[n-1] + a_2 y[n-2] = b_0 x[n] + b_1 x[n-1]\\] Replacing \\(n \\rightarrow n+2\\), the advance form is \\[a_0 y[n+2] + a_1 y[n+1] + a_2 y[n] = b_0 x[n+2] + b_1 x[n+1]\\]\n\n\n\n\n\n\nExample\n\n\nConsider the difference equation \\[3y[n+1] + 4y[n] + 5y[n-1] = 2x[n+1]\\] The advance form would be: \\[3y[n+2] + 4y[n+1] + 5y[n] = 2x[n+2]\\] or using the advance operator \\[\\left(3E^2 + 4E + 5\\right)y = 2E^2x\\] with \\(Q(E) = 3E^2 + 4E + 5\\) and \\(P(E) = 2E^2\\).\nThe delay form would be: \\[3y[n] + 4y[n-1] + 5y[n-2] = 2x[n]\\] or using the delay operator \\[\\left(5D^2 + 4D + 3\\right)y = 2x\\] with \\(Q(D) = 5D^2 + 4D + 3\\) and \\(P(D) = 2\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#iterative-solution-of-lccdes",
    "href": "05-dt-lccde.html#iterative-solution-of-lccdes",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.2 Iterative solution of LCCDEs",
    "text": "5.2 Iterative solution of LCCDEs\nDifference equations are different (pun!) from differential equations in that they can be solved by manually running the equation forward using previous values of the output and current and previous values of the input, given some initial conditions. This is called an iterative solution for this reason.\nTo perform an iterative solution we need the difference equation in delay form \\[a_0y[n] + a_1y[n-1] + \\cdots a_N y[n-N] = b_0 x[n] + \\cdots b_Mx[n-M]\\] We then solve for the current output \\(y[n]\\) \\[y[n] =  - \\left(\\frac{a_1}{a_0}y[n-1] + \\cdots \\frac{a_N}{a_0} y[n-N]\\right) + \\frac{b_0}{a_0} x[n] + \\cdots \\frac{b_M}{a_0}x[n-M]\\]\nNow lets examine what this expression says in words. To compute the current output \\(y[n]\\) we need the value of the previous \\(N-1\\) outputs, the value of the current input \\(x[n]\\) and \\(M-1\\) previous inputs (and the coefficients). Then we can compute the next output \\(y[n+1]\\) by adding the previous computation result for \\(y[n]\\) to our list of things to remember, and forgetting one previous value of \\(y\\). This can continue as long as we like.\n\n\nExample\n\n\nConsider the first-order difference equation \\[y[n+1] + y[n] = x[n+1]\\] where \\(y[-1] = 1\\) and \\(x[n] = u[n]\\). We first convert this to delay form \\[y[n] = -y[n-1] + x[n]\\; .\\] Then we can compute \\(y[0]\\) as \\[y[0] = -y[-1] + x[0] = -1 + 1 = 0\\] and continuing\n\\[\\begin{align*}\n  y[1] &= -y[0] + x[1] = 0 + 1 = 1\\\\\n  y[2] &= -y[1] + x[2] = -1 + 1 = 0\\\\\n  y[3] &= -y[2] + x[3] = 0 + 1 = 1\\\\\n  \\mbox{etc.} &\n\\end{align*}\\]\nWe can see that this will continue to give the alternating sequence \\(1,0,1,0,1,\\cdots\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#solution-of-the-homogeneous-lccde",
    "href": "05-dt-lccde.html#solution-of-the-homogeneous-lccde",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.3 Solution of the homogeneous LCCDE",
    "text": "5.3 Solution of the homogeneous LCCDE\nNote the iterative solution does not give us (directly) and analytical expression for the output at arbitrary \\(n\\). We have to start at the initial conditions and compute our way up to \\(n\\). We now consider an analytical solution when the input is zero, the solution to the homogeneous difference equation \\[Q(E)\\, y = a_0y[n+N] + a_1y[n+N-1] + \\cdots a_N y[n] = 0 \\; .\\] given \\(N\\) sequential auxiliary conditions on \\(y\\).\nSimilar to differential equations, the homogeneous solution depends on the roots of the characteristic equation \\(Q(E)=0\\) whose roots are either real or occur in complex conjugate pairs. Let \\(\\lambda_i\\) be the \\(i\\)-th root of \\(Q(E) = 0\\), then the solution is of the form \\[y[n] = \\sum\\limits_{i=1}^N C_i \\lambda_i^{n}\\] where the parameters \\(C_i\\) are determined from the auxiliary conditions.\nFor a real system (when the coefficients of the difference equation are real) and when the roots are complex \\(\\lambda_{1,2} = |\\lambda|e^{\\pm j\\beta}\\), it is cleaner to assume a form for those terms as \\[y[n] = C |\\lambda|^n\\cos(\\beta n + \\theta)\\] for constants \\(C\\) and \\(\\theta\\).\n\n\nExample\n\n\nFind the solution to the first-order homogeneous LCCDE \\[y[n+1] + \\frac{1}{2}y[n] = 0 \\mbox{ with } y[0] = 5 \\; .\\] Note \\(Q(E) = E + \\frac{1}{2}\\) has a single root \\(\\lambda_1 = -\\frac{1}{2}\\). Thus the solution is of the form \\[y[n] = C\\left( -\\frac{1}{2}\\right)^n\\] where the parameter \\(C\\) is found using \\[y[0] = C = 5\\] to give the final solution \\[y[n] = 5\\left( -\\frac{1}{2}\\right)^n\\]\n\n\n\n\nExample\n\n\nFind the solution to the second-order homogeneous LCCDE \\[y[n+2] + y[n+1] + \\frac{1}{2}y[n] = 0 \\mbox{ with } y[0] = 1 \\mbox{ and } y[1] = 0\\; .\\] Note \\(Q(E) = E^2 + E + \\frac{1}{2}\\) has a pair of complex roots \\(\\lambda_{1,2} = -\\frac{1}{2} \\pm j\\frac{1}{2}\\). Thus the solution is of the form \\[y[n] = C \\left|\\frac{1}{\\sqrt{2}}\\right|^n\\cos\\left(\\frac{3\\pi}{4} n + \\theta\\right)\\] where the parameters are found using \\[y[0] = C\\cos\\left(\\theta\\right) = 1\\] \\[y[1] = C\\frac{1}{\\sqrt{2}}\\cos\\left(\\frac{3\\pi}{4} + \\theta\\right) = 0\\] This is true when \\[C = \\sqrt{2} \\mbox{ and } \\theta = -\\frac{\\pi}{4} + 2\\pi m\\] for any \\(m\\in \\mathbb{Z}\\) since \\(\\cos\\) is periodic in \\(2\\pi\\). A final solution is then \\[y[n] = \\sqrt{2} \\left|\\frac{1}{\\sqrt{2}}\\right|^n\\cos\\left(\\frac{3\\pi}{4} n - \\frac{\\pi}{4}\\right)\\]\n\n\nSee the appendix for a general technique to solve for these constants.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "05-dt-lccde.html#impulse-response-from-lccde",
    "href": "05-dt-lccde.html#impulse-response-from-lccde",
    "title": "5  DT systems as linear constant coefficient difference equations",
    "section": "5.4 Impulse response from LCCDE",
    "text": "5.4 Impulse response from LCCDE\nToday our goal is to find the solution to \\(Q(E)y=P(E)x\\) when \\(x[n] = \\delta[n]\\) assuming \\(y[n] = 0\\) for \\(n &lt; 0\\), giving the impulse response \\(y[n] = h[n]\\). We skip the derivation here and just give a procedure.\nStep 1: Let \\(y_h\\) be the homogeneous solution to \\(Q(E)y_h=0\\) for \\(n &gt; N\\).\nStep 2: Assume a form for \\(h[n]\\) given by \\[h[n] = \\frac{b_N}{a_N}\\delta[n] + y_h[n]u[n]\\]\nStep 3: Using the iterative procedure above find the \\(N\\) auxiliary conditions we need by,\n\nfirst, rewrite the equation in delay form and solve for \\(y[n]\\),\nthen let \\(x[n] = \\delta[n]\\) and manually compute \\(h[0]\\) assuming \\(h[n] = 0\\) for \\(n &lt; 0\\),\nrepeating the previous step for \\(h[1]\\), continuing up to \\(h[N-1]\\).\n\nStep 4: Using the auxillary conditions in step 3, solve for the constants in the solution \\(h[n]\\) from step 2.\n\n\nExample\n\n\nFind the impulse response of the system given by \\[y[n+2] -\\frac{1}{4}y[n+1] -\\frac{1}{8}y[n]= 2x[n+1]\\]\nFor step 1 we solve the equation \\[y_h[n+2] -\\frac{1}{4}y_h[n+1] -\\frac{1}{8}y_h[n] = 0\\] which is of the form \\[y_h[n] = C_1 \\left( -\\frac{1}{4}\\right)^n + C_2 \\left( \\frac{1}{2}\\right)^n\\] since the roots of \\(Q(E) = E^2 - \\frac{1}{4}E - \\frac{1}{8}\\) are \\(-\\frac{1}{4}\\) and \\(\\frac{1}{2}\\).\nFor step 3, we find the auxiliary conditions needed to find \\(C_1\\) and \\(C_2\\) by rewriting the original equation in delay form and solving for \\(y[0]\\) and \\(y[1]\\) when \\(x[n] = \\delta[n]\\). \\[y[n] = \\frac{1}{4}y[n-1] + \\frac{1}{8}y[n-2] + 2x[n-1]\\] Let \\(x[n] = \\delta[n]\\) and manually compute \\(y[0]\\) assuming \\(y[n] = 0\\) for \\(n &lt; 0\\) \\[y[0] = \\frac{1}{4}\\underbrace{y[0-1]}_{0} + \\frac{1}{8}\\underbrace{y[0-2]}_{0} + 2\\underbrace{\\delta[0-1]}_{0} = 0\\] Repeat for \\(y[1]\\) \\[y[1] = \\frac{1}{4}\\underbrace{y[1-1]}_{0} + \\frac{1}{8}\\underbrace{y[1-2]}_{0} + 2\\underbrace{\\delta[1-1]}_{1} = 2\\] Now we find the constants using step 4 \\[h[0] = C_1  + C_2  = 0\\] \\[h[1] = C_1 \\left( -\\frac{1}{4}\\right) + C_2 \\left( \\frac{1}{2}\\right) = 2\\] which gives \\(C_1 = -\\frac{8}{3}\\) and \\(C_2 = \\frac{8}{3}\\). Thus the final impulse response is \\[h[n] = \\frac{b_N}{a_N}\\delta[n] + y_h[n]u[n] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^nu[n] + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^n u[n]\\] since \\(b_N = 0\\).\n\n\nNote we can confirm our closed-form result in the previous example, for a few values of \\(n\\), by iteratively solving the difference equation \\[h[0] = \\frac{1}{4}\\underbrace{h[0-1]}_{0} + \\frac{1}{8}\\underbrace{h[0-2]}_{0} + 2\\underbrace{\\delta[0-1]}_{0} = 0\\] \\[h[1] = \\frac{1}{4}\\underbrace{h[1-1]}_{0} + \\frac{1}{8}\\underbrace{h[1-2]}_{0} + 2\\underbrace{\\delta[1-1]}_{1} = 2\\] \\[h[2] = \\frac{1}{4}\\underbrace{h[2-1]}_{2} + \\frac{1}{8}\\underbrace{h[2-2]}_{0} + 2\\underbrace{\\delta[2-1]}_{0} = \\frac{1}{2}\\] \\[h[3] = \\frac{1}{4}\\underbrace{h[3-1]}_{\\frac{1}{2}} + \\frac{1}{8}\\underbrace{h[3-2]}_{2} + 2\\underbrace{\\delta[2-1]}_{0} = \\frac{3}{8}\\] and comparing to our closed-form solution a the same values of \\(n\\) \\[h[0] = -\\frac{8}{3} + \\frac{8}{3} = 0\\] \\[h[1] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right) + \\frac{8}{3}\\left( \\frac{1}{2}\\right) = 2\\] \\[h[2] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^2 + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^2 = \\frac{1}{2}\\] \\[h[3] = -\\frac{8}{3}\\left( -\\frac{1}{4}\\right)^3 + \\frac{8}{3}\\left( \\frac{1}{2}\\right)^3 = \\frac{3}{8}\\]\n\n\nExample\n\n\nFind the impulse response of the system given by \\[y[n+1] - \\frac{1}{2}y[n] = x[n+1] + x[n]\\]\nIn step 1 we note the solution to \\(Q(E)y[n] = 0\\) is of the form \\[y_h[n] = C\\left( \\frac{1}{2}\\right)^n\\] From step 2 we note \\(b_N = 1\\) and \\(a_N = -\\frac{1}{2}\\), so that \\[h[n] = -2\\delta[n]  +  C\\left( \\frac{1}{2}\\right)^n\\, u[n]\\] In step 3 we manually find \\(h[0]\\)\n\\[\\begin{align*}\n    y[n] &= \\frac{1}{2}y[n-1] + x[n] + x[n-1]\\\\\n    h[n] &= \\frac{1}{2}y[n-1] + \\delta[n] + \\delta[n-1]\\\\\n    h[0] &= 0 + 1 + 0 = 1  \n\\end{align*}\\]\nAnd in step 4 we solve for \\(C\\) \\[h[0] = -2  +  C = 1 \\mbox{ implies } C = 3\\] to give \\[h[n] = -2\\delta[n]  +  3\\left( \\frac{1}{2}\\right)^n\\, u[n]\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DT systems as linear constant coefficient difference equations</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html",
    "href": "06-ct-lti.html",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "",
    "text": "6.1 System types\nToday’s topic is our introduction to CT systems and the important case of CT Linear, Time-Invariant Systems.\nA system is an interconncted set of components or sub-systems. Mathematically a system is a transformation, \\(T\\), between one or more signals, a rule that maps functions to functions.\nWe will focus on single input - single output, CT and DT systems.\nAs a shorthand notation for the graphical description above we can use \\(x \\mapsto y\\). A system maps a function \\(x\\) to a function \\(y\\):\nWhen a system has no input, the system is autonomous. An autonomous system just produces output: \\(\\mapsto y\\).\nWe can think of an autonomous system as a function generator, producing signals for use, or as modeling a measurement process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#system-types",
    "href": "06-ct-lti.html#system-types",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "",
    "text": "single input - single output (SISO) system.\n\n\n\nSISO Block Diagram\n\n\nsingle input - multiple output (SIMO) system\n\n\n\nSIMO Block Diagram\n\n\ngeneral case, multiple input - multiple output (MIMO)\n\n\n\nMIMO Block Diagram\n\n\n\n\n\nIf both input and output are CT signals, it is a CT system.\n\n\n\nGeneric Block Diagram of CT System\n\n\nIf both input and output are DT signals, it is a DT system.\n\n\n\nGeneric Block Diagram of a DT System\n\n\nIf input and output are not both CT or DT signals, it is a hybrid CT-DT system.\n\n\n\nGeneric Block Diagram of a Hybrid DT/CT System\n\n\n\n\n\nGeneric Block Diagram of a Hybrid CT/DT System\n\n\n\n\n\nCT system \\[x(t) \\mapsto y(t)\\]\nDT system \\[x[n] \\mapsto y[n]\\]\nHybrid CT-DT system \\[x[n] \\mapsto y(t)\\]\nor\n\\[x(t) \\mapsto y[n]\\]\n\n\n\n\n\nGeneric Block Diagram of an Autonomous System",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#ct-system-representations",
    "href": "06-ct-lti.html#ct-system-representations",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.2 CT system representations",
    "text": "6.2 CT system representations\nWe can mathematically represent, or model, systems multiple ways.\n\npurely mathematically - in time domain we will use\n\nfor CT systems: linear, constant coefficient differential equations. e.g. \\[y^{\\prime\\prime} + ay^\\prime + by = x\\]\nfor DT systems: linear, constant coefficient difference equation, e.g. \\[y[n] = a y[n-1] + b y[n-2] + x[n]\\]\n\nor\n\nfor CT systems: CT impulse response\nfor DT systems: DT impulse response\n\npurely mathematically - in frequency domain we will use\n\nfrequency response\ntransfer function (complex frequency, covered in ECE 3704)\n\ngraphically, using a mixture of math and block diagrams\n\nMathematical models:\n\nprovide abstraction, removing (often) irrelevant detail.\ncan be more or less detailed, an internal v.s. external (block box) description\nare not unique with respect to instantiation (implementation)\nare limited to the regime they were designed for\n\n\n\nExample\n\n\nConsider the RC circuit. It is a single input - single output system. We will be able to represent it mathematically or graphically and internally or externally.\n\nExternal - Graphical\n\n\n\nExternal Model\n\n\nExternal - Symbolic\n\\(y(t) = h(t)*x(t)\\)\nInternal - Graphical\n\n\n\nInternal Model\n\n\nInternal - Symbolic\n\\(y^\\prime + \\frac{1}{RC} y = \\frac{1}{RC} x(t)\\)\n\n\n\nNote: internal models usually have several paramters (the resistor and capacitor values in the example above), while the external model does not. Thus another term for external model is a lumped parameter model.\nIt does not matter what the underlying system implementation is. For example, consider a mechanical system, described by a second-order ODE:\n\n\n\nMechanical Diagram\n\n\n\n\n\n\\(y\\) = position\n\\(M\\) = mass\n\n\n\\(y^\\prime\\) = velocity\n\\(K\\) = spring constant\n\n\n\\(y^{\\prime\\prime}\\) = acceleration\n\\(B\\) = coefficient of friction\n\n\n\n\\[y^{\\prime\\prime} + \\frac{B}{M} y^\\prime + \\frac{K}{M}y = \\frac{1}{M}f(t)\\]\nCompare this to the parallel RLC circuit, described by the second-order ODE:\n\n\n\nCircuit Diagram\n\n\n\n\n\n\\(y\\) = voltage\n\\(R\\) = resistance\n\n\n\\(Cy^\\prime\\) = capacitor current\n\\(L\\) = inductance\n\n\n\n\\(C\\) = capacitance\n\n\n\n\\[y^{\\prime\\prime} + \\frac{1}{RC} y^\\prime + \\frac{1}{LC}y = \\frac{1}{LC}f(t)\\]\nComparing these systems, if \\(R = \\frac{1}{B}\\), \\(L = \\frac{1}{K}\\), and \\(C = M\\), they are mathematically identical.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#system-properties-and-classification",
    "href": "06-ct-lti.html#system-properties-and-classification",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.3 System properties and classification",
    "text": "6.3 System properties and classification\nChoosing the right kind of system model is important. Here are some important properties that allow us to broadly classify systems.\n\nMemory\nInvertability\nCausality\nStability\nTime-invariance\nLinearity\n\nLet’s define each it turn.\n\n6.3.1 Memory\nThe output of a system with memory depends on previous or future inputs and is said to be dynamic. Otherwise the system is memoryless or instantaneous, and the output \\(y(t)\\) at time \\(t\\) depends only on \\(x(t)\\). For example in CT: \\[y(t) = 2x(t)\\] is a memoryless system, while \\[y(t) = \\int\\limits_{-\\infty}^{t} x(\\tau) \\; dt\\] has memory.\n\n\n6.3.2 Invertability\nA system is invertable if there exists a system that when placed in series with the original recovers the input. \\[x(t) \\mapsto{T} y(t) \\mapsto{T^{-1}} x(t)\\] where \\(T^{-1}\\) is the inverse system of \\(T\\). For example, consider a system \\[x(t) \\mapsto y(t) = \\int\\limits_{-\\infty}^t x(\\tau) \\; d\\tau\\] and a system \\[y(t) \\mapsto z(t) = \\frac{dy}{dt}\\] The combination in series \\(x(t) \\mapsto y(t) \\mapsto z(t) = x(t)\\), i.e. the derivative undoes the integral.\n\n\n6.3.3 Causality\nA CT system is causal if the output at time \\(t\\) depends on the input for time values at or before \\(t\\): \\[y(t) \\;\\text{depends on}\\; x(\\tau) \\;\\text{for} \\; \\tau \\leq t\\] All physical CT systems are causal, even if all continuous systems are not (e.g. continuous 2D images \\(f(u,v)\\), have no \"before\" and \"after\").\nFor example, consider a CT system whose impulse response is \\(h(t) = e^{-t^2}\\). This implies the system produces output before (i.e. for \\(t &lt; 0\\)) the impulse is applied at \\(t=0\\), somehow anticipating the arrival of the impulse. Barring time-travel, this is physically impossible.\n\n\n6.3.4 Stability\nA CT system is (BIBO) stable if applying a bounded-input (BI) \\[\\left|x(t)\\right| &lt; \\infty \\; \\forall \\; t\\] results in a bounded-output (BO) \\(x(t) \\mapsto y(t)\\) and \\[\\left|y(t)\\right| &lt; \\infty \\; \\forall \\; t\\] Note, bounded in practice is limited by the physical situation, e.g. positive and negative rails in a physical circuit.\nFor example, a CT system described by the LCCDE \\[\\frac{dy}{dt}(t) - 2y(t) = x(t)\\] is unstable because the solution \\(y(t)\\) will have one term of the form \\(Ce^{2t}\\), for most non-zero inputs \\(x(t)\\) or any non-zero initial condition, that grows unbounded as time increases.\n\n\n6.3.5 Time-invariance\nA CT system is time-invariant if, given \\[x(t) \\mapsto y(t)\\] then a time-shift of the input leads to the same time-shift in the output \\[x(t-\\tau) \\mapsto y(t-\\tau)\\]\nAn important counterexample is a CT system described by a LCCDE, e.g. \\[\\frac{dy}{dt}(t) + y(t) = x(t)\\] but non-zero auxillary conditions at some \\(t_0\\), \\(y(t_0) = y_0 \\neq 0\\). Such systems will have a term in its solution that depends on \\(y_0\\). However if I time shift the input, the term that depends on \\(y_0\\) does not shift (since it is anchored to \\(t_0\\)) and the total output does not shift identically with the input. Thus the system cannot be time-invariant.\n\n\n6.3.6 Linearity\nA CT system is linear if the output due to a sum of scaled individual inputs is the same as the scaled sum of the individual outputs with respect to those inputs. In other words given \\[x_1(t) \\mapsto y_1(t) \\;\\text{and}\\; x_2(t) \\mapsto y_2(t)\\] then \\[a x_1(t) + b x_2(t) \\mapsto a y_1(t) + b y_2(t)\\] for constants \\(a\\) and \\(b\\). Note this property extends to sums of arbitrary signals, e.g. if \\[x_i(t) \\mapsto y_i(t) \\; \\forall\\; i \\in [1 \\cdots N]\\] then given \\(N\\) constants \\(a_i\\), if the system is linear \\[\\sum\\limits_{i = 1}^N a_i x_i(t) \\mapsto \\sum\\limits_{i = 1}^N a_i y_i(t)\\] This is a very important property, called superposition, and it simplifies the analysis of systems greatly.\nSimilar to time-invariance an important non-linear system is that is described by a LCCDE with non-zero auxillary conditions at some \\(t_0\\), \\(y(t_0) = y_0\\). Again such systems will have a term in it’s solution that depends on \\(y_0\\). Given two inputs, each individual response will have that term in it, so thier sum has double that term. However the response due to the sum of the inputs would again only have one and the sum of the responses would not be the same as the response of the sum. Such a system cannot be linear.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "06-ct-lti.html#stable-lti-systems",
    "href": "06-ct-lti.html#stable-lti-systems",
    "title": "6  Linear Time Invariant CT Systems",
    "section": "6.4 Stable LTI Systems",
    "text": "6.4 Stable LTI Systems\nThe remainder of this course is about stable, linear, time-invariant (LTI) systems. As we have seen in CT such systems can be described by a LCCDE with zero auxillary (initial) conditions (the system is at rest).\nWe have seen previously how to find the impulse response, \\(h(t)\\), of such systems. We now note some relationships between the impulse response and the system properties described above.\n\nIf a system is memoryless then \\(h(t) = C \\delta(t)\\) for some constant \\(C\\).\nIf a system is causal then \\(h(t) = 0\\) for \\(t &lt; 0\\).\nIf a system is BIBO stable then \\[\\int\\limits_{-\\infty}^{\\infty} |h(t)| \\; dt &lt; \\infty\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Time Invariant CT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html",
    "href": "07-dt-lti.html",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "",
    "text": "7.1 DT system representations\nToday’s topic is our introduction to systems and the important case of DT Linear, Time-Invariant Systems.\nWe can mathematically represent, or model, DT systems multiple ways.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#dt-system-representations",
    "href": "07-dt-lti.html#dt-system-representations",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "",
    "text": "purely mathematically - in time domain we will use\n\nlinear, constant coefficient difference equations, e.g. \\[y[n] = a y[n-1] + b y[n-2] + x[n]\\]\nDT impulse response \\(h[n]\\)\n\npurely mathematically - in frequency domain we will use\n\nfrequency response\ntransfer function (complex frequency, covered in ECE 3704)\n\ngraphically, using a mixture of math and block diagrams",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#system-properties-and-classification",
    "href": "07-dt-lti.html#system-properties-and-classification",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.2 System properties and classification",
    "text": "7.2 System properties and classification\nChoosing the right kind of system model is important. Here are some important properties that allow us to broadly classify systems.\n\nMemory\nInvertability\nCausality\nStability\nTime-invariance\nLinearity\n\nLet’s define each it turn.\n\n7.2.1 Memory\nThe output of a DT system with memory depends on previous or future inputs and is said to be dynamic. Otherwise the system is memoryless or instantaneous, and the output \\(y[n]\\) at index \\(n\\) depends only on \\(x[n]\\). For example: \\[y[n] = 2x[n]\\] is a memoryless system, while \\[y[n+1] + y[n] = x[n]\\] has memory. To see this, write the difference equation in recursive form \\[y[n] = -y[n-1] + x[n-1]\\] and we see explicitly the current output \\(y[n]\\) depends on past values of output and input.\n\n\n7.2.2 Invertability\nA system is invertible if there exists a system that when placed in series with the original recovers the input. \\[x[n] \\mapsto{T} y[n] \\mapsto{T^{-1}} x[n]\\] where \\(T^{-1}\\) is the inverse system of \\(T\\). For example, consider a system \\[x[n] \\mapsto y[n] = \\sum\\limits_{m=-\\infty}^{n} x[m]\\] and a system \\[y[n] \\mapsto z[n] = y[n] - y[n-1]\\] The combination in series \\(x[n] \\mapsto y[n] \\mapsto z[n] = x[n]\\), since \\[z[n] = y[n] - y[n-1] = \\sum\\limits_{m=-\\infty}^{n} x[m] - \\sum\\limits_{m=-\\infty}^{n-1} x[m] = x[n]\\] i.e. the difference undoes the accumulation.\n\n\n7.2.3 Causality\nA DT system is causal if the output at index \\(n\\) depends on the input for index values at or before \\(n\\): \\[y[n] \\;\\text{depends on}\\; x[m] \\;\\text{for} \\; m \\leq n\\] While all physical CT systems are causal, practical DT systems may not be since we can use memory to \"shift time\". For CT systems we cannot store the infinite number of values between two time points \\(t_1\\) and \\(t_2\\), but we can store the \\(n_2-n_1\\) values of a DT system between between two indices \\(n_1\\) and \\(n_2\\) (assuming infinite precision).\n\n\nExample\n\n\nConsider a DT system whose difference equation is \\[y[n] = -x[n-1] + 2x[n] - x[n+1]\\] We see the current output \\(y[n]\\) depends on a \"future\" value of the input \\(x[n+1]\\). Thus the system is not causal. In practice we can shift the difference equation to \\[y[n-1] = -x[n-2] + 2x[n-1] - x[n]\\] and then delay the output by one sample to get \\(y[n]\\).\n\n\n\n\nExample\n\n\nConsider a DT system whose difference equation is \\[y[n] = -y[n-1] + 2x[n]\\] We see the current output \\(y[n]\\) depends on a \"past\" value of the output \\(y[n-1]\\) and the current input \\(x[n]\\). Thus the system is causal. In practice we can immediately compute \\(y[n]\\) with no delay.\n\n\n\n\n7.2.4 Stability\nA DT system is (BIBO) stable if applying a bounded-input (BI) \\[\\left|x[n]\\right| &lt; \\infty \\; \\forall \\; n\\] results in a bounded-output (BO) \\(x[n] \\mapsto y[n]\\) and \\[\\left|y[n]\\right| &lt; \\infty \\; \\forall \\; n\\] Note, bounded in practice is limited by the physical situation, e.g. the number of bits used to store values.\nFor example, a DT system described by the LCCDE \\[y[n+1] - 2 y[n] = x[n+1]\\] is unstable because the solution \\(y[n]\\) will have one term of the form \\(\\left( 2\\right)^n\\), for most non-zero inputs \\(x[n]\\) or any non-zero initial condition, that grows unbounded as \\(n\\) increases.\n\n\n7.2.5 Time-invariance\nA DT system is time(index)-invariant if, given \\[x[n] \\mapsto y[n]\\] then an index-shift of the input leads to the same index-shift in the output \\[x[n-m] \\mapsto y[n-m]\\]\nAn important example is a DT system described by a LCCDE, e.g. \\[y[n+1] - \\frac{1}{2} y[n] = x[n+1]\\] or in recursive form \\[y[n] = \\frac{1}{2} y[n-1] + x[n]\\]\nIf we index shift the input \\(x[n - m]\\) we replace \\(n\\) by \\(n-m\\) and the difference equation becomes \\[y[n-m+1] - \\frac{1}{2} y[n-m] = x[n-m+1]\\] which has the same solution shifted by \\(m\\) \\[y[n-m] = \\frac{1}{2} y[n-m -1] + x[n-m]\\]\nIf a coefficient depends on \\(n\\) however, e.g \\[y[n+1] - \\frac{n}{2} y[n] = x[n+1]\\] so that it is no longer LCC then the solution depends on \\(m\\) and the system is no longer time-invariant.\n\n\n7.2.6 Linearity\nA DT system is linear if the output due to a sum of scaled individual inputs is the same as the scaled sum of the individual outputs with respect to those inputs. In other words given \\[x_1[n] \\mapsto y_1[n] \\;\\text{and}\\; x_2[n] \\mapsto y_2[n]\\] then \\[a x_1[n] + b x_2[n] \\mapsto a y_1[n] + b y_2[n]\\] for constants \\(a\\) and \\(b\\). Note this property extends to sums of arbitrary signals, e.g. if \\[x_i[n] \\mapsto y_i[n] \\; \\forall\\; i \\in [1 \\cdots N]\\] then given \\(N\\) constants \\(a_i\\), if the system is linear \\[\\sum\\limits_{i = 1}^N a_i x_i[n] \\mapsto \\sum\\limits_{i = 1}^N a_i y_i[n]\\] This is a very important property, called superposition, and it simplifies the analysis of systems greatly.\nAn important non-linear system is that is described by a LCCDE with non-zero auxiliary conditions at some \\(n_0\\), \\(y[n_0] = y_0\\). As in CT, such systems will have a term in it’s solution that depends on \\(y_0\\). Given two inputs, each individual response will have that term in it, so their sum has double that term. However the response due to the sum of the inputs would again only have one and the sum of the responses would not be the same as the response of the sum. Such a system cannot be linear. Thus the system must be \"at rest\" before applying the input in order to be a linear system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "07-dt-lti.html#stable-lti-systems",
    "href": "07-dt-lti.html#stable-lti-systems",
    "title": "7  Linear Time Invariant DT Systems",
    "section": "7.3 Stable LTI Systems",
    "text": "7.3 Stable LTI Systems\nThe remainder of this course is about stable, linear, time-invariant (LTI) systems. As we have seen in DT such systems can be described by a LCCDE with zero auxiliary (initial) conditions (the system is at rest).\nWe have seen previously how to find the impulse response, \\(h[n]\\), of such systems. We now note some relationships between the impulse response and the system properties described above.\n\nIf a system is memoryless then \\(h[n] = C \\delta[n]\\) for some constant \\(C\\).\nIf a system is causal then \\(h[n] = 0\\) for \\(n &lt; 0\\).\nIf a system is BIBO stable then \\[\\sum\\limits_{-\\infty}^{\\infty} |h[n]| &lt; \\infty\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Invariant DT Systems</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html",
    "href": "08-ct-conv.html",
    "title": "8  CT Convolution",
    "section": "",
    "text": "8.1 Review CT LTI systems and superposition property\nRecall the superposition property of LTI systems. If a CT system is LTI then the superposition property holds. Given a system where \\[x_i(t) \\mapsto y_i(t) \\; \\forall\\; i\\] then \\[\\sum\\limits_{i} a_i x_i(t) \\mapsto \\sum\\limits_{i} a_i y_i(t)\\]\nSuperposition enables a powerful problem reduction strategy. The overall idea for is that if:\nThis will be a recurring pattern in this course. In this lecture, the simple signals are weighted, time shifts of one signal, the delta function, \\(\\delta(t)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#review-ct-lti-systems-and-superposition-property",
    "href": "08-ct-conv.html#review-ct-lti-systems-and-superposition-property",
    "title": "8  CT Convolution",
    "section": "",
    "text": "we can write an arbitrary signal as a sum of simple signals, and\nwe can determine the response to the simple signals, then\nwe can easily express the output due to the input using superposition",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#convolution-integral",
    "href": "08-ct-conv.html#convolution-integral",
    "title": "8  CT Convolution",
    "section": "8.2 Convolution Integral",
    "text": "8.2 Convolution Integral\nTo derive this we start with the sifting property of the CT impulse function (from chapter 2) \\[\\int\\limits_{a}^{b} x(t)\\delta(t-t_0) \\; dt = x(t_0)\\] for any \\(a &lt; t_0 &lt; b\\). A slight change of variables (\\(t_0 \\rightarrow \\tau\\)) and limits (\\(a \\rightarrow -\\infty\\) and \\(b \\rightarrow \\infty\\)) gives: \\[x(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)\\delta(t-\\tau) \\; d\\tau\\] showing that we can write any CT signal as an infinite sum (integral) of weighted and time-shifted impluse functions.\nLet \\(h(t)\\) be the CT impulse response, the output due to the input \\(\\delta(t)\\), i.e. \\(\\delta(t) \\mapsto h(t)\\). Then if the system is time-invariant: \\(\\delta(t-\\tau) \\mapsto h(t-\\tau)\\) and by superposition if the input is writen as \\[x(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)\\delta(t-\\tau) \\; d\\tau\\] then the output is given by \\[y(t) = \\int\\limits_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau) \\; d\\tau = x(t) * h(t)\\] This is called the convolution integral .\nIt is worth pausing here to see the signifigance. For a LTI CT system, if I know its impulse response \\(h(t)\\), I can find the response due to any input using convolution. For this reason the impulse response is another way to represent an LTI system.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#graphical-view-of-the-convolution-integral.",
    "href": "08-ct-conv.html#graphical-view-of-the-convolution-integral.",
    "title": "8  CT Convolution",
    "section": "8.3 Graphical View of the Convolution Integral.",
    "text": "8.3 Graphical View of the Convolution Integral.\nLet us break the convolution expression down into pieces. In its general form the convolution of two signals \\(x_1(t)\\) and \\(x_2(t)\\) is \\[x_1(t) * x_2(t) = \\int\\limits_{-\\infty}^{\\infty} x_1(\\tau)x_2(t-\\tau) \\; d\\tau\\]\nSuppose \\(x_1(t)\\) and \\(x_2(t)\\) are signals that look like\n\n\n\nThe two signals being convolved.\n\n\nThen \\(x_1(\\tau)\\) and \\(x_2(-\\tau)\\) look like\n\n\n\nThe second signal reflected.\n\n\nThe signal \\(x_2(t-\\tau)\\) is \\(x_2(-\\tau)\\) shifted by \\(t\\) (since \\(x_2(-\\tau+t)= x_2(t-\\tau)\\)) and then looks like\n\n\n\nThe second reflected signal, shifted.\n\n\nThen the integrand of convolution is the product \\(x_1(\\tau)x_2(t-\\tau)\\) whose plot depends of the value of \\(t\\). Some examples, where the individual signals are dashed and their product is in bold:\n\n\n\nThe product of the two signals under the convolution integrand.\n\n\nThen convolution is the total integral of the product (bold curves above) for that value of \\(t\\). For the example above we see the integral will be zero for \\(t\\) less than \\(t_0\\) since the two signals do not overlap and their product is zero. For \\(t_0 &lt; t &lt; t_1\\) the signals overap and the product is non-zero, and the effective bounds of integration are \\([t_0,t]\\). For \\(t &gt; t_1\\) the signals again overap and the product is non-zero, but the effective bounds of integration are \\([t_0,t_1]\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#examples-of-ct-convolution",
    "href": "08-ct-conv.html#examples-of-ct-convolution",
    "title": "8  CT Convolution",
    "section": "8.4 Examples of CT Convolution",
    "text": "8.4 Examples of CT Convolution\n\n\nExample\n\n\nConsider the convolution of two unit step functions. \\[u(t) * u(t) = \\int\\limits_{-\\infty}^{\\infty} u(\\tau)u(t-\\tau) \\; d\\tau\\] The product \\(u(\\tau) u(t-\\tau)\\) is non-zero only when \\(t\\geq 0\\) as illustrated here\n\n\n\nThe product of the two step signals under the convolution integrand.\n\n\nThe convolution integral is then the shaded area \\[u(t) * u(t) = \\left\\{ \\begin{array}{lc}\n  0 & t&lt; 0\\\\\n  \\int\\limits_{0}^{t} d\\tau = t  & t \\geq 0\\\\\n\\end{array}\\right.\\] Combining this back into a single expression gives: \\[u(t) * u(t) = tu(t)\\] Thus the convolution of two step signals is a ramp signal.\n\n\n\n\nExample\n\n\nLet \\(x_1(t) = u(t)\\) and \\(x_2(t) = e^{-at}u(t)\\) for constant \\(a\\in\\mathbb{C}\\), then \\[u(t) * e^{-at}u(t) = \\int\\limits_{-\\infty}^{\\infty} u(\\tau)e^{-a(t-\\tau)}u(t-\\tau) \\; d\\tau\\] Similar to the previous example, the product \\(u(\\tau) e^{-a(t-\\tau)} u(t-\\tau)\\) is non-zero only when \\(t\\geq 0\\)\n\n\n\nThe product of the two step signals under the convolution integrand.\n\n\nThe convolution integral is then the shaded area \\[u(t) * e^{-at}u(t) = \\left\\{ \\begin{array}{lc}\n  0 & t&lt; 0\\\\\n  \\int\\limits_{0}^{t} e^{-a(t-\\tau)} d\\tau = \\frac{1-e^{-at}}{a}  & t \\geq 0\\\\\n\\end{array}\\right.\\] Combining this back into a single expression gives: \\[u(t) * e^{-at}u(t) = \\frac{1-e^{-at}}{a}u(t)\\]\n\n\n\n\nExample\n\n\nLet \\(x_1(t) = \\delta(t)\\) and \\(x_2(t)\\) be an arbitrary signal. Then \\[\\delta(t) * x_2(t) = \\int\\limits_{-\\infty}^{\\infty} \\delta(\\tau)x_2(t-\\tau) \\; d\\tau\\] By the sifting property of the delta function this evaluates to \\[\\delta(t) * x_2(t) = x_2(t)\\] or in other words convolution with a delta function just results in the signal it was convolved with. That is it acts like the identity function, with respect to convolution.\n\n\nThe appendix lists several CT convolution results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "08-ct-conv.html#properties-of-ct-convolution",
    "href": "08-ct-conv.html#properties-of-ct-convolution",
    "title": "8  CT Convolution",
    "section": "8.5 Properties of CT Convolution",
    "text": "8.5 Properties of CT Convolution\nThere are several useful properties of convolution. We do not prove these here, but it is not terribly difficult to do so. Given signals \\(x_1(t)\\), \\(x_2(t)\\), and \\(x_3(t)\\):\n\nCommunative Property\n\nThe ordering of the signals does not matter. \\[x_1(t) * x_2(t) = x_2(t) * x_1(t)\\]\n\nDistributive Property\n\nConvolution is distributed over addition. \\[x_1(t) * \\left[x_2(t) + x_3(t)\\right] = \\left[x_1(t) * x_2(t) \\right] + \\left[x_1(t) * x_3(t) \\right]\\]\n\nAssociative Property\n\nThe order of convolution does not matter. \\[x_1(t) * \\left[x_2(t) * x_3(t)\\right] = \\left[x_1(t) * x_2(t) \\right] * x_3(t)\\]\n\nTime Shift\n\nGiven \\(x_3(t) = x_1(t) * x_2(t)\\) then for time shifts \\(\\tau_1, \\tau_2 \\in \\mathbb{R}\\) \\[x_1(t-\\tau_1) * x_2(t-\\tau_2) = x_3(t-\\tau_1 - \\tau_2)\\]\n\nMultiplicative Scaling\n\nGiven \\(x_3(t) = x_1(t) * x_2(t)\\) then for constants \\(a,b \\in \\mathbb{C}\\) \\[\\left[a\\, x_1(t)\\right] * \\left[b\\, x_2(t)\\right] = a\\, b\\, x_3(t)\\]\n\n\nThese properties can be used in combination with a table like that above to compute the convolution of a wide variety of signals without evaluating the integrals.\n\n\nExample\n\n\nHere is a simple example. Let \\(x_1(t) = e^tu(t)\\) and \\(x_2(t) = 2\\delta(t) + 5e^{-3t}u(t)\\). \\[x_1(t) * x_2(t) =  e^tu(t) * \\left[2\\delta(t) + 5e^{-3t}u(t)\\right]\\] Using the distributive property \\[x_1(t) * x_2(t) =  2\\left[\\delta(t) * e^tu(t)\\right]  + 5\\left[e^tu(t) * e^{-3t}u(t)\\right]\\] Using previously derived results involving the delta function and the table row 3 \\[x_1(t) * x_2(t) = 2 e^t\\, u(t) + 5\\left[ \\frac{e^t-e^{-3t}}{4}\\right]u(t)\\] Doing some simplification gives the result \\[x_1(t) * x_2(t) = \\left[ \\frac{13}{4}e^t-\\frac{5}{4}e^{-3t}\\right]u(t)\\]\n\n\n\n\nExample\n\n\nHere is a more complicated example. Let \\(x_1(t) = 2e^{-5t}u(t-1)\\) and \\(x_2(t) = \\left(1-e^{-t}\\right)u(t)\\). \\[x_1(t) * x_2(t) = \\left[2e^{-5t}u(t-1)\\right] * \\left[\\left(1-e^{-t}\\right)u(t)\\right]\\] We first rewrite \\(e^{-5t}u(t-1)=e^{-5}e^{-5(t-1)}u(t-1) = e^{-5}e^{-5t}u(t)\\Big|_{t=t-1}\\) so that we can remove the time shift \\[x_1(t) * x_2(t) = 2e^{-5}\\left[e^{-5t}u(t)\\right] * \\left[\\left(1-e^{-t}\\right)u(t)\\right]\\Big|_{t=t-1}\\] We now apply the distributive property \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\left(e^{-5t}u(t) * u(t)\\right) - \\left(e^{-5t}u(t)* e^{-t}u(t)\\right)\\right]\\Big|_{t=t-1}\\] Using the table rows 1 and 3 we get \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5}\\left(1-e^{-5t}\\right)u(t) + \\frac{1}{4}\\left(e^{-5t} - e^{-t}\\right)u(t)\\right]\\Big|_{t=t-1}\\] Combining terms we simplify to \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5} - \\frac{1}{4}e^{-t} + \\frac{1}{20}e^{-5t} \\right]u(t)\\Big|_{t=t-1}\\] Replacing the time shift gives the final result \\[x_1(t) * x_2(t) = 2e^{-5}\\left[\\frac{1}{5} - \\frac{1}{4}e^{-(t-1)} + \\frac{1}{20}e^{-5(t-1)} \\right]u(t-1)\\] which can be cleaned up a bit more by distributing the leading term \\[x_1(t) * x_2(t) =\\left[\\frac{2}{5}e^{-5} -\\frac{1}{2}e^{-(t+4)} +\\frac{1}{10}e^{-5t}\\right]u(t-1)\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CT Convolution</span>"
    ]
  },
  {
    "objectID": "26-formulas-tables.html",
    "href": "26-formulas-tables.html",
    "title": "Useful Mathematical Definitions and Tables",
    "section": "",
    "text": "Definition of modulus for integers\nLet \\(n\\in\\mathbb{Z}\\) and \\(N \\in \\mathbb{N}\\). The mod operator \\[n \\% N = \\left\\{ \\begin{array}{cc}\n  \\text{remainder}\\left(\\frac{n}{N}\\right) & n \\geq 0\\\\\n  N - \\text{remainder}\\left(\\frac{|n|}{N}\\right) & n &lt; 0\n  \\end{array}\n\\right.\\] where \\(\\text{remainder}\\) is the remainder after dividing \\(n\\) by \\(N\\).",
    "crumbs": [
      "Appendices",
      "Useful Mathematical Definitions and Tables"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:ctconv",
    "href": "26-formulas-tables.html#table:ctconv",
    "title": "Useful Mathematical Definitions and Tables",
    "section": "Table of Representative Convolution Integrals",
    "text": "Table of Representative Convolution Integrals\n\nCT Convolution Table\n\n\n\\(x_1(t)\\)\n\\(x_2(t)\\)\n\\(x_1(t) * x_2(t)\\)\n\n\n\n\n\\(u(t)\\)\n\\(e^{a t}u(t)\\)\n\\(\\frac{1-e^{a t}}{-a}u(t)\\)\n\n\n\\(u(t)\\)\n\\(u(t)\\)\n\\(tu(t)\\)\n\n\n\\(e^{a_1 t}u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{e^{a_1 t}-e^{a_2 t}}{a_1 - a_2}u(t)\\) for \\(a_1 \\neq a_2\\)\n\n\n\\(e^{a t}u(t)\\)\n\\(e^{a t}u(t)\\)\n\\(te^{a t}u(t)\\)\n\n\n\\(te^{a_1 t}u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{e^{a_2 t}-e^{a_1 t} + (a_1-a_2)te^{a_1 t}}{(a_1 - a_2)^2}u(t)\\) for \\(a_1 \\neq a_2\\)\n\n\n\\(e^{a_1 t}\\cos(\\beta t + \\theta)u(t)\\)\n\\(e^{a_2 t}u(t)\\)\n\\(\\frac{\\cos(\\theta - \\phi)e^{a_2 t} - e^{a_1 t}\\cos(\\beta t + \\theta - \\phi)}{\\sqrt{(a_1 + a_2)^2 + \\beta^2}}u(t)\\)\n\n\n\n\n\\(\\phi = \\arctan\\left( \\frac{-\\beta}{a_1 + a_2}\\right)\\)",
    "crumbs": [
      "Appendices",
      "Useful Mathematical Definitions and Tables"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:dtconv",
    "href": "26-formulas-tables.html#table:dtconv",
    "title": "Useful Mathematical Definitions and Tables",
    "section": "Table of Representative Convolution Sums",
    "text": "Table of Representative Convolution Sums\n\nDT Convolution Table\n\n\n\\(x_1[n]\\)\n\\(x_2[n]\\)\n\\(x_1[n] * x_2[n]\\)\n\n\n\n\n\\(u[n]\\)\n\\(u[n]\\)\n\\((n+1)u[n]\\)\n\n\n\\(\\gamma^{n}u[n]\\)\n\\(u[n]\\)\n\\(\\frac{1-\\gamma^{n+1}}{1-\\gamma}u[n]\\)\n\n\n\\(\\gamma_1^{n}u[n]\\)\n\\(\\gamma_2^{n}u[n]\\)\n\\(\\frac{\\gamma_1^{n+1}-\\gamma_2^{n+1}}{\\gamma_1-\\gamma_2}u[n]\\) for \\(\\gamma_1 \\neq \\gamma_2\\)\n\n\n\\(\\gamma^{n}u[n]\\)\n\\(\\gamma^{n}u[n]\\)\n\\((n+1)\\gamma^{n}u[n]\\)\n\n\n\\(|\\gamma_1|^{n}\\cos\\left(\\beta n + \\theta \\right)u[n]\\)\n\\(|\\gamma_2|^{n}u[n]\\)\n\\(\\frac{1}{R}\\left[ |\\gamma_1|^{n+1}\\cos\\left( \\beta (n+1) + \\theta - \\phi\\right) - |\\gamma_2|^{n+1}\\cos\\left( \\theta - \\phi\\right)\\right]u[n]\\)\n\n\n\n\n\\(R = \\left[ |\\gamma_1|^2 + |\\gamma_2|^2 -2|\\gamma_1||\\gamma_2|\\cos(\\beta)\\right]^{\\frac{1}{2}}\\)\n\n\n\n\n\\(\\phi = \\arctan\\left( \\frac{|\\gamma_1|\\sin(\\beta)}{|\\gamma_1|\\cos(\\beta) - |\\gamma_2|} \\right)\\)",
    "crumbs": [
      "Appendices",
      "Useful Mathematical Definitions and Tables"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:ctft",
    "href": "26-formulas-tables.html#table:ctft",
    "title": "Useful Mathematical Definitions and Tables",
    "section": "Table of Representative CT Fourier Transform Pairs",
    "text": "Table of Representative CT Fourier Transform Pairs\n\nCT Fourier Transform Table\n\n\n\\(x(t)\\)\n\\(X(j\\omega)\\)\n\n\n\n\n\\(1\\)\n\\(2\\pi\\delta(\\omega)\\)\n\n\n\\(\\delta(t)\\)\n\\(1\\)\n\n\n\\(u(t)\\)\n\\(\\pi\\delta(\\omega) + \\frac{1}{j\\omega}\\)\n\n\n\\(e^{-at}u(t)\\)\n\\(\\frac{1}{a + j\\omega}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(te^{-at}u(t)\\)\n\\(\\frac{1}{\\left(a + j\\omega\\right)^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(e^{j\\omega_0 t}\\)\n\\(2\\pi\\delta(\\omega-\\omega_0)\\)\n\n\n\\(\\cos(\\omega_0 t)\\)\n\\(\\pi\\left[ \\delta(\\omega-\\omega_0) + \\delta(\\omega+\\omega_0)\\right]\\)\n\n\n\\(\\sin(\\omega_0 t)\\)\n\\(j\\pi\\left[ \\delta(\\omega+\\omega_0) - \\delta(\\omega-\\omega_0)\\right]\\)\n\n\n\\(e^{-at}\\cos(\\omega_0 t)u(t)\\)\n\\(\\frac{a+j\\omega}{(a+j\\omega)^2 + \\omega_0^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(e^{-at}\\sin(\\omega_0 t)u(t)\\)\n\\(\\frac{\\omega_0}{(a+j\\omega)^2 + \\omega_0^2}\\) for \\(Re\\{a\\} &gt; 0\\)\n\n\n\\(\\delta(t - t_0)\\)\n\\(e^{-j t_0 \\omega}\\)\n\n\n\\(K_0\\)\n\\(2 K_0 \\pi \\delta(\\omega)\\)\n\n\n\\(e^{-a|t|}\\), \\(\\text{Re}\\{a\\} &gt; 0\\)\n\\(\\frac{2a}{a^2 + \\omega^2}\\)\n\n\n\\(u(t + T) - u(t - T)\\)\n\\(2T \\frac{\\sin{(\\omega T)}}{\\omega T}\\)\n\n\n\\(\\frac{\\sin{({W}t)}}{W t}\\)\n\\(\\frac{\\pi}{W} [u(\\omega + W) - u(\\omega - W)]\\)\n\n\n\\(e^{-\\frac{t^2}{2 \\sigma^2}}\\)\n\\(\\sigma \\sqrt{2 \\pi} e^{-\\frac{\\sigma^2 \\omega^2}{2}}\\)\n\n\n\\(\\sum\\limits_{k=-\\infty}^{\\infty} a_k e^{j k \\omega_0 t}\\)\n\\(2 \\pi \\sum\\limits_{k=-\\infty}^{\\infty} a_k \\delta{(\\omega - k \\omega_0)}\\)\n\n\n\\(\\sum\\limits_{n=-\\infty}^{\\infty} \\delta(t - nT)\\)\n\\(\\omega_0 \\sum\\limits_{k=-\\infty}^{\\infty} \\delta{(\\omega - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{T}\\)",
    "crumbs": [
      "Appendices",
      "Useful Mathematical Definitions and Tables"
    ]
  },
  {
    "objectID": "26-formulas-tables.html#table:dtft",
    "href": "26-formulas-tables.html#table:dtft",
    "title": "Useful Mathematical Definitions and Tables",
    "section": "Table of Representative DT Fourier Transform Pairs",
    "text": "Table of Representative DT Fourier Transform Pairs\n\nDT Fourier Transform Table\n\n\n\\(x[n]\\)\n\\(X(e^{j\\omega})\\)\n\n\n\n\n\\(\\delta[n]\\)\n\\(1\\)\n\n\n\\(\\delta[n - n_0]\\)\n\\(e^{-j \\omega n_0}\\)\n\n\n\\(u[n]\\)\n\\(\\frac{1}{1 - e^{-j \\omega}} + \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta(\\omega - 2 k \\pi)\\)\n\n\n\\(K_0\\)\n\\(2 K_0 \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta(\\omega - 2 k \\pi)\\)\n\n\n\\(a^n u[n]\\), \\(|a| &lt; 1\\)\n\\(\\frac{1}{(1 - ae^{-j \\omega)}}\\)\n\n\n\\(n a^n u[n]\\), \\(|a| &lt; 1\\)\n\\(\\frac{a e^{-j \\omega}}{(1 - a e^{-j \\omega})^2}\\)\n\n\n\\(a^{|n|}\\), \\(|a| &lt; 1\\)\n\\(\\frac{1-a^2}{1 - 2 a \\cos{\\omega} + a^2}\\)\n\n\n\\(e^{j \\omega_0 n}\\)\n\\(2 \\pi \\sum\\limits_{k = - \\infty}^{\\infty} \\delta{(\\omega - \\omega_0 - 2 k \\pi)}\\)\n\n\n\\(\\cos{(\\omega_0 n)}\\)\n\\(\\pi \\sum\\limits_{k = - \\infty}^{\\infty} [\\delta(\\omega + \\omega_0 - 2 k \\pi) + \\delta(\\omega - \\omega_0 - 2 k \\pi)]\\)\n\n\n\\(\\sin{(\\omega_0 n)}\\)\n\\(j \\pi \\sum\\limits_{k = - \\infty}^{\\infty} [\\delta(\\omega + \\omega_0 - 2 k \\pi) - \\delta(\\omega - \\omega_0 - 2 k \\pi)]\\)\n\n\n\\(u[n + n_0] - u[n - n_0]\\)\n\\(\\frac{\\sin{(\\omega (n_0 + 0.5))}}{\\sin{0.5 \\omega}}\\)\n\n\n\\(\\frac{\\sin{({W}n)}}{\\pi n}\\)\n\\(\\sum\\limits_{k = - \\infty}^{\\infty} X_1(\\omega - 2 k \\pi), X_1(\\omega) = \\begin{array}{cc} 1 & 0 \\le |\\omega| \\le W, 0 &lt; W &lt; \\pi\\\\ 0 & W &lt; |\\omega| \\le \\pi,  0 &lt; W &lt; \\pi \\end{array}\\)\n\n\n\\(\\sum\\limits_{k=n_0}^{n_0+N-1} a_k e^{j k \\omega_0 n}\\)\n\\(2 \\pi \\sum\\limits_{k=-\\infty}^{\\infty} a_k \\delta{(\\omega - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{N}\\),\n\n\n\\(\\sum\\limits_{k=-\\infty}^{\\infty} \\delta[n - kN]\\)\n\\(\\omega_0 \\sum\\limits_{k=-\\infty}^{\\infty} \\delta{(n - k \\omega_0)}\\), \\(\\omega_0 = \\frac{2 \\pi}{N}\\),",
    "crumbs": [
      "Appendices",
      "Useful Mathematical Definitions and Tables"
    ]
  }
]